{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Задачи 2 и 3 се извршени со нецело податочно множество, поточно 1 милион линии од текст фајлот се испроцесирани за задача 2, и 30 илјади за 3-та, поради РАМ лимити или предолги тренирања. Ваков проблем немав со првата задача па затоа таа е решена со цел датасет."
      ],
      "metadata": {
        "id": "Bu-JEyV6YVDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!7z x amazon-meta.txt.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaDtnam3bwDA",
        "outputId": "2c142d6d-b1e5-4ab2-ad5e-fcecf42e7fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 210807517 bytes (202 MiB)\n",
            "\n",
            "Extracting archive: amazon-meta.txt.gz\n",
            "--\n",
            "Path = amazon-meta.txt.gz\n",
            "Type = gzip\n",
            "Headers Size = 26\n",
            "\n",
            "  0% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% - amazon-meta.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       977506331\n",
            "Compressed: 210807517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk0XTt6g4kdr",
        "outputId": "ec099d25-8b7f-4595-98bc-1028e036d84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.1\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.1)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1) (1.3.0)\n",
            "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.1 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 triton-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.1+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etQodEng4xf-",
        "outputId": "02f3663a-bc8a-49ff-974a-831a28f8d685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/pyg_lib-0.4.0%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (943 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.4/943.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt22cu121 torch_cluster-1.6.3+pt22cu121 torch_scatter-2.1.2+pt22cu121 torch_sparse-0.6.18+pt22cu121 torch_spline_conv-1.2.2+pt22cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1SOX2Sk8ixP",
        "outputId": "0c233528-cba3-4112-bf11-7525d7e6814c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/amazon-meta.txt' /content/"
      ],
      "metadata": {
        "id": "3qJUYznzcV8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch_geometric.nn import to_hetero\n",
        "from torch_geometric.datasets import IMDB\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch.nn.functional import dropout\n",
        "from torch_geometric.nn import GCNConv, Linear, SAGEConv\n",
        "from torch.optim import Adam\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "rC4Fo97Y6C9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv((-1, -1), 64)\n",
        "        self.conv2 = SAGEConv((-1, -1), 128)\n",
        "\n",
        "        self.linear1 = Linear(128, 64)\n",
        "        self.linear2 = Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).tanh()\n",
        "        x = dropout(x, p=0.3)\n",
        "\n",
        "        x = self.conv2(x, edge_index).tanh()\n",
        "        x = dropout(x, p=0.3)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-dcKSDQX8dh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Претпроцесирање на дата и креирање граф објект"
      ],
      "metadata": {
        "id": "3n60sadGsssk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_and_create_hetero_data(file_path, limit=None):\n",
        "    products_by_id = {}\n",
        "    group_by_id = {}\n",
        "    similar_by_id = defaultdict(list)\n",
        "    reviewers_by_p_id = defaultdict(list)\n",
        "    customer_map = {}\n",
        "    asin_map = {}\n",
        "    next_product_id = 0\n",
        "    next_customer_id = 0\n",
        "    groups = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        total_lines = sum(1 for _ in file)\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        current_asin = None\n",
        "        for line_num, line in enumerate(tqdm(file, total=total_lines, desc=\"Processing file\", unit=\"line\")):\n",
        "            if limit and line_num >= limit:\n",
        "                break\n",
        "\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"ASIN:\"):\n",
        "                current_asin = line.split(\"ASIN: \")[1]\n",
        "                if current_asin not in asin_map:\n",
        "                    asin_map[current_asin] = next_product_id\n",
        "                    products_by_id[next_product_id] = current_asin\n",
        "                    next_product_id += 1\n",
        "            elif line.startswith(\"group:\"):\n",
        "                groups.append(line.split(\"group: \")[1])\n",
        "                group_by_id[asin_map[current_asin]] = groups[-1]\n",
        "            elif line.startswith(\"similar:\"):\n",
        "                similar_asins = line.split(\"similar: \")[1].split()[1:]\n",
        "                for similar_asin in similar_asins:\n",
        "                    if similar_asin not in asin_map:\n",
        "                        asin_map[similar_asin] = next_product_id\n",
        "                        products_by_id[next_product_id] = similar_asin\n",
        "                        next_product_id += 1\n",
        "                    similar_by_id[asin_map[current_asin]].append(asin_map[similar_asin])\n",
        "            elif \"cutomer:\" in line:\n",
        "                customer_info = line.split(\"cutomer: \")[1].split()\n",
        "                customer_id, rating = customer_info[0], float(customer_info[2])\n",
        "                if customer_id not in customer_map:\n",
        "                    customer_map[customer_id] = next_customer_id\n",
        "                    next_customer_id += 1\n",
        "                reviewers_by_p_id[asin_map[current_asin]].append((customer_map[customer_id], rating))\n",
        "\n",
        "    data = HeteroData()\n",
        "\n",
        "    data['products'].x = torch.rand(len(products_by_id), 10)\n",
        "    data['customers'].x = torch.rand(len(customer_map), 10)\n",
        "\n",
        "    similar_edges = []\n",
        "    for source, targets in similar_by_id.items():\n",
        "        for target in targets:\n",
        "            similar_edges.append((source, target))\n",
        "    if similar_edges:\n",
        "        data['products', 'similar_to', 'products'].edge_index = torch.tensor(similar_edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    review_edges = []\n",
        "    review_ratings = []\n",
        "    for product, reviews in reviewers_by_p_id.items():\n",
        "        for customer, rating in reviews:\n",
        "            review_edges.append((product, customer))\n",
        "            review_ratings.append(rating)\n",
        "    if review_edges:\n",
        "        data['products', 'reviewed_by', 'customers'].edge_index = torch.tensor(review_edges, dtype=torch.long).t().contiguous()\n",
        "        data['products', 'reviewed_by', 'customers'].edge_attr = torch.tensor(review_ratings, dtype=torch.float32)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    product_labels = encoder.fit_transform([group_by_id.get(pid, \"\") for pid in range(len(products_by_id))])\n",
        "    data['products'].y = torch.tensor(product_labels, dtype=torch.long)\n",
        "\n",
        "    num_products = len(products_by_id)\n",
        "    indices = np.arange(num_products)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_cutoff = int(0.65 * num_products)\n",
        "    test_cutoff = int(0.95 * num_products)\n",
        "\n",
        "    train_mask = torch.zeros(num_products, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_products, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_products, dtype=torch.bool)\n",
        "\n",
        "    train_mask[indices[:train_cutoff]] = True\n",
        "    test_mask[indices[train_cutoff:test_cutoff]] = True\n",
        "    val_mask[indices[test_cutoff:]] = True\n",
        "\n",
        "    data['products'].train_mask = train_mask\n",
        "    data['products'].test_mask = test_mask\n",
        "    data['products'].val_mask = val_mask\n",
        "\n",
        "    train_loader = NeighborLoader(data, num_neighbors=[3, 3], shuffle=True,\n",
        "                                  input_nodes=('products', train_mask), batch_size=128)\n",
        "    val_loader = NeighborLoader(data, num_neighbors=[3, 3], shuffle=False,\n",
        "                                input_nodes=('products', val_mask), batch_size=128)\n",
        "    test_loader = NeighborLoader(data, num_neighbors=[3, 3], shuffle=False,\n",
        "                                 input_nodes=('products', test_mask), batch_size=128)\n",
        "\n",
        "    data['products'].x = data['products'].x.float()\n",
        "    data['customers'].x = data['customers'].x.float()\n",
        "    for edge_type in data.edge_types:\n",
        "        if 'edge_attr' in data[edge_type]:\n",
        "            data[edge_type].edge_attr = data[edge_type].edge_attr.float()\n",
        "    return data, train_loader, val_loader, test_loader\n",
        "\n",
        "file_path = 'amazon-meta.txt'\n",
        "data, train_loader, val_loader, test_loader = process_and_create_hetero_data(file_path, limit=30000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gCklJqt6ddK",
        "outputId": "c143c117-0577-4d61-f5d1-a712c33478c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing file:   0%|          | 30000/15010574 [00:00<00:44, 337324.49line/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# По процесирање на цел датасет, испаѓа дека се 11 класи(вклучувајќи ја и празен стринг класата), а не само 4-те наведени во фајлот.\n",
        "# Оваа листа ја нема во кодот бидејќи ја добив со принтање на сите класи кои кодот погоре ги екстрахира, па потоа го ископирав аутпутот и го внесов како листа. Ќелијата го кочеше цел колаб и морав да ја избришам"
      ],
      "metadata": {
        "id": "IDxwwgEzTqQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqyjdijlLvhn",
        "outputId": "44b7360e-da83-4516-db71-54f7852bb7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'',\n",
              " 'Baby Product',\n",
              " 'Book',\n",
              " 'CE',\n",
              " 'DVD',\n",
              " 'Music',\n",
              " 'Software',\n",
              " 'Sports',\n",
              " 'Toy',\n",
              " 'Video',\n",
              " 'Video Games'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1ebrx_v79Ru",
        "outputId": "471da47b-5b63-4fe4-9788-3ea0fc996ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  products={\n",
              "    x=[721342, 10],\n",
              "    y=[721342],\n",
              "    train_mask=[721342],\n",
              "    test_mask=[721342],\n",
              "    val_mask=[721342],\n",
              "  },\n",
              "  customers={ x=[1555170, 10] },\n",
              "  (products, similar_to, products)={ edge_index=[2, 1788725] },\n",
              "  (products, reviewed_by, customers)={\n",
              "    edge_index=[2, 7593244],\n",
              "    edge_attr=[7593244],\n",
              "  }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 1"
      ],
      "metadata": {
        "id": "7EU5jdyUTvBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classification(model,\n",
        "                         train_loader, val_loader,\n",
        "                         optimizer, criterion,\n",
        "                         epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_dict = batch.x_dict\n",
        "            edge_index_dict = batch.edge_index_dict\n",
        "            train_mask = batch['products'].train_mask\n",
        "            data_y = batch['products'].y\n",
        "\n",
        "            out = model(x_dict, edge_index_dict)\n",
        "            loss = criterion(out['products'][train_mask], data_y[train_mask])\n",
        "            train_loss = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Loss: {train_loss:.4f}')"
      ],
      "metadata": {
        "id": "91B0LkydCzkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = GCN(num_classes=11)"
      ],
      "metadata": {
        "id": "V9LCWIDFDMX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = to_hetero(base_model, data.metadata(), aggr='sum')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McGV_PtoDMv6",
        "outputId": "9bf4774a-8286-4a03-88f2-9ce71b09c771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
            "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/fx.py:132: UserWarning: Found function 'dropout_1' with keyword argument 'training'. During FX tracing, this will likely be baked in as a constant value. Consider replacing this function by a module to properly encapsulate its training flag.\n",
            "  warnings.warn(f\"Found function '{node.name}' with keyword \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.float()"
      ],
      "metadata": {
        "id": "IWuWEOXyDRXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import one_hot\n",
        "\n",
        "\n",
        "def train_classification(model,\n",
        "                         train_loader, val_loader,\n",
        "                         optimizer, criterion,\n",
        "                         epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_dict = batch.x_dict\n",
        "            edge_index_dict = batch.edge_index_dict\n",
        "            train_mask = batch['products'].train_mask\n",
        "            data_y = batch['products'].y\n",
        "\n",
        "            out = model(x_dict, edge_index_dict)\n",
        "            loss = criterion(out['products'][train_mask], data_y[train_mask])\n",
        "            train_loss = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Loss: {train_loss:.4f}')\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            model.eval()\n",
        "\n",
        "            x_dict = batch.x_dict\n",
        "            edge_index_dict = batch.edge_index_dict\n",
        "            val_mask = batch['products'].val_mask\n",
        "            data_y = batch['products'].y\n",
        "\n",
        "            out = model(x_dict, edge_index_dict)\n",
        "            loss = criterion(out['products'][val_mask], data_y[val_mask])\n",
        "            val_loss = loss.item()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "BLh-za3BC2BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "criterion = CrossEntropyLoss()\n",
        "train_classification(model=model,\n",
        "                     train_loader=train_loader, val_loader=val_loader,\n",
        "                     optimizer=optimizer, criterion=criterion,\n",
        "                     epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3nFWuqDC6HR",
        "outputId": "4bb93ab8-afa9-4a04-e3e5-0575af7b0136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 008, Step: 2892, Loss: 0.9240\n",
            "Epoch: 008, Step: 2893, Loss: 0.9095\n",
            "Epoch: 008, Step: 2894, Loss: 1.0160\n",
            "Epoch: 008, Step: 2895, Loss: 0.9837\n",
            "Epoch: 008, Step: 2896, Loss: 0.8539\n",
            "Epoch: 008, Step: 2897, Loss: 0.9708\n",
            "Epoch: 008, Step: 2898, Loss: 0.8055\n",
            "Epoch: 008, Step: 2899, Loss: 1.0800\n",
            "Epoch: 008, Step: 2900, Loss: 0.8989\n",
            "Epoch: 008, Step: 2901, Loss: 0.9798\n",
            "Epoch: 008, Step: 2902, Loss: 0.9023\n",
            "Epoch: 008, Step: 2903, Loss: 1.0332\n",
            "Epoch: 008, Step: 2904, Loss: 1.0901\n",
            "Epoch: 008, Step: 2905, Loss: 1.0825\n",
            "Epoch: 008, Step: 2906, Loss: 1.0369\n",
            "Epoch: 008, Step: 2907, Loss: 0.9497\n",
            "Epoch: 008, Step: 2908, Loss: 0.9347\n",
            "Epoch: 008, Step: 2909, Loss: 0.9424\n",
            "Epoch: 008, Step: 2910, Loss: 0.8714\n",
            "Epoch: 008, Step: 2911, Loss: 1.0795\n",
            "Epoch: 008, Step: 2912, Loss: 1.0676\n",
            "Epoch: 008, Step: 2913, Loss: 0.8373\n",
            "Epoch: 008, Step: 2914, Loss: 0.9044\n",
            "Epoch: 008, Step: 2915, Loss: 0.8956\n",
            "Epoch: 008, Step: 2916, Loss: 0.9359\n",
            "Epoch: 008, Step: 2917, Loss: 1.0231\n",
            "Epoch: 008, Step: 2918, Loss: 1.0804\n",
            "Epoch: 008, Step: 2919, Loss: 0.8223\n",
            "Epoch: 008, Step: 2920, Loss: 1.0404\n",
            "Epoch: 008, Step: 2921, Loss: 0.8498\n",
            "Epoch: 008, Step: 2922, Loss: 1.0164\n",
            "Epoch: 008, Step: 2923, Loss: 0.9348\n",
            "Epoch: 008, Step: 2924, Loss: 1.0424\n",
            "Epoch: 008, Step: 2925, Loss: 0.9525\n",
            "Epoch: 008, Step: 2926, Loss: 0.8933\n",
            "Epoch: 008, Step: 2927, Loss: 0.8602\n",
            "Epoch: 008, Step: 2928, Loss: 1.0165\n",
            "Epoch: 008, Step: 2929, Loss: 0.8584\n",
            "Epoch: 008, Step: 2930, Loss: 0.9155\n",
            "Epoch: 008, Step: 2931, Loss: 0.9711\n",
            "Epoch: 008, Step: 2932, Loss: 0.8952\n",
            "Epoch: 008, Step: 2933, Loss: 0.8486\n",
            "Epoch: 008, Step: 2934, Loss: 1.0281\n",
            "Epoch: 008, Step: 2935, Loss: 0.7898\n",
            "Epoch: 008, Step: 2936, Loss: 0.9147\n",
            "Epoch: 008, Step: 2937, Loss: 0.9645\n",
            "Epoch: 008, Step: 2938, Loss: 0.7544\n",
            "Epoch: 008, Step: 2939, Loss: 0.9676\n",
            "Epoch: 008, Step: 2940, Loss: 0.9199\n",
            "Epoch: 008, Step: 2941, Loss: 0.8970\n",
            "Epoch: 008, Step: 2942, Loss: 0.8694\n",
            "Epoch: 008, Step: 2943, Loss: 1.0124\n",
            "Epoch: 008, Step: 2944, Loss: 0.8632\n",
            "Epoch: 008, Step: 2945, Loss: 0.8872\n",
            "Epoch: 008, Step: 2946, Loss: 0.9521\n",
            "Epoch: 008, Step: 2947, Loss: 0.9555\n",
            "Epoch: 008, Step: 2948, Loss: 0.8735\n",
            "Epoch: 008, Step: 2949, Loss: 1.0040\n",
            "Epoch: 008, Step: 2950, Loss: 1.0275\n",
            "Epoch: 008, Step: 2951, Loss: 1.2444\n",
            "Epoch: 008, Step: 2952, Loss: 1.2221\n",
            "Epoch: 008, Step: 2953, Loss: 0.9699\n",
            "Epoch: 008, Step: 2954, Loss: 0.9013\n",
            "Epoch: 008, Step: 2955, Loss: 0.8479\n",
            "Epoch: 008, Step: 2956, Loss: 0.8135\n",
            "Epoch: 008, Step: 2957, Loss: 0.8445\n",
            "Epoch: 008, Step: 2958, Loss: 1.1114\n",
            "Epoch: 008, Step: 2959, Loss: 0.7282\n",
            "Epoch: 008, Step: 2960, Loss: 0.9781\n",
            "Epoch: 008, Step: 2961, Loss: 1.0739\n",
            "Epoch: 008, Step: 2962, Loss: 1.0121\n",
            "Epoch: 008, Step: 2963, Loss: 0.9819\n",
            "Epoch: 008, Step: 2964, Loss: 0.9150\n",
            "Epoch: 008, Step: 2965, Loss: 1.0893\n",
            "Epoch: 008, Step: 2966, Loss: 0.8875\n",
            "Epoch: 008, Step: 2967, Loss: 0.9232\n",
            "Epoch: 008, Step: 2968, Loss: 1.1544\n",
            "Epoch: 008, Step: 2969, Loss: 1.0847\n",
            "Epoch: 008, Step: 2970, Loss: 0.9254\n",
            "Epoch: 008, Step: 2971, Loss: 0.9605\n",
            "Epoch: 008, Step: 2972, Loss: 1.0800\n",
            "Epoch: 008, Step: 2973, Loss: 0.8978\n",
            "Epoch: 008, Step: 2974, Loss: 0.9351\n",
            "Epoch: 008, Step: 2975, Loss: 0.9506\n",
            "Epoch: 008, Step: 2976, Loss: 0.8249\n",
            "Epoch: 008, Step: 2977, Loss: 1.0706\n",
            "Epoch: 008, Step: 2978, Loss: 1.1423\n",
            "Epoch: 008, Step: 2979, Loss: 0.8951\n",
            "Epoch: 008, Step: 2980, Loss: 0.9981\n",
            "Epoch: 008, Step: 2981, Loss: 0.8983\n",
            "Epoch: 008, Step: 2982, Loss: 0.9626\n",
            "Epoch: 008, Step: 2983, Loss: 0.9271\n",
            "Epoch: 008, Step: 2984, Loss: 1.1378\n",
            "Epoch: 008, Step: 2985, Loss: 0.9105\n",
            "Epoch: 008, Step: 2986, Loss: 0.8965\n",
            "Epoch: 008, Step: 2987, Loss: 0.9528\n",
            "Epoch: 008, Step: 2988, Loss: 1.0238\n",
            "Epoch: 008, Step: 2989, Loss: 1.0431\n",
            "Epoch: 008, Step: 2990, Loss: 0.8669\n",
            "Epoch: 008, Step: 2991, Loss: 1.0178\n",
            "Epoch: 008, Step: 2992, Loss: 0.9937\n",
            "Epoch: 008, Step: 2993, Loss: 0.8791\n",
            "Epoch: 008, Step: 2994, Loss: 0.8647\n",
            "Epoch: 008, Step: 2995, Loss: 1.0410\n",
            "Epoch: 008, Step: 2996, Loss: 0.8333\n",
            "Epoch: 008, Step: 2997, Loss: 0.9414\n",
            "Epoch: 008, Step: 2998, Loss: 1.0044\n",
            "Epoch: 008, Step: 2999, Loss: 1.0792\n",
            "Epoch: 008, Step: 3000, Loss: 1.0406\n",
            "Epoch: 008, Step: 3001, Loss: 0.9763\n",
            "Epoch: 008, Step: 3002, Loss: 0.9441\n",
            "Epoch: 008, Step: 3003, Loss: 0.9824\n",
            "Epoch: 008, Step: 3004, Loss: 0.9195\n",
            "Epoch: 008, Step: 3005, Loss: 0.8909\n",
            "Epoch: 008, Step: 3006, Loss: 0.9004\n",
            "Epoch: 008, Step: 3007, Loss: 1.0569\n",
            "Epoch: 008, Step: 3008, Loss: 0.9808\n",
            "Epoch: 008, Step: 3009, Loss: 0.9155\n",
            "Epoch: 008, Step: 3010, Loss: 0.8992\n",
            "Epoch: 008, Step: 3011, Loss: 1.0612\n",
            "Epoch: 008, Step: 3012, Loss: 0.9434\n",
            "Epoch: 008, Step: 3013, Loss: 0.9464\n",
            "Epoch: 008, Step: 3014, Loss: 1.0577\n",
            "Epoch: 008, Step: 3015, Loss: 0.8856\n",
            "Epoch: 008, Step: 3016, Loss: 1.1008\n",
            "Epoch: 008, Step: 3017, Loss: 1.0254\n",
            "Epoch: 008, Step: 3018, Loss: 0.9379\n",
            "Epoch: 008, Step: 3019, Loss: 0.8483\n",
            "Epoch: 008, Step: 3020, Loss: 0.8708\n",
            "Epoch: 008, Step: 3021, Loss: 0.9867\n",
            "Epoch: 008, Step: 3022, Loss: 1.1496\n",
            "Epoch: 008, Step: 3023, Loss: 1.1077\n",
            "Epoch: 008, Step: 3024, Loss: 0.9169\n",
            "Epoch: 008, Step: 3025, Loss: 0.8441\n",
            "Epoch: 008, Step: 3026, Loss: 0.9486\n",
            "Epoch: 008, Step: 3027, Loss: 0.9708\n",
            "Epoch: 008, Step: 3028, Loss: 0.9377\n",
            "Epoch: 008, Step: 3029, Loss: 0.9977\n",
            "Epoch: 008, Step: 3030, Loss: 0.8913\n",
            "Epoch: 008, Step: 3031, Loss: 1.0293\n",
            "Epoch: 008, Step: 3032, Loss: 1.0525\n",
            "Epoch: 008, Step: 3033, Loss: 0.9565\n",
            "Epoch: 008, Step: 3034, Loss: 0.8863\n",
            "Epoch: 008, Step: 3035, Loss: 1.1358\n",
            "Epoch: 008, Step: 3036, Loss: 1.0370\n",
            "Epoch: 008, Step: 3037, Loss: 0.9505\n",
            "Epoch: 008, Step: 3038, Loss: 0.9446\n",
            "Epoch: 008, Step: 3039, Loss: 0.9040\n",
            "Epoch: 008, Step: 3040, Loss: 1.0079\n",
            "Epoch: 008, Step: 3041, Loss: 1.1031\n",
            "Epoch: 008, Step: 3042, Loss: 0.7783\n",
            "Epoch: 008, Step: 3043, Loss: 1.1375\n",
            "Epoch: 008, Step: 3044, Loss: 0.9890\n",
            "Epoch: 008, Step: 3045, Loss: 1.1086\n",
            "Epoch: 008, Step: 3046, Loss: 1.1236\n",
            "Epoch: 008, Step: 3047, Loss: 1.0068\n",
            "Epoch: 008, Step: 3048, Loss: 0.9583\n",
            "Epoch: 008, Step: 3049, Loss: 0.9829\n",
            "Epoch: 008, Step: 3050, Loss: 1.0281\n",
            "Epoch: 008, Step: 3051, Loss: 1.0770\n",
            "Epoch: 008, Step: 3052, Loss: 1.0934\n",
            "Epoch: 008, Step: 3053, Loss: 0.9486\n",
            "Epoch: 008, Step: 3054, Loss: 0.8788\n",
            "Epoch: 008, Step: 3055, Loss: 0.9833\n",
            "Epoch: 008, Step: 3056, Loss: 0.9118\n",
            "Epoch: 008, Step: 3057, Loss: 0.9326\n",
            "Epoch: 008, Step: 3058, Loss: 0.9282\n",
            "Epoch: 008, Step: 3059, Loss: 0.8820\n",
            "Epoch: 008, Step: 3060, Loss: 0.8990\n",
            "Epoch: 008, Step: 3061, Loss: 0.8432\n",
            "Epoch: 008, Step: 3062, Loss: 1.0357\n",
            "Epoch: 008, Step: 3063, Loss: 1.1606\n",
            "Epoch: 008, Step: 3064, Loss: 0.9987\n",
            "Epoch: 008, Step: 3065, Loss: 0.9962\n",
            "Epoch: 008, Step: 3066, Loss: 1.0074\n",
            "Epoch: 008, Step: 3067, Loss: 0.8397\n",
            "Epoch: 008, Step: 3068, Loss: 0.8962\n",
            "Epoch: 008, Step: 3069, Loss: 0.9267\n",
            "Epoch: 008, Step: 3070, Loss: 0.8898\n",
            "Epoch: 008, Step: 3071, Loss: 0.9231\n",
            "Epoch: 008, Step: 3072, Loss: 0.8929\n",
            "Epoch: 008, Step: 3073, Loss: 1.0761\n",
            "Epoch: 008, Step: 3074, Loss: 1.1071\n",
            "Epoch: 008, Step: 3075, Loss: 0.9758\n",
            "Epoch: 008, Step: 3076, Loss: 0.9804\n",
            "Epoch: 008, Step: 3077, Loss: 1.0255\n",
            "Epoch: 008, Step: 3078, Loss: 1.0015\n",
            "Epoch: 008, Step: 3079, Loss: 1.0544\n",
            "Epoch: 008, Step: 3080, Loss: 1.0078\n",
            "Epoch: 008, Step: 3081, Loss: 1.1730\n",
            "Epoch: 008, Step: 3082, Loss: 0.9825\n",
            "Epoch: 008, Step: 3083, Loss: 1.0524\n",
            "Epoch: 008, Step: 3084, Loss: 0.8308\n",
            "Epoch: 008, Step: 3085, Loss: 1.0377\n",
            "Epoch: 008, Step: 3086, Loss: 0.9739\n",
            "Epoch: 008, Step: 3087, Loss: 0.8727\n",
            "Epoch: 008, Step: 3088, Loss: 0.9754\n",
            "Epoch: 008, Step: 3089, Loss: 0.9383\n",
            "Epoch: 008, Step: 3090, Loss: 0.8320\n",
            "Epoch: 008, Step: 3091, Loss: 0.9246\n",
            "Epoch: 008, Step: 3092, Loss: 0.8444\n",
            "Epoch: 008, Step: 3093, Loss: 0.9109\n",
            "Epoch: 008, Step: 3094, Loss: 0.8899\n",
            "Epoch: 008, Step: 3095, Loss: 1.0410\n",
            "Epoch: 008, Step: 3096, Loss: 0.9673\n",
            "Epoch: 008, Step: 3097, Loss: 1.0343\n",
            "Epoch: 008, Step: 3098, Loss: 1.0406\n",
            "Epoch: 008, Step: 3099, Loss: 1.0482\n",
            "Epoch: 008, Step: 3100, Loss: 1.1101\n",
            "Epoch: 008, Step: 3101, Loss: 1.0650\n",
            "Epoch: 008, Step: 3102, Loss: 0.9537\n",
            "Epoch: 008, Step: 3103, Loss: 0.9951\n",
            "Epoch: 008, Step: 3104, Loss: 1.0335\n",
            "Epoch: 008, Step: 3105, Loss: 0.8710\n",
            "Epoch: 008, Step: 3106, Loss: 0.8578\n",
            "Epoch: 008, Step: 3107, Loss: 0.9664\n",
            "Epoch: 008, Step: 3108, Loss: 0.9605\n",
            "Epoch: 008, Step: 3109, Loss: 0.9483\n",
            "Epoch: 008, Step: 3110, Loss: 0.9349\n",
            "Epoch: 008, Step: 3111, Loss: 0.9442\n",
            "Epoch: 008, Step: 3112, Loss: 0.8736\n",
            "Epoch: 008, Step: 3113, Loss: 0.9203\n",
            "Epoch: 008, Step: 3114, Loss: 1.0037\n",
            "Epoch: 008, Step: 3115, Loss: 1.0821\n",
            "Epoch: 008, Step: 3116, Loss: 0.9581\n",
            "Epoch: 008, Step: 3117, Loss: 0.9038\n",
            "Epoch: 008, Step: 3118, Loss: 0.9767\n",
            "Epoch: 008, Step: 3119, Loss: 0.9355\n",
            "Epoch: 008, Step: 3120, Loss: 0.9909\n",
            "Epoch: 008, Step: 3121, Loss: 0.7830\n",
            "Epoch: 008, Step: 3122, Loss: 1.0540\n",
            "Epoch: 008, Step: 3123, Loss: 0.9943\n",
            "Epoch: 008, Step: 3124, Loss: 0.9164\n",
            "Epoch: 008, Step: 3125, Loss: 0.8966\n",
            "Epoch: 008, Step: 3126, Loss: 1.0089\n",
            "Epoch: 008, Step: 3127, Loss: 0.9714\n",
            "Epoch: 008, Step: 3128, Loss: 1.0914\n",
            "Epoch: 008, Step: 3129, Loss: 0.9943\n",
            "Epoch: 008, Step: 3130, Loss: 0.8142\n",
            "Epoch: 008, Step: 3131, Loss: 0.9289\n",
            "Epoch: 008, Step: 3132, Loss: 0.8912\n",
            "Epoch: 008, Step: 3133, Loss: 0.9931\n",
            "Epoch: 008, Step: 3134, Loss: 0.8699\n",
            "Epoch: 008, Step: 3135, Loss: 0.9023\n",
            "Epoch: 008, Step: 3136, Loss: 0.9687\n",
            "Epoch: 008, Step: 3137, Loss: 1.1949\n",
            "Epoch: 008, Step: 3138, Loss: 0.8777\n",
            "Epoch: 008, Step: 3139, Loss: 1.0682\n",
            "Epoch: 008, Step: 3140, Loss: 0.8938\n",
            "Epoch: 008, Step: 3141, Loss: 0.7902\n",
            "Epoch: 008, Step: 3142, Loss: 0.9595\n",
            "Epoch: 008, Step: 3143, Loss: 0.8018\n",
            "Epoch: 008, Step: 3144, Loss: 0.9872\n",
            "Epoch: 008, Step: 3145, Loss: 0.8124\n",
            "Epoch: 008, Step: 3146, Loss: 1.0627\n",
            "Epoch: 008, Step: 3147, Loss: 0.9925\n",
            "Epoch: 008, Step: 3148, Loss: 1.0020\n",
            "Epoch: 008, Step: 3149, Loss: 0.9186\n",
            "Epoch: 008, Step: 3150, Loss: 0.9494\n",
            "Epoch: 008, Step: 3151, Loss: 1.0038\n",
            "Epoch: 008, Step: 3152, Loss: 0.9028\n",
            "Epoch: 008, Step: 3153, Loss: 0.8968\n",
            "Epoch: 008, Step: 3154, Loss: 0.9657\n",
            "Epoch: 008, Step: 3155, Loss: 0.9904\n",
            "Epoch: 008, Step: 3156, Loss: 1.0595\n",
            "Epoch: 008, Step: 3157, Loss: 0.8802\n",
            "Epoch: 008, Step: 3158, Loss: 0.9274\n",
            "Epoch: 008, Step: 3159, Loss: 0.9649\n",
            "Epoch: 008, Step: 3160, Loss: 0.9827\n",
            "Epoch: 008, Step: 3161, Loss: 1.0848\n",
            "Epoch: 008, Step: 3162, Loss: 0.9099\n",
            "Epoch: 008, Step: 3163, Loss: 1.1508\n",
            "Epoch: 008, Step: 3164, Loss: 0.9239\n",
            "Epoch: 008, Step: 3165, Loss: 1.0250\n",
            "Epoch: 008, Step: 3166, Loss: 0.9113\n",
            "Epoch: 008, Step: 3167, Loss: 0.7832\n",
            "Epoch: 008, Step: 3168, Loss: 0.9007\n",
            "Epoch: 008, Step: 3169, Loss: 1.0262\n",
            "Epoch: 008, Step: 3170, Loss: 0.9970\n",
            "Epoch: 008, Step: 3171, Loss: 1.0545\n",
            "Epoch: 008, Step: 3172, Loss: 0.9811\n",
            "Epoch: 008, Step: 3173, Loss: 0.9672\n",
            "Epoch: 008, Step: 3174, Loss: 0.8806\n",
            "Epoch: 008, Step: 3175, Loss: 0.9955\n",
            "Epoch: 008, Step: 3176, Loss: 0.9470\n",
            "Epoch: 008, Step: 3177, Loss: 0.9350\n",
            "Epoch: 008, Step: 3178, Loss: 1.1082\n",
            "Epoch: 008, Step: 3179, Loss: 0.8600\n",
            "Epoch: 008, Step: 3180, Loss: 0.8571\n",
            "Epoch: 008, Step: 3181, Loss: 0.8800\n",
            "Epoch: 008, Step: 3182, Loss: 0.8765\n",
            "Epoch: 008, Step: 3183, Loss: 1.0042\n",
            "Epoch: 008, Step: 3184, Loss: 0.9865\n",
            "Epoch: 008, Step: 3185, Loss: 1.1715\n",
            "Epoch: 008, Step: 3186, Loss: 0.8407\n",
            "Epoch: 008, Step: 3187, Loss: 0.8750\n",
            "Epoch: 008, Step: 3188, Loss: 0.8623\n",
            "Epoch: 008, Step: 3189, Loss: 1.0645\n",
            "Epoch: 008, Step: 3190, Loss: 0.8789\n",
            "Epoch: 008, Step: 3191, Loss: 1.0707\n",
            "Epoch: 008, Step: 3192, Loss: 0.9849\n",
            "Epoch: 008, Step: 3193, Loss: 0.9508\n",
            "Epoch: 008, Step: 3194, Loss: 1.0272\n",
            "Epoch: 008, Step: 3195, Loss: 1.1427\n",
            "Epoch: 008, Step: 3196, Loss: 0.9947\n",
            "Epoch: 008, Step: 3197, Loss: 0.9583\n",
            "Epoch: 008, Step: 3198, Loss: 0.8724\n",
            "Epoch: 008, Step: 3199, Loss: 0.8960\n",
            "Epoch: 008, Step: 3200, Loss: 0.9993\n",
            "Epoch: 008, Step: 3201, Loss: 1.0295\n",
            "Epoch: 008, Step: 3202, Loss: 0.9531\n",
            "Epoch: 008, Step: 3203, Loss: 0.7786\n",
            "Epoch: 008, Step: 3204, Loss: 1.0910\n",
            "Epoch: 008, Step: 3205, Loss: 0.7201\n",
            "Epoch: 008, Step: 3206, Loss: 0.8101\n",
            "Epoch: 008, Step: 3207, Loss: 0.9983\n",
            "Epoch: 008, Step: 3208, Loss: 0.9522\n",
            "Epoch: 008, Step: 3209, Loss: 0.8889\n",
            "Epoch: 008, Step: 3210, Loss: 1.0979\n",
            "Epoch: 008, Step: 3211, Loss: 1.0021\n",
            "Epoch: 008, Step: 3212, Loss: 0.9942\n",
            "Epoch: 008, Step: 3213, Loss: 0.8506\n",
            "Epoch: 008, Step: 3214, Loss: 0.9060\n",
            "Epoch: 008, Step: 3215, Loss: 1.0244\n",
            "Epoch: 008, Step: 3216, Loss: 0.8232\n",
            "Epoch: 008, Step: 3217, Loss: 1.0274\n",
            "Epoch: 008, Step: 3218, Loss: 1.0287\n",
            "Epoch: 008, Step: 3219, Loss: 0.9117\n",
            "Epoch: 008, Step: 3220, Loss: 0.7811\n",
            "Epoch: 008, Step: 3221, Loss: 0.9210\n",
            "Epoch: 008, Step: 3222, Loss: 0.8328\n",
            "Epoch: 008, Step: 3223, Loss: 0.8471\n",
            "Epoch: 008, Step: 3224, Loss: 1.0126\n",
            "Epoch: 008, Step: 3225, Loss: 0.7528\n",
            "Epoch: 008, Step: 3226, Loss: 0.9146\n",
            "Epoch: 008, Step: 3227, Loss: 0.8477\n",
            "Epoch: 008, Step: 3228, Loss: 0.8713\n",
            "Epoch: 008, Step: 3229, Loss: 1.0558\n",
            "Epoch: 008, Step: 3230, Loss: 0.9188\n",
            "Epoch: 008, Step: 3231, Loss: 0.9571\n",
            "Epoch: 008, Step: 3232, Loss: 0.8407\n",
            "Epoch: 008, Step: 3233, Loss: 1.0362\n",
            "Epoch: 008, Step: 3234, Loss: 1.2513\n",
            "Epoch: 008, Step: 3235, Loss: 0.8451\n",
            "Epoch: 008, Step: 3236, Loss: 1.0532\n",
            "Epoch: 008, Step: 3237, Loss: 0.9646\n",
            "Epoch: 008, Step: 3238, Loss: 0.8699\n",
            "Epoch: 008, Step: 3239, Loss: 0.9814\n",
            "Epoch: 008, Step: 3240, Loss: 1.0525\n",
            "Epoch: 008, Step: 3241, Loss: 0.9698\n",
            "Epoch: 008, Step: 3242, Loss: 0.9831\n",
            "Epoch: 008, Step: 3243, Loss: 1.0606\n",
            "Epoch: 008, Step: 3244, Loss: 0.8927\n",
            "Epoch: 008, Step: 3245, Loss: 1.1478\n",
            "Epoch: 008, Step: 3246, Loss: 0.9372\n",
            "Epoch: 008, Step: 3247, Loss: 1.0219\n",
            "Epoch: 008, Step: 3248, Loss: 1.0849\n",
            "Epoch: 008, Step: 3249, Loss: 0.9466\n",
            "Epoch: 008, Step: 3250, Loss: 1.0519\n",
            "Epoch: 008, Step: 3251, Loss: 0.9850\n",
            "Epoch: 008, Step: 3252, Loss: 0.8019\n",
            "Epoch: 008, Step: 3253, Loss: 0.9131\n",
            "Epoch: 008, Step: 3254, Loss: 0.9113\n",
            "Epoch: 008, Step: 3255, Loss: 0.9903\n",
            "Epoch: 008, Step: 3256, Loss: 1.0117\n",
            "Epoch: 008, Step: 3257, Loss: 1.0163\n",
            "Epoch: 008, Step: 3258, Loss: 0.8458\n",
            "Epoch: 008, Step: 3259, Loss: 1.1010\n",
            "Epoch: 008, Step: 3260, Loss: 1.0592\n",
            "Epoch: 008, Step: 3261, Loss: 0.9748\n",
            "Epoch: 008, Step: 3262, Loss: 0.8436\n",
            "Epoch: 008, Step: 3263, Loss: 0.9623\n",
            "Epoch: 008, Step: 3264, Loss: 0.9087\n",
            "Epoch: 008, Step: 3265, Loss: 1.0862\n",
            "Epoch: 008, Step: 3266, Loss: 0.9421\n",
            "Epoch: 008, Step: 3267, Loss: 1.0396\n",
            "Epoch: 008, Step: 3268, Loss: 1.0685\n",
            "Epoch: 008, Step: 3269, Loss: 0.9353\n",
            "Epoch: 008, Step: 3270, Loss: 0.8738\n",
            "Epoch: 008, Step: 3271, Loss: 0.8140\n",
            "Epoch: 008, Step: 3272, Loss: 1.0323\n",
            "Epoch: 008, Step: 3273, Loss: 0.9499\n",
            "Epoch: 008, Step: 3274, Loss: 1.0229\n",
            "Epoch: 008, Step: 3275, Loss: 1.1992\n",
            "Epoch: 008, Step: 3276, Loss: 1.0714\n",
            "Epoch: 008, Step: 3277, Loss: 0.7866\n",
            "Epoch: 008, Step: 3278, Loss: 0.9250\n",
            "Epoch: 008, Step: 3279, Loss: 1.0857\n",
            "Epoch: 008, Step: 3280, Loss: 0.9823\n",
            "Epoch: 008, Step: 3281, Loss: 1.0911\n",
            "Epoch: 008, Step: 3282, Loss: 0.8927\n",
            "Epoch: 008, Step: 3283, Loss: 1.0372\n",
            "Epoch: 008, Step: 3284, Loss: 0.8096\n",
            "Epoch: 008, Step: 3285, Loss: 0.9025\n",
            "Epoch: 008, Step: 3286, Loss: 0.8429\n",
            "Epoch: 008, Step: 3287, Loss: 0.9673\n",
            "Epoch: 008, Step: 3288, Loss: 0.9224\n",
            "Epoch: 008, Step: 3289, Loss: 0.9279\n",
            "Epoch: 008, Step: 3290, Loss: 1.0282\n",
            "Epoch: 008, Step: 3291, Loss: 1.0383\n",
            "Epoch: 008, Step: 3292, Loss: 1.0086\n",
            "Epoch: 008, Step: 3293, Loss: 0.9505\n",
            "Epoch: 008, Step: 3294, Loss: 1.0196\n",
            "Epoch: 008, Step: 3295, Loss: 1.0590\n",
            "Epoch: 008, Step: 3296, Loss: 1.1321\n",
            "Epoch: 008, Step: 3297, Loss: 0.9413\n",
            "Epoch: 008, Step: 3298, Loss: 1.0471\n",
            "Epoch: 008, Step: 3299, Loss: 0.8646\n",
            "Epoch: 008, Step: 3300, Loss: 0.9105\n",
            "Epoch: 008, Step: 3301, Loss: 0.9575\n",
            "Epoch: 008, Step: 3302, Loss: 0.8656\n",
            "Epoch: 008, Step: 3303, Loss: 1.1046\n",
            "Epoch: 008, Step: 3304, Loss: 1.1668\n",
            "Epoch: 008, Step: 3305, Loss: 0.9370\n",
            "Epoch: 008, Step: 3306, Loss: 0.9791\n",
            "Epoch: 008, Step: 3307, Loss: 0.9849\n",
            "Epoch: 008, Step: 3308, Loss: 0.8094\n",
            "Epoch: 008, Step: 3309, Loss: 0.9896\n",
            "Epoch: 008, Step: 3310, Loss: 0.9592\n",
            "Epoch: 008, Step: 3311, Loss: 0.9357\n",
            "Epoch: 008, Step: 3312, Loss: 0.9537\n",
            "Epoch: 008, Step: 3313, Loss: 0.9679\n",
            "Epoch: 008, Step: 3314, Loss: 0.9714\n",
            "Epoch: 008, Step: 3315, Loss: 0.8541\n",
            "Epoch: 008, Step: 3316, Loss: 1.0083\n",
            "Epoch: 008, Step: 3317, Loss: 0.8866\n",
            "Epoch: 008, Step: 3318, Loss: 0.8583\n",
            "Epoch: 008, Step: 3319, Loss: 0.9808\n",
            "Epoch: 008, Step: 3320, Loss: 1.0575\n",
            "Epoch: 008, Step: 3321, Loss: 0.8785\n",
            "Epoch: 008, Step: 3322, Loss: 0.8017\n",
            "Epoch: 008, Step: 3323, Loss: 0.9058\n",
            "Epoch: 008, Step: 3324, Loss: 1.0396\n",
            "Epoch: 008, Step: 3325, Loss: 0.9102\n",
            "Epoch: 008, Step: 3326, Loss: 0.9848\n",
            "Epoch: 008, Step: 3327, Loss: 1.1981\n",
            "Epoch: 008, Step: 3328, Loss: 0.9263\n",
            "Epoch: 008, Step: 3329, Loss: 0.9644\n",
            "Epoch: 008, Step: 3330, Loss: 0.8072\n",
            "Epoch: 008, Step: 3331, Loss: 1.0508\n",
            "Epoch: 008, Step: 3332, Loss: 0.8156\n",
            "Epoch: 008, Step: 3333, Loss: 1.0723\n",
            "Epoch: 008, Step: 3334, Loss: 0.9056\n",
            "Epoch: 008, Step: 3335, Loss: 0.8351\n",
            "Epoch: 008, Step: 3336, Loss: 1.0628\n",
            "Epoch: 008, Step: 3337, Loss: 0.8531\n",
            "Epoch: 008, Step: 3338, Loss: 0.9671\n",
            "Epoch: 008, Step: 3339, Loss: 0.7501\n",
            "Epoch: 008, Step: 3340, Loss: 0.9556\n",
            "Epoch: 008, Step: 3341, Loss: 0.8519\n",
            "Epoch: 008, Step: 3342, Loss: 0.7408\n",
            "Epoch: 008, Step: 3343, Loss: 0.8682\n",
            "Epoch: 008, Step: 3344, Loss: 1.0614\n",
            "Epoch: 008, Step: 3345, Loss: 0.9130\n",
            "Epoch: 008, Step: 3346, Loss: 1.1635\n",
            "Epoch: 008, Step: 3347, Loss: 1.1017\n",
            "Epoch: 008, Step: 3348, Loss: 0.9435\n",
            "Epoch: 008, Step: 3349, Loss: 1.1037\n",
            "Epoch: 008, Step: 3350, Loss: 0.7621\n",
            "Epoch: 008, Step: 3351, Loss: 0.8451\n",
            "Epoch: 008, Step: 3352, Loss: 0.8646\n",
            "Epoch: 008, Step: 3353, Loss: 0.9309\n",
            "Epoch: 008, Step: 3354, Loss: 0.8015\n",
            "Epoch: 008, Step: 3355, Loss: 0.9603\n",
            "Epoch: 008, Step: 3356, Loss: 0.8213\n",
            "Epoch: 008, Step: 3357, Loss: 1.0000\n",
            "Epoch: 008, Step: 3358, Loss: 1.0081\n",
            "Epoch: 008, Step: 3359, Loss: 0.9126\n",
            "Epoch: 008, Step: 3360, Loss: 0.7790\n",
            "Epoch: 008, Step: 3361, Loss: 1.0579\n",
            "Epoch: 008, Step: 3362, Loss: 0.9909\n",
            "Epoch: 008, Step: 3363, Loss: 0.9828\n",
            "Epoch: 008, Step: 3364, Loss: 0.8866\n",
            "Epoch: 008, Step: 3365, Loss: 0.8461\n",
            "Epoch: 008, Step: 3366, Loss: 0.8329\n",
            "Epoch: 008, Step: 3367, Loss: 0.9685\n",
            "Epoch: 008, Step: 3368, Loss: 1.0976\n",
            "Epoch: 008, Step: 3369, Loss: 1.0086\n",
            "Epoch: 008, Step: 3370, Loss: 0.8024\n",
            "Epoch: 008, Step: 3371, Loss: 0.9721\n",
            "Epoch: 008, Step: 3372, Loss: 0.9594\n",
            "Epoch: 008, Step: 3373, Loss: 0.9497\n",
            "Epoch: 008, Step: 3374, Loss: 1.0098\n",
            "Epoch: 008, Step: 3375, Loss: 0.8875\n",
            "Epoch: 008, Step: 3376, Loss: 0.9284\n",
            "Epoch: 008, Step: 3377, Loss: 0.8383\n",
            "Epoch: 008, Step: 3378, Loss: 0.8738\n",
            "Epoch: 008, Step: 3379, Loss: 0.8072\n",
            "Epoch: 008, Step: 3380, Loss: 0.8493\n",
            "Epoch: 008, Step: 3381, Loss: 0.8365\n",
            "Epoch: 008, Step: 3382, Loss: 1.0601\n",
            "Epoch: 008, Step: 3383, Loss: 0.9330\n",
            "Epoch: 008, Step: 3384, Loss: 0.9877\n",
            "Epoch: 008, Step: 3385, Loss: 0.8855\n",
            "Epoch: 008, Step: 3386, Loss: 0.9914\n",
            "Epoch: 008, Step: 3387, Loss: 0.9864\n",
            "Epoch: 008, Step: 3388, Loss: 0.9446\n",
            "Epoch: 008, Step: 3389, Loss: 0.9545\n",
            "Epoch: 008, Step: 3390, Loss: 0.8153\n",
            "Epoch: 008, Step: 3391, Loss: 0.7858\n",
            "Epoch: 008, Step: 3392, Loss: 0.8979\n",
            "Epoch: 008, Step: 3393, Loss: 1.0765\n",
            "Epoch: 008, Step: 3394, Loss: 0.8035\n",
            "Epoch: 008, Step: 3395, Loss: 1.0021\n",
            "Epoch: 008, Step: 3396, Loss: 0.9061\n",
            "Epoch: 008, Step: 3397, Loss: 0.7589\n",
            "Epoch: 008, Step: 3398, Loss: 0.9751\n",
            "Epoch: 008, Step: 3399, Loss: 0.9271\n",
            "Epoch: 008, Step: 3400, Loss: 0.8213\n",
            "Epoch: 008, Step: 3401, Loss: 0.8838\n",
            "Epoch: 008, Step: 3402, Loss: 1.1851\n",
            "Epoch: 008, Step: 3403, Loss: 0.9029\n",
            "Epoch: 008, Step: 3404, Loss: 0.8852\n",
            "Epoch: 008, Step: 3405, Loss: 0.9220\n",
            "Epoch: 008, Step: 3406, Loss: 0.9521\n",
            "Epoch: 008, Step: 3407, Loss: 0.8739\n",
            "Epoch: 008, Step: 3408, Loss: 0.7895\n",
            "Epoch: 008, Step: 3409, Loss: 0.9512\n",
            "Epoch: 008, Step: 3410, Loss: 0.9603\n",
            "Epoch: 008, Step: 3411, Loss: 0.8962\n",
            "Epoch: 008, Step: 3412, Loss: 0.8753\n",
            "Epoch: 008, Step: 3413, Loss: 0.9068\n",
            "Epoch: 008, Step: 3414, Loss: 0.9488\n",
            "Epoch: 008, Step: 3415, Loss: 0.9378\n",
            "Epoch: 008, Step: 3416, Loss: 1.0259\n",
            "Epoch: 008, Step: 3417, Loss: 1.0267\n",
            "Epoch: 008, Step: 3418, Loss: 0.8664\n",
            "Epoch: 008, Step: 3419, Loss: 0.9283\n",
            "Epoch: 008, Step: 3420, Loss: 0.9591\n",
            "Epoch: 008, Step: 3421, Loss: 1.1721\n",
            "Epoch: 008, Step: 3422, Loss: 0.8397\n",
            "Epoch: 008, Step: 3423, Loss: 1.0252\n",
            "Epoch: 008, Step: 3424, Loss: 1.0511\n",
            "Epoch: 008, Step: 3425, Loss: 1.0013\n",
            "Epoch: 008, Step: 3426, Loss: 1.0752\n",
            "Epoch: 008, Step: 3427, Loss: 0.9869\n",
            "Epoch: 008, Step: 3428, Loss: 1.0487\n",
            "Epoch: 008, Step: 3429, Loss: 0.8194\n",
            "Epoch: 008, Step: 3430, Loss: 0.9196\n",
            "Epoch: 008, Step: 3431, Loss: 1.0102\n",
            "Epoch: 008, Step: 3432, Loss: 0.9800\n",
            "Epoch: 008, Step: 3433, Loss: 1.0037\n",
            "Epoch: 008, Step: 3434, Loss: 1.0039\n",
            "Epoch: 008, Step: 3435, Loss: 0.9954\n",
            "Epoch: 008, Step: 3436, Loss: 0.9252\n",
            "Epoch: 008, Step: 3437, Loss: 0.9962\n",
            "Epoch: 008, Step: 3438, Loss: 0.7482\n",
            "Epoch: 008, Step: 3439, Loss: 0.8573\n",
            "Epoch: 008, Step: 3440, Loss: 1.1279\n",
            "Epoch: 008, Step: 3441, Loss: 0.8239\n",
            "Epoch: 008, Step: 3442, Loss: 1.0861\n",
            "Epoch: 008, Step: 3443, Loss: 0.9025\n",
            "Epoch: 008, Step: 3444, Loss: 0.9938\n",
            "Epoch: 008, Step: 3445, Loss: 0.9889\n",
            "Epoch: 008, Step: 3446, Loss: 0.9236\n",
            "Epoch: 008, Step: 3447, Loss: 0.9035\n",
            "Epoch: 008, Step: 3448, Loss: 0.9748\n",
            "Epoch: 008, Step: 3449, Loss: 0.8880\n",
            "Epoch: 008, Step: 3450, Loss: 0.9352\n",
            "Epoch: 008, Step: 3451, Loss: 0.9312\n",
            "Epoch: 008, Step: 3452, Loss: 1.0141\n",
            "Epoch: 008, Step: 3453, Loss: 0.8463\n",
            "Epoch: 008, Step: 3454, Loss: 0.9573\n",
            "Epoch: 008, Step: 3455, Loss: 0.8741\n",
            "Epoch: 008, Step: 3456, Loss: 0.8403\n",
            "Epoch: 008, Step: 3457, Loss: 0.8608\n",
            "Epoch: 008, Step: 3458, Loss: 0.8431\n",
            "Epoch: 008, Step: 3459, Loss: 0.9298\n",
            "Epoch: 008, Step: 3460, Loss: 0.8388\n",
            "Epoch: 008, Step: 3461, Loss: 0.8096\n",
            "Epoch: 008, Step: 3462, Loss: 1.0181\n",
            "Epoch: 008, Step: 3463, Loss: 0.8785\n",
            "Epoch: 008, Step: 3464, Loss: 0.8495\n",
            "Epoch: 008, Step: 3465, Loss: 0.9712\n",
            "Epoch: 008, Step: 3466, Loss: 0.8374\n",
            "Epoch: 008, Step: 3467, Loss: 0.7998\n",
            "Epoch: 008, Step: 3468, Loss: 0.8129\n",
            "Epoch: 008, Step: 3469, Loss: 0.9767\n",
            "Epoch: 008, Step: 3470, Loss: 0.8602\n",
            "Epoch: 008, Step: 3471, Loss: 0.9814\n",
            "Epoch: 008, Step: 3472, Loss: 0.9418\n",
            "Epoch: 008, Step: 3473, Loss: 0.8175\n",
            "Epoch: 008, Step: 3474, Loss: 0.9186\n",
            "Epoch: 008, Step: 3475, Loss: 0.8524\n",
            "Epoch: 008, Step: 3476, Loss: 0.9257\n",
            "Epoch: 008, Step: 3477, Loss: 0.8368\n",
            "Epoch: 008, Step: 3478, Loss: 0.7689\n",
            "Epoch: 008, Step: 3479, Loss: 0.8590\n",
            "Epoch: 008, Step: 3480, Loss: 0.8867\n",
            "Epoch: 008, Step: 3481, Loss: 0.8610\n",
            "Epoch: 008, Step: 3482, Loss: 0.8594\n",
            "Epoch: 008, Step: 3483, Loss: 1.0633\n",
            "Epoch: 008, Step: 3484, Loss: 1.0176\n",
            "Epoch: 008, Step: 3485, Loss: 1.0051\n",
            "Epoch: 008, Step: 3486, Loss: 1.0489\n",
            "Epoch: 008, Step: 3487, Loss: 0.8697\n",
            "Epoch: 008, Step: 3488, Loss: 0.8668\n",
            "Epoch: 008, Step: 3489, Loss: 0.9867\n",
            "Epoch: 008, Step: 3490, Loss: 0.9241\n",
            "Epoch: 008, Step: 3491, Loss: 0.8986\n",
            "Epoch: 008, Step: 3492, Loss: 1.0074\n",
            "Epoch: 008, Step: 3493, Loss: 0.8538\n",
            "Epoch: 008, Step: 3494, Loss: 1.1548\n",
            "Epoch: 008, Step: 3495, Loss: 1.0650\n",
            "Epoch: 008, Step: 3496, Loss: 1.2176\n",
            "Epoch: 008, Step: 3497, Loss: 0.9665\n",
            "Epoch: 008, Step: 3498, Loss: 0.8750\n",
            "Epoch: 008, Step: 3499, Loss: 1.0399\n",
            "Epoch: 008, Step: 3500, Loss: 1.0663\n",
            "Epoch: 008, Step: 3501, Loss: 1.0243\n",
            "Epoch: 008, Step: 3502, Loss: 0.9948\n",
            "Epoch: 008, Step: 3503, Loss: 1.0723\n",
            "Epoch: 008, Step: 3504, Loss: 0.8766\n",
            "Epoch: 008, Step: 3505, Loss: 0.9233\n",
            "Epoch: 008, Step: 3506, Loss: 0.8309\n",
            "Epoch: 008, Step: 3507, Loss: 0.9697\n",
            "Epoch: 008, Step: 3508, Loss: 0.8643\n",
            "Epoch: 008, Step: 3509, Loss: 0.9567\n",
            "Epoch: 008, Step: 3510, Loss: 1.0370\n",
            "Epoch: 008, Step: 3511, Loss: 0.9724\n",
            "Epoch: 008, Step: 3512, Loss: 1.0578\n",
            "Epoch: 008, Step: 3513, Loss: 0.9193\n",
            "Epoch: 008, Step: 3514, Loss: 0.8085\n",
            "Epoch: 008, Step: 3515, Loss: 1.0364\n",
            "Epoch: 008, Step: 3516, Loss: 0.9747\n",
            "Epoch: 008, Step: 3517, Loss: 1.0281\n",
            "Epoch: 008, Step: 3518, Loss: 0.9385\n",
            "Epoch: 008, Step: 3519, Loss: 0.9675\n",
            "Epoch: 008, Step: 3520, Loss: 1.0033\n",
            "Epoch: 008, Step: 3521, Loss: 0.9984\n",
            "Epoch: 008, Step: 3522, Loss: 0.9873\n",
            "Epoch: 008, Step: 3523, Loss: 0.8509\n",
            "Epoch: 008, Step: 3524, Loss: 0.9027\n",
            "Epoch: 008, Step: 3525, Loss: 0.8319\n",
            "Epoch: 008, Step: 3526, Loss: 0.9546\n",
            "Epoch: 008, Step: 3527, Loss: 1.0206\n",
            "Epoch: 008, Step: 3528, Loss: 0.8826\n",
            "Epoch: 008, Step: 3529, Loss: 0.8572\n",
            "Epoch: 008, Step: 3530, Loss: 1.0095\n",
            "Epoch: 008, Step: 3531, Loss: 0.9053\n",
            "Epoch: 008, Step: 3532, Loss: 0.7878\n",
            "Epoch: 008, Step: 3533, Loss: 0.9575\n",
            "Epoch: 008, Step: 3534, Loss: 0.8456\n",
            "Epoch: 008, Step: 3535, Loss: 1.0744\n",
            "Epoch: 008, Step: 3536, Loss: 1.1035\n",
            "Epoch: 008, Step: 3537, Loss: 0.8972\n",
            "Epoch: 008, Step: 3538, Loss: 1.0774\n",
            "Epoch: 008, Step: 3539, Loss: 1.0285\n",
            "Epoch: 008, Step: 3540, Loss: 0.9148\n",
            "Epoch: 008, Step: 3541, Loss: 0.9690\n",
            "Epoch: 008, Step: 3542, Loss: 0.8774\n",
            "Epoch: 008, Step: 3543, Loss: 1.0312\n",
            "Epoch: 008, Step: 3544, Loss: 0.8058\n",
            "Epoch: 008, Step: 3545, Loss: 0.8859\n",
            "Epoch: 008, Step: 3546, Loss: 0.9307\n",
            "Epoch: 008, Step: 3547, Loss: 0.9747\n",
            "Epoch: 008, Step: 3548, Loss: 0.9495\n",
            "Epoch: 008, Step: 3549, Loss: 0.8839\n",
            "Epoch: 008, Step: 3550, Loss: 1.0343\n",
            "Epoch: 008, Step: 3551, Loss: 0.9441\n",
            "Epoch: 008, Step: 3552, Loss: 0.8993\n",
            "Epoch: 008, Step: 3553, Loss: 0.8092\n",
            "Epoch: 008, Step: 3554, Loss: 1.0207\n",
            "Epoch: 008, Step: 3555, Loss: 1.0006\n",
            "Epoch: 008, Step: 3556, Loss: 0.9453\n",
            "Epoch: 008, Step: 3557, Loss: 1.0505\n",
            "Epoch: 008, Step: 3558, Loss: 0.9331\n",
            "Epoch: 008, Step: 3559, Loss: 1.0107\n",
            "Epoch: 008, Step: 3560, Loss: 0.9370\n",
            "Epoch: 008, Step: 3561, Loss: 1.1392\n",
            "Epoch: 008, Step: 3562, Loss: 1.0752\n",
            "Epoch: 008, Step: 3563, Loss: 1.0227\n",
            "Epoch: 008, Step: 3564, Loss: 0.9414\n",
            "Epoch: 008, Step: 3565, Loss: 1.0776\n",
            "Epoch: 008, Step: 3566, Loss: 1.0429\n",
            "Epoch: 008, Step: 3567, Loss: 1.0582\n",
            "Epoch: 008, Step: 3568, Loss: 1.0770\n",
            "Epoch: 008, Step: 3569, Loss: 1.0752\n",
            "Epoch: 008, Step: 3570, Loss: 0.8954\n",
            "Epoch: 008, Step: 3571, Loss: 0.9779\n",
            "Epoch: 008, Step: 3572, Loss: 1.0880\n",
            "Epoch: 008, Step: 3573, Loss: 0.9439\n",
            "Epoch: 008, Step: 3574, Loss: 1.0765\n",
            "Epoch: 008, Step: 3575, Loss: 0.7878\n",
            "Epoch: 008, Step: 3576, Loss: 0.9406\n",
            "Epoch: 008, Step: 3577, Loss: 1.0209\n",
            "Epoch: 008, Step: 3578, Loss: 0.9800\n",
            "Epoch: 008, Step: 3579, Loss: 0.9642\n",
            "Epoch: 008, Step: 3580, Loss: 1.0558\n",
            "Epoch: 008, Step: 3581, Loss: 0.8690\n",
            "Epoch: 008, Step: 3582, Loss: 1.0272\n",
            "Epoch: 008, Step: 3583, Loss: 0.9160\n",
            "Epoch: 008, Step: 3584, Loss: 0.8463\n",
            "Epoch: 008, Step: 3585, Loss: 0.8377\n",
            "Epoch: 008, Step: 3586, Loss: 0.9625\n",
            "Epoch: 008, Step: 3587, Loss: 0.9062\n",
            "Epoch: 008, Step: 3588, Loss: 0.8944\n",
            "Epoch: 008, Step: 3589, Loss: 1.0005\n",
            "Epoch: 008, Step: 3590, Loss: 0.8960\n",
            "Epoch: 008, Step: 3591, Loss: 0.9963\n",
            "Epoch: 008, Step: 3592, Loss: 0.9954\n",
            "Epoch: 008, Step: 3593, Loss: 1.0626\n",
            "Epoch: 008, Step: 3594, Loss: 0.9904\n",
            "Epoch: 008, Step: 3595, Loss: 0.9233\n",
            "Epoch: 008, Step: 3596, Loss: 0.8309\n",
            "Epoch: 008, Step: 3597, Loss: 1.0823\n",
            "Epoch: 008, Step: 3598, Loss: 0.9078\n",
            "Epoch: 008, Step: 3599, Loss: 1.1674\n",
            "Epoch: 008, Step: 3600, Loss: 0.9196\n",
            "Epoch: 008, Step: 3601, Loss: 1.1134\n",
            "Epoch: 008, Step: 3602, Loss: 0.9921\n",
            "Epoch: 008, Step: 3603, Loss: 1.0028\n",
            "Epoch: 008, Step: 3604, Loss: 0.8466\n",
            "Epoch: 008, Step: 3605, Loss: 0.8184\n",
            "Epoch: 008, Step: 3606, Loss: 1.0222\n",
            "Epoch: 008, Step: 3607, Loss: 0.9188\n",
            "Epoch: 008, Step: 3608, Loss: 1.1017\n",
            "Epoch: 008, Step: 3609, Loss: 1.0251\n",
            "Epoch: 008, Step: 3610, Loss: 0.7913\n",
            "Epoch: 008, Step: 3611, Loss: 0.7513\n",
            "Epoch: 008, Step: 3612, Loss: 0.9537\n",
            "Epoch: 008, Step: 3613, Loss: 0.8483\n",
            "Epoch: 008, Step: 3614, Loss: 0.9282\n",
            "Epoch: 008, Step: 3615, Loss: 0.9450\n",
            "Epoch: 008, Step: 3616, Loss: 0.9458\n",
            "Epoch: 008, Step: 3617, Loss: 0.8634\n",
            "Epoch: 008, Step: 3618, Loss: 0.9259\n",
            "Epoch: 008, Step: 3619, Loss: 1.0023\n",
            "Epoch: 008, Step: 3620, Loss: 1.1348\n",
            "Epoch: 008, Step: 3621, Loss: 0.9418\n",
            "Epoch: 008, Step: 3622, Loss: 0.9064\n",
            "Epoch: 008, Step: 3623, Loss: 0.9230\n",
            "Epoch: 008, Step: 3624, Loss: 0.9849\n",
            "Epoch: 008, Step: 3625, Loss: 0.9770\n",
            "Epoch: 008, Step: 3626, Loss: 1.0358\n",
            "Epoch: 008, Step: 3627, Loss: 1.0845\n",
            "Epoch: 008, Step: 3628, Loss: 0.8797\n",
            "Epoch: 008, Step: 3629, Loss: 0.9444\n",
            "Epoch: 008, Step: 3630, Loss: 0.9799\n",
            "Epoch: 008, Step: 3631, Loss: 0.9789\n",
            "Epoch: 008, Step: 3632, Loss: 0.8979\n",
            "Epoch: 008, Step: 3633, Loss: 0.9003\n",
            "Epoch: 008, Step: 3634, Loss: 1.1145\n",
            "Epoch: 008, Step: 3635, Loss: 0.9236\n",
            "Epoch: 008, Step: 3636, Loss: 1.0009\n",
            "Epoch: 008, Step: 3637, Loss: 1.0942\n",
            "Epoch: 008, Step: 3638, Loss: 0.9103\n",
            "Epoch: 008, Step: 3639, Loss: 1.0009\n",
            "Epoch: 008, Step: 3640, Loss: 0.9188\n",
            "Epoch: 008, Step: 3641, Loss: 0.9964\n",
            "Epoch: 008, Step: 3642, Loss: 0.8954\n",
            "Epoch: 008, Step: 3643, Loss: 0.8949\n",
            "Epoch: 008, Step: 3644, Loss: 0.9662\n",
            "Epoch: 008, Step: 3645, Loss: 0.8683\n",
            "Epoch: 008, Step: 3646, Loss: 1.0556\n",
            "Epoch: 008, Step: 3647, Loss: 0.9502\n",
            "Epoch: 008, Step: 3648, Loss: 0.9591\n",
            "Epoch: 008, Step: 3649, Loss: 1.0527\n",
            "Epoch: 008, Step: 3650, Loss: 1.1229\n",
            "Epoch: 008, Step: 3651, Loss: 0.8308\n",
            "Epoch: 008, Step: 3652, Loss: 0.7534\n",
            "Epoch: 008, Step: 3653, Loss: 0.7747\n",
            "Epoch: 008, Step: 3654, Loss: 1.1013\n",
            "Epoch: 008, Step: 3655, Loss: 0.9046\n",
            "Epoch: 008, Step: 3656, Loss: 1.0897\n",
            "Epoch: 008, Step: 3657, Loss: 0.9957\n",
            "Epoch: 008, Step: 3658, Loss: 0.9682\n",
            "Epoch: 008, Step: 3659, Loss: 0.9715\n",
            "Epoch: 008, Step: 3660, Loss: 0.9348\n",
            "Epoch: 008, Step: 3661, Loss: 1.0905\n",
            "Epoch: 008, Step: 3662, Loss: 1.0399\n",
            "Epoch: 008, Step: 3663, Loss: 0.8849\n",
            "Epoch: 008, Step: 000, Val Loss: 1.0203\n",
            "Epoch: 008, Step: 001, Val Loss: 0.9847\n",
            "Epoch: 008, Step: 002, Val Loss: 1.1315\n",
            "Epoch: 008, Step: 003, Val Loss: 1.1408\n",
            "Epoch: 008, Step: 004, Val Loss: 1.0899\n",
            "Epoch: 008, Step: 005, Val Loss: 0.9990\n",
            "Epoch: 008, Step: 006, Val Loss: 1.0632\n",
            "Epoch: 008, Step: 007, Val Loss: 1.1853\n",
            "Epoch: 008, Step: 008, Val Loss: 1.0426\n",
            "Epoch: 008, Step: 009, Val Loss: 1.0399\n",
            "Epoch: 008, Step: 010, Val Loss: 1.1888\n",
            "Epoch: 008, Step: 011, Val Loss: 1.1503\n",
            "Epoch: 008, Step: 012, Val Loss: 1.1647\n",
            "Epoch: 008, Step: 013, Val Loss: 1.0847\n",
            "Epoch: 008, Step: 014, Val Loss: 1.2516\n",
            "Epoch: 008, Step: 015, Val Loss: 1.2131\n",
            "Epoch: 008, Step: 016, Val Loss: 1.1188\n",
            "Epoch: 008, Step: 017, Val Loss: 1.1458\n",
            "Epoch: 008, Step: 018, Val Loss: 1.2493\n",
            "Epoch: 008, Step: 019, Val Loss: 1.2724\n",
            "Epoch: 008, Step: 020, Val Loss: 1.1073\n",
            "Epoch: 008, Step: 021, Val Loss: 1.0746\n",
            "Epoch: 008, Step: 022, Val Loss: 1.0752\n",
            "Epoch: 008, Step: 023, Val Loss: 1.1550\n",
            "Epoch: 008, Step: 024, Val Loss: 0.9964\n",
            "Epoch: 008, Step: 025, Val Loss: 1.0503\n",
            "Epoch: 008, Step: 026, Val Loss: 1.0699\n",
            "Epoch: 008, Step: 027, Val Loss: 1.1136\n",
            "Epoch: 008, Step: 028, Val Loss: 1.0892\n",
            "Epoch: 008, Step: 029, Val Loss: 1.0812\n",
            "Epoch: 008, Step: 030, Val Loss: 1.0884\n",
            "Epoch: 008, Step: 031, Val Loss: 1.0582\n",
            "Epoch: 008, Step: 032, Val Loss: 1.0550\n",
            "Epoch: 008, Step: 033, Val Loss: 1.1761\n",
            "Epoch: 008, Step: 034, Val Loss: 0.9379\n",
            "Epoch: 008, Step: 035, Val Loss: 0.8984\n",
            "Epoch: 008, Step: 036, Val Loss: 1.0198\n",
            "Epoch: 008, Step: 037, Val Loss: 1.1089\n",
            "Epoch: 008, Step: 038, Val Loss: 1.1692\n",
            "Epoch: 008, Step: 039, Val Loss: 1.1832\n",
            "Epoch: 008, Step: 040, Val Loss: 1.0264\n",
            "Epoch: 008, Step: 041, Val Loss: 1.0681\n",
            "Epoch: 008, Step: 042, Val Loss: 1.0367\n",
            "Epoch: 008, Step: 043, Val Loss: 0.9468\n",
            "Epoch: 008, Step: 044, Val Loss: 1.2372\n",
            "Epoch: 008, Step: 045, Val Loss: 1.0865\n",
            "Epoch: 008, Step: 046, Val Loss: 1.2291\n",
            "Epoch: 008, Step: 047, Val Loss: 1.2150\n",
            "Epoch: 008, Step: 048, Val Loss: 1.0452\n",
            "Epoch: 008, Step: 049, Val Loss: 1.1537\n",
            "Epoch: 008, Step: 050, Val Loss: 1.0308\n",
            "Epoch: 008, Step: 051, Val Loss: 1.0428\n",
            "Epoch: 008, Step: 052, Val Loss: 1.0369\n",
            "Epoch: 008, Step: 053, Val Loss: 1.1862\n",
            "Epoch: 008, Step: 054, Val Loss: 1.0641\n",
            "Epoch: 008, Step: 055, Val Loss: 1.3206\n",
            "Epoch: 008, Step: 056, Val Loss: 1.0858\n",
            "Epoch: 008, Step: 057, Val Loss: 1.0790\n",
            "Epoch: 008, Step: 058, Val Loss: 1.0238\n",
            "Epoch: 008, Step: 059, Val Loss: 1.0073\n",
            "Epoch: 008, Step: 060, Val Loss: 1.1442\n",
            "Epoch: 008, Step: 061, Val Loss: 1.0389\n",
            "Epoch: 008, Step: 062, Val Loss: 1.1976\n",
            "Epoch: 008, Step: 063, Val Loss: 1.0741\n",
            "Epoch: 008, Step: 064, Val Loss: 0.9297\n",
            "Epoch: 008, Step: 065, Val Loss: 1.1265\n",
            "Epoch: 008, Step: 066, Val Loss: 1.0186\n",
            "Epoch: 008, Step: 067, Val Loss: 1.1034\n",
            "Epoch: 008, Step: 068, Val Loss: 1.0810\n",
            "Epoch: 008, Step: 069, Val Loss: 1.1341\n",
            "Epoch: 008, Step: 070, Val Loss: 1.0691\n",
            "Epoch: 008, Step: 071, Val Loss: 1.1090\n",
            "Epoch: 008, Step: 072, Val Loss: 1.0645\n",
            "Epoch: 008, Step: 073, Val Loss: 1.1578\n",
            "Epoch: 008, Step: 074, Val Loss: 1.0536\n",
            "Epoch: 008, Step: 075, Val Loss: 1.0377\n",
            "Epoch: 008, Step: 076, Val Loss: 0.9907\n",
            "Epoch: 008, Step: 077, Val Loss: 0.9900\n",
            "Epoch: 008, Step: 078, Val Loss: 1.0666\n",
            "Epoch: 008, Step: 079, Val Loss: 1.0392\n",
            "Epoch: 008, Step: 080, Val Loss: 0.9628\n",
            "Epoch: 008, Step: 081, Val Loss: 1.1119\n",
            "Epoch: 008, Step: 082, Val Loss: 0.9277\n",
            "Epoch: 008, Step: 083, Val Loss: 1.1283\n",
            "Epoch: 008, Step: 084, Val Loss: 1.0505\n",
            "Epoch: 008, Step: 085, Val Loss: 1.2025\n",
            "Epoch: 008, Step: 086, Val Loss: 1.0555\n",
            "Epoch: 008, Step: 087, Val Loss: 0.9963\n",
            "Epoch: 008, Step: 088, Val Loss: 1.1206\n",
            "Epoch: 008, Step: 089, Val Loss: 1.2244\n",
            "Epoch: 008, Step: 090, Val Loss: 1.1132\n",
            "Epoch: 008, Step: 091, Val Loss: 1.1322\n",
            "Epoch: 008, Step: 092, Val Loss: 1.0598\n",
            "Epoch: 008, Step: 093, Val Loss: 1.0457\n",
            "Epoch: 008, Step: 094, Val Loss: 0.9582\n",
            "Epoch: 008, Step: 095, Val Loss: 1.1023\n",
            "Epoch: 008, Step: 096, Val Loss: 1.0318\n",
            "Epoch: 008, Step: 097, Val Loss: 1.1475\n",
            "Epoch: 008, Step: 098, Val Loss: 1.0470\n",
            "Epoch: 008, Step: 099, Val Loss: 1.0523\n",
            "Epoch: 008, Step: 100, Val Loss: 1.0075\n",
            "Epoch: 008, Step: 101, Val Loss: 1.1117\n",
            "Epoch: 008, Step: 102, Val Loss: 1.1396\n",
            "Epoch: 008, Step: 103, Val Loss: 0.9535\n",
            "Epoch: 008, Step: 104, Val Loss: 1.0664\n",
            "Epoch: 008, Step: 105, Val Loss: 1.1091\n",
            "Epoch: 008, Step: 106, Val Loss: 1.0063\n",
            "Epoch: 008, Step: 107, Val Loss: 1.0035\n",
            "Epoch: 008, Step: 108, Val Loss: 1.1641\n",
            "Epoch: 008, Step: 109, Val Loss: 0.9756\n",
            "Epoch: 008, Step: 110, Val Loss: 1.1747\n",
            "Epoch: 008, Step: 111, Val Loss: 1.1421\n",
            "Epoch: 008, Step: 112, Val Loss: 0.9866\n",
            "Epoch: 008, Step: 113, Val Loss: 1.0590\n",
            "Epoch: 008, Step: 114, Val Loss: 1.2839\n",
            "Epoch: 008, Step: 115, Val Loss: 1.0312\n",
            "Epoch: 008, Step: 116, Val Loss: 1.2294\n",
            "Epoch: 008, Step: 117, Val Loss: 1.0826\n",
            "Epoch: 008, Step: 118, Val Loss: 1.1982\n",
            "Epoch: 008, Step: 119, Val Loss: 1.0905\n",
            "Epoch: 008, Step: 120, Val Loss: 1.1685\n",
            "Epoch: 008, Step: 121, Val Loss: 1.1049\n",
            "Epoch: 008, Step: 122, Val Loss: 1.0039\n",
            "Epoch: 008, Step: 123, Val Loss: 0.9051\n",
            "Epoch: 008, Step: 124, Val Loss: 1.0999\n",
            "Epoch: 008, Step: 125, Val Loss: 1.1406\n",
            "Epoch: 008, Step: 126, Val Loss: 0.9595\n",
            "Epoch: 008, Step: 127, Val Loss: 1.0099\n",
            "Epoch: 008, Step: 128, Val Loss: 0.9813\n",
            "Epoch: 008, Step: 129, Val Loss: 1.0460\n",
            "Epoch: 008, Step: 130, Val Loss: 0.9883\n",
            "Epoch: 008, Step: 131, Val Loss: 0.9594\n",
            "Epoch: 008, Step: 132, Val Loss: 1.1555\n",
            "Epoch: 008, Step: 133, Val Loss: 0.9707\n",
            "Epoch: 008, Step: 134, Val Loss: 1.0526\n",
            "Epoch: 008, Step: 135, Val Loss: 1.0762\n",
            "Epoch: 008, Step: 136, Val Loss: 1.2909\n",
            "Epoch: 008, Step: 137, Val Loss: 0.9860\n",
            "Epoch: 008, Step: 138, Val Loss: 0.9895\n",
            "Epoch: 008, Step: 139, Val Loss: 1.0358\n",
            "Epoch: 008, Step: 140, Val Loss: 1.1042\n",
            "Epoch: 008, Step: 141, Val Loss: 0.9782\n",
            "Epoch: 008, Step: 142, Val Loss: 1.1230\n",
            "Epoch: 008, Step: 143, Val Loss: 1.0669\n",
            "Epoch: 008, Step: 144, Val Loss: 0.9349\n",
            "Epoch: 008, Step: 145, Val Loss: 1.0103\n",
            "Epoch: 008, Step: 146, Val Loss: 1.1434\n",
            "Epoch: 008, Step: 147, Val Loss: 1.0529\n",
            "Epoch: 008, Step: 148, Val Loss: 1.0083\n",
            "Epoch: 008, Step: 149, Val Loss: 1.0447\n",
            "Epoch: 008, Step: 150, Val Loss: 1.0839\n",
            "Epoch: 008, Step: 151, Val Loss: 0.9302\n",
            "Epoch: 008, Step: 152, Val Loss: 1.0845\n",
            "Epoch: 008, Step: 153, Val Loss: 1.1429\n",
            "Epoch: 008, Step: 154, Val Loss: 0.9637\n",
            "Epoch: 008, Step: 155, Val Loss: 1.2049\n",
            "Epoch: 008, Step: 156, Val Loss: 1.1311\n",
            "Epoch: 008, Step: 157, Val Loss: 1.0600\n",
            "Epoch: 008, Step: 158, Val Loss: 0.9943\n",
            "Epoch: 008, Step: 159, Val Loss: 1.0528\n",
            "Epoch: 008, Step: 160, Val Loss: 1.0419\n",
            "Epoch: 008, Step: 161, Val Loss: 1.2024\n",
            "Epoch: 008, Step: 162, Val Loss: 1.0263\n",
            "Epoch: 008, Step: 163, Val Loss: 0.9257\n",
            "Epoch: 008, Step: 164, Val Loss: 1.0132\n",
            "Epoch: 008, Step: 165, Val Loss: 1.1585\n",
            "Epoch: 008, Step: 166, Val Loss: 1.1292\n",
            "Epoch: 008, Step: 167, Val Loss: 1.1339\n",
            "Epoch: 008, Step: 168, Val Loss: 1.1254\n",
            "Epoch: 008, Step: 169, Val Loss: 0.9953\n",
            "Epoch: 008, Step: 170, Val Loss: 1.1050\n",
            "Epoch: 008, Step: 171, Val Loss: 1.1725\n",
            "Epoch: 008, Step: 172, Val Loss: 1.0168\n",
            "Epoch: 008, Step: 173, Val Loss: 1.2134\n",
            "Epoch: 008, Step: 174, Val Loss: 1.1279\n",
            "Epoch: 008, Step: 175, Val Loss: 1.0858\n",
            "Epoch: 008, Step: 176, Val Loss: 1.1663\n",
            "Epoch: 008, Step: 177, Val Loss: 0.9493\n",
            "Epoch: 008, Step: 178, Val Loss: 1.0212\n",
            "Epoch: 008, Step: 179, Val Loss: 1.0744\n",
            "Epoch: 008, Step: 180, Val Loss: 0.9388\n",
            "Epoch: 008, Step: 181, Val Loss: 1.1129\n",
            "Epoch: 008, Step: 182, Val Loss: 0.9836\n",
            "Epoch: 008, Step: 183, Val Loss: 1.0610\n",
            "Epoch: 008, Step: 184, Val Loss: 1.2310\n",
            "Epoch: 008, Step: 185, Val Loss: 0.9462\n",
            "Epoch: 008, Step: 186, Val Loss: 1.0335\n",
            "Epoch: 008, Step: 187, Val Loss: 1.1184\n",
            "Epoch: 008, Step: 188, Val Loss: 1.0683\n",
            "Epoch: 008, Step: 189, Val Loss: 0.9681\n",
            "Epoch: 008, Step: 190, Val Loss: 1.1153\n",
            "Epoch: 008, Step: 191, Val Loss: 1.0532\n",
            "Epoch: 008, Step: 192, Val Loss: 0.9006\n",
            "Epoch: 008, Step: 193, Val Loss: 0.9540\n",
            "Epoch: 008, Step: 194, Val Loss: 0.8801\n",
            "Epoch: 008, Step: 195, Val Loss: 1.0517\n",
            "Epoch: 008, Step: 196, Val Loss: 1.0879\n",
            "Epoch: 008, Step: 197, Val Loss: 0.9343\n",
            "Epoch: 008, Step: 198, Val Loss: 1.0634\n",
            "Epoch: 008, Step: 199, Val Loss: 1.1093\n",
            "Epoch: 008, Step: 200, Val Loss: 1.0369\n",
            "Epoch: 008, Step: 201, Val Loss: 1.0260\n",
            "Epoch: 008, Step: 202, Val Loss: 0.9020\n",
            "Epoch: 008, Step: 203, Val Loss: 0.8703\n",
            "Epoch: 008, Step: 204, Val Loss: 0.9326\n",
            "Epoch: 008, Step: 205, Val Loss: 1.0231\n",
            "Epoch: 008, Step: 206, Val Loss: 1.0528\n",
            "Epoch: 008, Step: 207, Val Loss: 1.2333\n",
            "Epoch: 008, Step: 208, Val Loss: 0.9708\n",
            "Epoch: 008, Step: 209, Val Loss: 1.2071\n",
            "Epoch: 008, Step: 210, Val Loss: 0.9544\n",
            "Epoch: 008, Step: 211, Val Loss: 0.8648\n",
            "Epoch: 008, Step: 212, Val Loss: 0.9267\n",
            "Epoch: 008, Step: 213, Val Loss: 0.9026\n",
            "Epoch: 008, Step: 214, Val Loss: 1.1360\n",
            "Epoch: 008, Step: 215, Val Loss: 1.0614\n",
            "Epoch: 008, Step: 216, Val Loss: 1.1265\n",
            "Epoch: 008, Step: 217, Val Loss: 1.0102\n",
            "Epoch: 008, Step: 218, Val Loss: 1.0023\n",
            "Epoch: 008, Step: 219, Val Loss: 0.9108\n",
            "Epoch: 008, Step: 220, Val Loss: 0.9642\n",
            "Epoch: 008, Step: 221, Val Loss: 0.9817\n",
            "Epoch: 008, Step: 222, Val Loss: 0.9724\n",
            "Epoch: 008, Step: 223, Val Loss: 1.1192\n",
            "Epoch: 008, Step: 224, Val Loss: 0.9422\n",
            "Epoch: 008, Step: 225, Val Loss: 0.7541\n",
            "Epoch: 008, Step: 226, Val Loss: 1.0088\n",
            "Epoch: 008, Step: 227, Val Loss: 0.9427\n",
            "Epoch: 008, Step: 228, Val Loss: 0.9858\n",
            "Epoch: 008, Step: 229, Val Loss: 1.0643\n",
            "Epoch: 008, Step: 230, Val Loss: 0.8992\n",
            "Epoch: 008, Step: 231, Val Loss: 1.1215\n",
            "Epoch: 008, Step: 232, Val Loss: 1.1648\n",
            "Epoch: 008, Step: 233, Val Loss: 0.9819\n",
            "Epoch: 008, Step: 234, Val Loss: 0.9576\n",
            "Epoch: 008, Step: 235, Val Loss: 0.9951\n",
            "Epoch: 008, Step: 236, Val Loss: 0.9156\n",
            "Epoch: 008, Step: 237, Val Loss: 0.9387\n",
            "Epoch: 008, Step: 238, Val Loss: 1.1034\n",
            "Epoch: 008, Step: 239, Val Loss: 0.9716\n",
            "Epoch: 008, Step: 240, Val Loss: 1.0571\n",
            "Epoch: 008, Step: 241, Val Loss: 0.8306\n",
            "Epoch: 008, Step: 242, Val Loss: 0.9547\n",
            "Epoch: 008, Step: 243, Val Loss: 0.8690\n",
            "Epoch: 008, Step: 244, Val Loss: 0.8784\n",
            "Epoch: 008, Step: 245, Val Loss: 1.0101\n",
            "Epoch: 008, Step: 246, Val Loss: 1.0195\n",
            "Epoch: 008, Step: 247, Val Loss: 0.8805\n",
            "Epoch: 008, Step: 248, Val Loss: 0.8798\n",
            "Epoch: 008, Step: 249, Val Loss: 1.0098\n",
            "Epoch: 008, Step: 250, Val Loss: 0.9514\n",
            "Epoch: 008, Step: 251, Val Loss: 1.0043\n",
            "Epoch: 008, Step: 252, Val Loss: 0.8923\n",
            "Epoch: 008, Step: 253, Val Loss: 0.9385\n",
            "Epoch: 008, Step: 254, Val Loss: 1.0229\n",
            "Epoch: 008, Step: 255, Val Loss: 1.0809\n",
            "Epoch: 008, Step: 256, Val Loss: 1.0156\n",
            "Epoch: 008, Step: 257, Val Loss: 1.0095\n",
            "Epoch: 008, Step: 258, Val Loss: 0.7607\n",
            "Epoch: 008, Step: 259, Val Loss: 0.9779\n",
            "Epoch: 008, Step: 260, Val Loss: 1.1433\n",
            "Epoch: 008, Step: 261, Val Loss: 0.9656\n",
            "Epoch: 008, Step: 262, Val Loss: 1.0138\n",
            "Epoch: 008, Step: 263, Val Loss: 0.9936\n",
            "Epoch: 008, Step: 264, Val Loss: 0.9467\n",
            "Epoch: 008, Step: 265, Val Loss: 0.8478\n",
            "Epoch: 008, Step: 266, Val Loss: 0.9816\n",
            "Epoch: 008, Step: 267, Val Loss: 0.9977\n",
            "Epoch: 008, Step: 268, Val Loss: 0.8645\n",
            "Epoch: 008, Step: 269, Val Loss: 0.8425\n",
            "Epoch: 008, Step: 270, Val Loss: 0.9145\n",
            "Epoch: 008, Step: 271, Val Loss: 1.0303\n",
            "Epoch: 008, Step: 272, Val Loss: 1.1082\n",
            "Epoch: 008, Step: 273, Val Loss: 1.0484\n",
            "Epoch: 008, Step: 274, Val Loss: 0.9614\n",
            "Epoch: 008, Step: 275, Val Loss: 0.8972\n",
            "Epoch: 008, Step: 276, Val Loss: 0.8938\n",
            "Epoch: 008, Step: 277, Val Loss: 1.1186\n",
            "Epoch: 008, Step: 278, Val Loss: 1.1336\n",
            "Epoch: 008, Step: 279, Val Loss: 1.0674\n",
            "Epoch: 008, Step: 280, Val Loss: 1.4347\n",
            "Epoch: 008, Step: 281, Val Loss: 2.6217\n",
            "Epoch: 009, Step: 000, Loss: 0.9637\n",
            "Epoch: 009, Step: 001, Loss: 1.0354\n",
            "Epoch: 009, Step: 002, Loss: 0.9865\n",
            "Epoch: 009, Step: 003, Loss: 0.9657\n",
            "Epoch: 009, Step: 004, Loss: 1.0576\n",
            "Epoch: 009, Step: 005, Loss: 0.8546\n",
            "Epoch: 009, Step: 006, Loss: 0.9457\n",
            "Epoch: 009, Step: 007, Loss: 0.9725\n",
            "Epoch: 009, Step: 008, Loss: 0.9059\n",
            "Epoch: 009, Step: 009, Loss: 0.9373\n",
            "Epoch: 009, Step: 010, Loss: 0.8377\n",
            "Epoch: 009, Step: 011, Loss: 0.8958\n",
            "Epoch: 009, Step: 012, Loss: 0.8985\n",
            "Epoch: 009, Step: 013, Loss: 0.9822\n",
            "Epoch: 009, Step: 014, Loss: 0.8027\n",
            "Epoch: 009, Step: 015, Loss: 1.1026\n",
            "Epoch: 009, Step: 016, Loss: 0.9306\n",
            "Epoch: 009, Step: 017, Loss: 1.0163\n",
            "Epoch: 009, Step: 018, Loss: 0.9849\n",
            "Epoch: 009, Step: 019, Loss: 0.9101\n",
            "Epoch: 009, Step: 020, Loss: 1.1294\n",
            "Epoch: 009, Step: 021, Loss: 1.1226\n",
            "Epoch: 009, Step: 022, Loss: 0.9983\n",
            "Epoch: 009, Step: 023, Loss: 0.9976\n",
            "Epoch: 009, Step: 024, Loss: 1.0417\n",
            "Epoch: 009, Step: 025, Loss: 0.9122\n",
            "Epoch: 009, Step: 026, Loss: 0.8004\n",
            "Epoch: 009, Step: 027, Loss: 1.0238\n",
            "Epoch: 009, Step: 028, Loss: 0.8733\n",
            "Epoch: 009, Step: 029, Loss: 0.9651\n",
            "Epoch: 009, Step: 030, Loss: 1.0340\n",
            "Epoch: 009, Step: 031, Loss: 0.8939\n",
            "Epoch: 009, Step: 032, Loss: 0.9437\n",
            "Epoch: 009, Step: 033, Loss: 0.8880\n",
            "Epoch: 009, Step: 034, Loss: 0.9900\n",
            "Epoch: 009, Step: 035, Loss: 1.0540\n",
            "Epoch: 009, Step: 036, Loss: 0.8035\n",
            "Epoch: 009, Step: 037, Loss: 0.8598\n",
            "Epoch: 009, Step: 038, Loss: 0.8944\n",
            "Epoch: 009, Step: 039, Loss: 0.9037\n",
            "Epoch: 009, Step: 040, Loss: 0.9258\n",
            "Epoch: 009, Step: 041, Loss: 0.9204\n",
            "Epoch: 009, Step: 042, Loss: 0.8950\n",
            "Epoch: 009, Step: 043, Loss: 0.8204\n",
            "Epoch: 009, Step: 044, Loss: 1.0441\n",
            "Epoch: 009, Step: 045, Loss: 0.9732\n",
            "Epoch: 009, Step: 046, Loss: 0.8722\n",
            "Epoch: 009, Step: 047, Loss: 0.8263\n",
            "Epoch: 009, Step: 048, Loss: 0.9970\n",
            "Epoch: 009, Step: 049, Loss: 1.0077\n",
            "Epoch: 009, Step: 050, Loss: 0.8598\n",
            "Epoch: 009, Step: 051, Loss: 1.0085\n",
            "Epoch: 009, Step: 052, Loss: 1.1571\n",
            "Epoch: 009, Step: 053, Loss: 1.0353\n",
            "Epoch: 009, Step: 054, Loss: 0.9218\n",
            "Epoch: 009, Step: 055, Loss: 0.9393\n",
            "Epoch: 009, Step: 056, Loss: 1.0183\n",
            "Epoch: 009, Step: 057, Loss: 1.0222\n",
            "Epoch: 009, Step: 058, Loss: 1.0212\n",
            "Epoch: 009, Step: 059, Loss: 0.9188\n",
            "Epoch: 009, Step: 060, Loss: 0.8509\n",
            "Epoch: 009, Step: 061, Loss: 0.9417\n",
            "Epoch: 009, Step: 062, Loss: 0.8641\n",
            "Epoch: 009, Step: 063, Loss: 0.9490\n",
            "Epoch: 009, Step: 064, Loss: 0.9460\n",
            "Epoch: 009, Step: 065, Loss: 0.8780\n",
            "Epoch: 009, Step: 066, Loss: 1.0363\n",
            "Epoch: 009, Step: 067, Loss: 0.8185\n",
            "Epoch: 009, Step: 068, Loss: 0.7919\n",
            "Epoch: 009, Step: 069, Loss: 0.9197\n",
            "Epoch: 009, Step: 070, Loss: 0.9457\n",
            "Epoch: 009, Step: 071, Loss: 1.0688\n",
            "Epoch: 009, Step: 072, Loss: 0.8870\n",
            "Epoch: 009, Step: 073, Loss: 0.9136\n",
            "Epoch: 009, Step: 074, Loss: 0.8432\n",
            "Epoch: 009, Step: 075, Loss: 1.0064\n",
            "Epoch: 009, Step: 076, Loss: 1.0782\n",
            "Epoch: 009, Step: 077, Loss: 0.9580\n",
            "Epoch: 009, Step: 078, Loss: 0.9509\n",
            "Epoch: 009, Step: 079, Loss: 0.8474\n",
            "Epoch: 009, Step: 080, Loss: 1.0630\n",
            "Epoch: 009, Step: 081, Loss: 1.0526\n",
            "Epoch: 009, Step: 082, Loss: 1.0383\n",
            "Epoch: 009, Step: 083, Loss: 1.0438\n",
            "Epoch: 009, Step: 084, Loss: 0.9001\n",
            "Epoch: 009, Step: 085, Loss: 1.0728\n",
            "Epoch: 009, Step: 086, Loss: 1.0229\n",
            "Epoch: 009, Step: 087, Loss: 0.8009\n",
            "Epoch: 009, Step: 088, Loss: 0.9622\n",
            "Epoch: 009, Step: 089, Loss: 1.0452\n",
            "Epoch: 009, Step: 090, Loss: 0.8006\n",
            "Epoch: 009, Step: 091, Loss: 0.9127\n",
            "Epoch: 009, Step: 092, Loss: 0.9736\n",
            "Epoch: 009, Step: 093, Loss: 0.8641\n",
            "Epoch: 009, Step: 094, Loss: 0.9211\n",
            "Epoch: 009, Step: 095, Loss: 0.9800\n",
            "Epoch: 009, Step: 096, Loss: 0.7983\n",
            "Epoch: 009, Step: 097, Loss: 0.8436\n",
            "Epoch: 009, Step: 098, Loss: 0.9186\n",
            "Epoch: 009, Step: 099, Loss: 1.0838\n",
            "Epoch: 009, Step: 100, Loss: 0.9314\n",
            "Epoch: 009, Step: 101, Loss: 1.1072\n",
            "Epoch: 009, Step: 102, Loss: 0.9950\n",
            "Epoch: 009, Step: 103, Loss: 0.9086\n",
            "Epoch: 009, Step: 104, Loss: 0.9212\n",
            "Epoch: 009, Step: 105, Loss: 0.9893\n",
            "Epoch: 009, Step: 106, Loss: 0.9307\n",
            "Epoch: 009, Step: 107, Loss: 0.8148\n",
            "Epoch: 009, Step: 108, Loss: 0.9854\n",
            "Epoch: 009, Step: 109, Loss: 1.0486\n",
            "Epoch: 009, Step: 110, Loss: 0.8346\n",
            "Epoch: 009, Step: 111, Loss: 0.8441\n",
            "Epoch: 009, Step: 112, Loss: 0.9706\n",
            "Epoch: 009, Step: 113, Loss: 1.0660\n",
            "Epoch: 009, Step: 114, Loss: 0.9221\n",
            "Epoch: 009, Step: 115, Loss: 1.1281\n",
            "Epoch: 009, Step: 116, Loss: 1.0199\n",
            "Epoch: 009, Step: 117, Loss: 0.9824\n",
            "Epoch: 009, Step: 118, Loss: 0.8735\n",
            "Epoch: 009, Step: 119, Loss: 0.9365\n",
            "Epoch: 009, Step: 120, Loss: 0.9718\n",
            "Epoch: 009, Step: 121, Loss: 0.9100\n",
            "Epoch: 009, Step: 122, Loss: 1.1093\n",
            "Epoch: 009, Step: 123, Loss: 0.8917\n",
            "Epoch: 009, Step: 124, Loss: 1.0581\n",
            "Epoch: 009, Step: 125, Loss: 0.7922\n",
            "Epoch: 009, Step: 126, Loss: 0.9445\n",
            "Epoch: 009, Step: 127, Loss: 0.9850\n",
            "Epoch: 009, Step: 128, Loss: 0.8856\n",
            "Epoch: 009, Step: 129, Loss: 0.8271\n",
            "Epoch: 009, Step: 130, Loss: 1.0273\n",
            "Epoch: 009, Step: 131, Loss: 0.8846\n",
            "Epoch: 009, Step: 132, Loss: 0.8056\n",
            "Epoch: 009, Step: 133, Loss: 0.9504\n",
            "Epoch: 009, Step: 134, Loss: 1.0869\n",
            "Epoch: 009, Step: 135, Loss: 0.6862\n",
            "Epoch: 009, Step: 136, Loss: 0.7502\n",
            "Epoch: 009, Step: 137, Loss: 0.9038\n",
            "Epoch: 009, Step: 138, Loss: 1.0220\n",
            "Epoch: 009, Step: 139, Loss: 0.9973\n",
            "Epoch: 009, Step: 140, Loss: 0.7458\n",
            "Epoch: 009, Step: 141, Loss: 1.0166\n",
            "Epoch: 009, Step: 142, Loss: 1.0150\n",
            "Epoch: 009, Step: 143, Loss: 1.0532\n",
            "Epoch: 009, Step: 144, Loss: 1.0939\n",
            "Epoch: 009, Step: 145, Loss: 0.9001\n",
            "Epoch: 009, Step: 146, Loss: 0.9485\n",
            "Epoch: 009, Step: 147, Loss: 1.0549\n",
            "Epoch: 009, Step: 148, Loss: 0.8941\n",
            "Epoch: 009, Step: 149, Loss: 1.0032\n",
            "Epoch: 009, Step: 150, Loss: 0.8432\n",
            "Epoch: 009, Step: 151, Loss: 1.0237\n",
            "Epoch: 009, Step: 152, Loss: 0.7803\n",
            "Epoch: 009, Step: 153, Loss: 0.9712\n",
            "Epoch: 009, Step: 154, Loss: 0.8957\n",
            "Epoch: 009, Step: 155, Loss: 0.9148\n",
            "Epoch: 009, Step: 156, Loss: 1.0040\n",
            "Epoch: 009, Step: 157, Loss: 1.1260\n",
            "Epoch: 009, Step: 158, Loss: 0.9779\n",
            "Epoch: 009, Step: 159, Loss: 0.9601\n",
            "Epoch: 009, Step: 160, Loss: 0.9277\n",
            "Epoch: 009, Step: 161, Loss: 0.9561\n",
            "Epoch: 009, Step: 162, Loss: 0.9817\n",
            "Epoch: 009, Step: 163, Loss: 0.8821\n",
            "Epoch: 009, Step: 164, Loss: 0.8303\n",
            "Epoch: 009, Step: 165, Loss: 1.0289\n",
            "Epoch: 009, Step: 166, Loss: 0.9598\n",
            "Epoch: 009, Step: 167, Loss: 0.8787\n",
            "Epoch: 009, Step: 168, Loss: 0.8415\n",
            "Epoch: 009, Step: 169, Loss: 0.9688\n",
            "Epoch: 009, Step: 170, Loss: 1.0888\n",
            "Epoch: 009, Step: 171, Loss: 0.8973\n",
            "Epoch: 009, Step: 172, Loss: 0.8665\n",
            "Epoch: 009, Step: 173, Loss: 0.8511\n",
            "Epoch: 009, Step: 174, Loss: 0.9959\n",
            "Epoch: 009, Step: 175, Loss: 0.8773\n",
            "Epoch: 009, Step: 176, Loss: 0.9631\n",
            "Epoch: 009, Step: 177, Loss: 0.8883\n",
            "Epoch: 009, Step: 178, Loss: 0.9873\n",
            "Epoch: 009, Step: 179, Loss: 0.8486\n",
            "Epoch: 009, Step: 180, Loss: 0.9982\n",
            "Epoch: 009, Step: 181, Loss: 1.0410\n",
            "Epoch: 009, Step: 182, Loss: 1.0044\n",
            "Epoch: 009, Step: 183, Loss: 1.0206\n",
            "Epoch: 009, Step: 184, Loss: 0.7213\n",
            "Epoch: 009, Step: 185, Loss: 0.9383\n",
            "Epoch: 009, Step: 186, Loss: 1.0241\n",
            "Epoch: 009, Step: 187, Loss: 1.0140\n",
            "Epoch: 009, Step: 188, Loss: 0.9915\n",
            "Epoch: 009, Step: 189, Loss: 0.8242\n",
            "Epoch: 009, Step: 190, Loss: 0.8814\n",
            "Epoch: 009, Step: 191, Loss: 0.9669\n",
            "Epoch: 009, Step: 192, Loss: 0.9495\n",
            "Epoch: 009, Step: 193, Loss: 1.0595\n",
            "Epoch: 009, Step: 194, Loss: 0.8534\n",
            "Epoch: 009, Step: 195, Loss: 1.1606\n",
            "Epoch: 009, Step: 196, Loss: 0.8815\n",
            "Epoch: 009, Step: 197, Loss: 0.9260\n",
            "Epoch: 009, Step: 198, Loss: 0.9589\n",
            "Epoch: 009, Step: 199, Loss: 1.0464\n",
            "Epoch: 009, Step: 200, Loss: 0.9452\n",
            "Epoch: 009, Step: 201, Loss: 0.9280\n",
            "Epoch: 009, Step: 202, Loss: 0.9500\n",
            "Epoch: 009, Step: 203, Loss: 0.9284\n",
            "Epoch: 009, Step: 204, Loss: 0.9849\n",
            "Epoch: 009, Step: 205, Loss: 1.0318\n",
            "Epoch: 009, Step: 206, Loss: 1.0982\n",
            "Epoch: 009, Step: 207, Loss: 1.0187\n",
            "Epoch: 009, Step: 208, Loss: 0.8550\n",
            "Epoch: 009, Step: 209, Loss: 0.8324\n",
            "Epoch: 009, Step: 210, Loss: 1.0189\n",
            "Epoch: 009, Step: 211, Loss: 0.9219\n",
            "Epoch: 009, Step: 212, Loss: 0.9977\n",
            "Epoch: 009, Step: 213, Loss: 0.9393\n",
            "Epoch: 009, Step: 214, Loss: 1.0085\n",
            "Epoch: 009, Step: 215, Loss: 0.9781\n",
            "Epoch: 009, Step: 216, Loss: 0.9139\n",
            "Epoch: 009, Step: 217, Loss: 1.0789\n",
            "Epoch: 009, Step: 218, Loss: 0.9152\n",
            "Epoch: 009, Step: 219, Loss: 0.9017\n",
            "Epoch: 009, Step: 220, Loss: 0.8712\n",
            "Epoch: 009, Step: 221, Loss: 1.0578\n",
            "Epoch: 009, Step: 222, Loss: 0.9116\n",
            "Epoch: 009, Step: 223, Loss: 0.7617\n",
            "Epoch: 009, Step: 224, Loss: 0.9343\n",
            "Epoch: 009, Step: 225, Loss: 0.9013\n",
            "Epoch: 009, Step: 226, Loss: 1.1343\n",
            "Epoch: 009, Step: 227, Loss: 0.8987\n",
            "Epoch: 009, Step: 228, Loss: 0.9625\n",
            "Epoch: 009, Step: 229, Loss: 0.8473\n",
            "Epoch: 009, Step: 230, Loss: 1.1840\n",
            "Epoch: 009, Step: 231, Loss: 1.1693\n",
            "Epoch: 009, Step: 232, Loss: 1.0782\n",
            "Epoch: 009, Step: 233, Loss: 0.9928\n",
            "Epoch: 009, Step: 234, Loss: 0.9076\n",
            "Epoch: 009, Step: 235, Loss: 0.8834\n",
            "Epoch: 009, Step: 236, Loss: 1.0612\n",
            "Epoch: 009, Step: 237, Loss: 1.0762\n",
            "Epoch: 009, Step: 238, Loss: 0.9230\n",
            "Epoch: 009, Step: 239, Loss: 0.7961\n",
            "Epoch: 009, Step: 240, Loss: 0.9675\n",
            "Epoch: 009, Step: 241, Loss: 0.8913\n",
            "Epoch: 009, Step: 242, Loss: 0.8857\n",
            "Epoch: 009, Step: 243, Loss: 1.0649\n",
            "Epoch: 009, Step: 244, Loss: 0.9866\n",
            "Epoch: 009, Step: 245, Loss: 0.9323\n",
            "Epoch: 009, Step: 246, Loss: 0.9309\n",
            "Epoch: 009, Step: 247, Loss: 1.1338\n",
            "Epoch: 009, Step: 248, Loss: 1.0208\n",
            "Epoch: 009, Step: 249, Loss: 0.8786\n",
            "Epoch: 009, Step: 250, Loss: 0.9876\n",
            "Epoch: 009, Step: 251, Loss: 0.9768\n",
            "Epoch: 009, Step: 252, Loss: 1.1257\n",
            "Epoch: 009, Step: 253, Loss: 0.9178\n",
            "Epoch: 009, Step: 254, Loss: 1.0629\n",
            "Epoch: 009, Step: 255, Loss: 1.1232\n",
            "Epoch: 009, Step: 256, Loss: 0.8060\n",
            "Epoch: 009, Step: 257, Loss: 0.8995\n",
            "Epoch: 009, Step: 258, Loss: 0.9098\n",
            "Epoch: 009, Step: 259, Loss: 0.9364\n",
            "Epoch: 009, Step: 260, Loss: 0.9372\n",
            "Epoch: 009, Step: 261, Loss: 0.8815\n",
            "Epoch: 009, Step: 262, Loss: 1.0341\n",
            "Epoch: 009, Step: 263, Loss: 1.0378\n",
            "Epoch: 009, Step: 264, Loss: 0.9573\n",
            "Epoch: 009, Step: 265, Loss: 0.9715\n",
            "Epoch: 009, Step: 266, Loss: 1.0209\n",
            "Epoch: 009, Step: 267, Loss: 0.8843\n",
            "Epoch: 009, Step: 268, Loss: 1.0219\n",
            "Epoch: 009, Step: 269, Loss: 1.1112\n",
            "Epoch: 009, Step: 270, Loss: 1.0151\n",
            "Epoch: 009, Step: 271, Loss: 0.9017\n",
            "Epoch: 009, Step: 272, Loss: 1.0052\n",
            "Epoch: 009, Step: 273, Loss: 0.9339\n",
            "Epoch: 009, Step: 274, Loss: 0.7952\n",
            "Epoch: 009, Step: 275, Loss: 1.0791\n",
            "Epoch: 009, Step: 276, Loss: 0.8319\n",
            "Epoch: 009, Step: 277, Loss: 0.9927\n",
            "Epoch: 009, Step: 278, Loss: 0.9449\n",
            "Epoch: 009, Step: 279, Loss: 0.9398\n",
            "Epoch: 009, Step: 280, Loss: 0.8949\n",
            "Epoch: 009, Step: 281, Loss: 0.7736\n",
            "Epoch: 009, Step: 282, Loss: 0.8491\n",
            "Epoch: 009, Step: 283, Loss: 1.0179\n",
            "Epoch: 009, Step: 284, Loss: 0.8827\n",
            "Epoch: 009, Step: 285, Loss: 0.9414\n",
            "Epoch: 009, Step: 286, Loss: 1.0292\n",
            "Epoch: 009, Step: 287, Loss: 0.8065\n",
            "Epoch: 009, Step: 288, Loss: 1.0114\n",
            "Epoch: 009, Step: 289, Loss: 0.9253\n",
            "Epoch: 009, Step: 290, Loss: 1.0034\n",
            "Epoch: 009, Step: 291, Loss: 0.9916\n",
            "Epoch: 009, Step: 292, Loss: 0.8976\n",
            "Epoch: 009, Step: 293, Loss: 0.9137\n",
            "Epoch: 009, Step: 294, Loss: 0.9664\n",
            "Epoch: 009, Step: 295, Loss: 0.8624\n",
            "Epoch: 009, Step: 296, Loss: 1.1178\n",
            "Epoch: 009, Step: 297, Loss: 0.9906\n",
            "Epoch: 009, Step: 298, Loss: 0.8802\n",
            "Epoch: 009, Step: 299, Loss: 0.9414\n",
            "Epoch: 009, Step: 300, Loss: 0.9716\n",
            "Epoch: 009, Step: 301, Loss: 0.9424\n",
            "Epoch: 009, Step: 302, Loss: 0.7900\n",
            "Epoch: 009, Step: 303, Loss: 0.8893\n",
            "Epoch: 009, Step: 304, Loss: 0.9911\n",
            "Epoch: 009, Step: 305, Loss: 0.9773\n",
            "Epoch: 009, Step: 306, Loss: 0.8600\n",
            "Epoch: 009, Step: 307, Loss: 0.9149\n",
            "Epoch: 009, Step: 308, Loss: 0.9253\n",
            "Epoch: 009, Step: 309, Loss: 0.9611\n",
            "Epoch: 009, Step: 310, Loss: 1.0383\n",
            "Epoch: 009, Step: 311, Loss: 1.0265\n",
            "Epoch: 009, Step: 312, Loss: 1.0247\n",
            "Epoch: 009, Step: 313, Loss: 0.9656\n",
            "Epoch: 009, Step: 314, Loss: 1.1256\n",
            "Epoch: 009, Step: 315, Loss: 0.9245\n",
            "Epoch: 009, Step: 316, Loss: 0.8823\n",
            "Epoch: 009, Step: 317, Loss: 1.1037\n",
            "Epoch: 009, Step: 318, Loss: 0.9211\n",
            "Epoch: 009, Step: 319, Loss: 0.9228\n",
            "Epoch: 009, Step: 320, Loss: 1.0497\n",
            "Epoch: 009, Step: 321, Loss: 0.9671\n",
            "Epoch: 009, Step: 322, Loss: 1.1022\n",
            "Epoch: 009, Step: 323, Loss: 0.8934\n",
            "Epoch: 009, Step: 324, Loss: 0.9225\n",
            "Epoch: 009, Step: 325, Loss: 1.0670\n",
            "Epoch: 009, Step: 326, Loss: 1.0249\n",
            "Epoch: 009, Step: 327, Loss: 0.8502\n",
            "Epoch: 009, Step: 328, Loss: 0.9667\n",
            "Epoch: 009, Step: 329, Loss: 0.9314\n",
            "Epoch: 009, Step: 330, Loss: 1.0855\n",
            "Epoch: 009, Step: 331, Loss: 0.8088\n",
            "Epoch: 009, Step: 332, Loss: 0.8822\n",
            "Epoch: 009, Step: 333, Loss: 0.9134\n",
            "Epoch: 009, Step: 334, Loss: 0.8129\n",
            "Epoch: 009, Step: 335, Loss: 0.9786\n",
            "Epoch: 009, Step: 336, Loss: 0.9612\n",
            "Epoch: 009, Step: 337, Loss: 0.9885\n",
            "Epoch: 009, Step: 338, Loss: 0.9103\n",
            "Epoch: 009, Step: 339, Loss: 0.9927\n",
            "Epoch: 009, Step: 340, Loss: 0.9943\n",
            "Epoch: 009, Step: 341, Loss: 1.0886\n",
            "Epoch: 009, Step: 342, Loss: 1.0314\n",
            "Epoch: 009, Step: 343, Loss: 0.9741\n",
            "Epoch: 009, Step: 344, Loss: 1.0581\n",
            "Epoch: 009, Step: 345, Loss: 0.9315\n",
            "Epoch: 009, Step: 346, Loss: 0.8354\n",
            "Epoch: 009, Step: 347, Loss: 0.7313\n",
            "Epoch: 009, Step: 348, Loss: 1.0041\n",
            "Epoch: 009, Step: 349, Loss: 0.8735\n",
            "Epoch: 009, Step: 350, Loss: 1.0351\n",
            "Epoch: 009, Step: 351, Loss: 0.9582\n",
            "Epoch: 009, Step: 352, Loss: 0.9337\n",
            "Epoch: 009, Step: 353, Loss: 0.9646\n",
            "Epoch: 009, Step: 354, Loss: 1.1346\n",
            "Epoch: 009, Step: 355, Loss: 0.9769\n",
            "Epoch: 009, Step: 356, Loss: 0.9377\n",
            "Epoch: 009, Step: 357, Loss: 0.8325\n",
            "Epoch: 009, Step: 358, Loss: 1.0050\n",
            "Epoch: 009, Step: 359, Loss: 0.7300\n",
            "Epoch: 009, Step: 360, Loss: 0.8414\n",
            "Epoch: 009, Step: 361, Loss: 1.1019\n",
            "Epoch: 009, Step: 362, Loss: 0.9704\n",
            "Epoch: 009, Step: 363, Loss: 0.9375\n",
            "Epoch: 009, Step: 364, Loss: 1.0479\n",
            "Epoch: 009, Step: 365, Loss: 0.8954\n",
            "Epoch: 009, Step: 366, Loss: 0.9949\n",
            "Epoch: 009, Step: 367, Loss: 0.8936\n",
            "Epoch: 009, Step: 368, Loss: 1.0097\n",
            "Epoch: 009, Step: 369, Loss: 0.9779\n",
            "Epoch: 009, Step: 370, Loss: 0.8448\n",
            "Epoch: 009, Step: 371, Loss: 0.9422\n",
            "Epoch: 009, Step: 372, Loss: 1.0557\n",
            "Epoch: 009, Step: 373, Loss: 0.9905\n",
            "Epoch: 009, Step: 374, Loss: 0.9964\n",
            "Epoch: 009, Step: 375, Loss: 0.9802\n",
            "Epoch: 009, Step: 376, Loss: 0.8638\n",
            "Epoch: 009, Step: 377, Loss: 0.8944\n",
            "Epoch: 009, Step: 378, Loss: 1.1485\n",
            "Epoch: 009, Step: 379, Loss: 1.1657\n",
            "Epoch: 009, Step: 380, Loss: 1.0434\n",
            "Epoch: 009, Step: 381, Loss: 0.9506\n",
            "Epoch: 009, Step: 382, Loss: 0.9518\n",
            "Epoch: 009, Step: 383, Loss: 0.9387\n",
            "Epoch: 009, Step: 384, Loss: 1.0353\n",
            "Epoch: 009, Step: 385, Loss: 0.8604\n",
            "Epoch: 009, Step: 386, Loss: 0.8827\n",
            "Epoch: 009, Step: 387, Loss: 1.0379\n",
            "Epoch: 009, Step: 388, Loss: 0.7998\n",
            "Epoch: 009, Step: 389, Loss: 1.1260\n",
            "Epoch: 009, Step: 390, Loss: 0.9962\n",
            "Epoch: 009, Step: 391, Loss: 0.9849\n",
            "Epoch: 009, Step: 392, Loss: 1.1065\n",
            "Epoch: 009, Step: 393, Loss: 0.9416\n",
            "Epoch: 009, Step: 394, Loss: 0.9654\n",
            "Epoch: 009, Step: 395, Loss: 0.9763\n",
            "Epoch: 009, Step: 396, Loss: 1.1933\n",
            "Epoch: 009, Step: 397, Loss: 0.8393\n",
            "Epoch: 009, Step: 398, Loss: 0.9964\n",
            "Epoch: 009, Step: 399, Loss: 0.9157\n",
            "Epoch: 009, Step: 400, Loss: 0.9736\n",
            "Epoch: 009, Step: 401, Loss: 1.0869\n",
            "Epoch: 009, Step: 402, Loss: 0.8001\n",
            "Epoch: 009, Step: 403, Loss: 0.8759\n",
            "Epoch: 009, Step: 404, Loss: 1.0110\n",
            "Epoch: 009, Step: 405, Loss: 0.9603\n",
            "Epoch: 009, Step: 406, Loss: 0.9334\n",
            "Epoch: 009, Step: 407, Loss: 1.0495\n",
            "Epoch: 009, Step: 408, Loss: 0.8918\n",
            "Epoch: 009, Step: 409, Loss: 1.0038\n",
            "Epoch: 009, Step: 410, Loss: 1.0548\n",
            "Epoch: 009, Step: 411, Loss: 0.9227\n",
            "Epoch: 009, Step: 412, Loss: 0.9219\n",
            "Epoch: 009, Step: 413, Loss: 0.8907\n",
            "Epoch: 009, Step: 414, Loss: 1.0949\n",
            "Epoch: 009, Step: 415, Loss: 1.0927\n",
            "Epoch: 009, Step: 416, Loss: 0.9374\n",
            "Epoch: 009, Step: 417, Loss: 0.7996\n",
            "Epoch: 009, Step: 418, Loss: 1.0508\n",
            "Epoch: 009, Step: 419, Loss: 0.9119\n",
            "Epoch: 009, Step: 420, Loss: 1.0227\n",
            "Epoch: 009, Step: 421, Loss: 0.7546\n",
            "Epoch: 009, Step: 422, Loss: 0.9547\n",
            "Epoch: 009, Step: 423, Loss: 0.8460\n",
            "Epoch: 009, Step: 424, Loss: 0.9536\n",
            "Epoch: 009, Step: 425, Loss: 0.9208\n",
            "Epoch: 009, Step: 426, Loss: 1.0249\n",
            "Epoch: 009, Step: 427, Loss: 0.9640\n",
            "Epoch: 009, Step: 428, Loss: 0.9518\n",
            "Epoch: 009, Step: 429, Loss: 0.8790\n",
            "Epoch: 009, Step: 430, Loss: 0.9154\n",
            "Epoch: 009, Step: 431, Loss: 1.0243\n",
            "Epoch: 009, Step: 432, Loss: 1.2276\n",
            "Epoch: 009, Step: 433, Loss: 0.8276\n",
            "Epoch: 009, Step: 434, Loss: 0.9939\n",
            "Epoch: 009, Step: 435, Loss: 1.0848\n",
            "Epoch: 009, Step: 436, Loss: 0.8853\n",
            "Epoch: 009, Step: 437, Loss: 0.8072\n",
            "Epoch: 009, Step: 438, Loss: 0.9535\n",
            "Epoch: 009, Step: 439, Loss: 1.0421\n",
            "Epoch: 009, Step: 440, Loss: 0.7887\n",
            "Epoch: 009, Step: 441, Loss: 0.8655\n",
            "Epoch: 009, Step: 442, Loss: 0.9387\n",
            "Epoch: 009, Step: 443, Loss: 0.9002\n",
            "Epoch: 009, Step: 444, Loss: 1.0446\n",
            "Epoch: 009, Step: 445, Loss: 0.9126\n",
            "Epoch: 009, Step: 446, Loss: 0.9263\n",
            "Epoch: 009, Step: 447, Loss: 0.8697\n",
            "Epoch: 009, Step: 448, Loss: 0.9155\n",
            "Epoch: 009, Step: 449, Loss: 0.7707\n",
            "Epoch: 009, Step: 450, Loss: 0.9300\n",
            "Epoch: 009, Step: 451, Loss: 0.9594\n",
            "Epoch: 009, Step: 452, Loss: 0.9253\n",
            "Epoch: 009, Step: 453, Loss: 0.9407\n",
            "Epoch: 009, Step: 454, Loss: 1.0570\n",
            "Epoch: 009, Step: 455, Loss: 0.8132\n",
            "Epoch: 009, Step: 456, Loss: 0.9442\n",
            "Epoch: 009, Step: 457, Loss: 0.8980\n",
            "Epoch: 009, Step: 458, Loss: 0.9200\n",
            "Epoch: 009, Step: 459, Loss: 1.1143\n",
            "Epoch: 009, Step: 460, Loss: 0.8634\n",
            "Epoch: 009, Step: 461, Loss: 0.9734\n",
            "Epoch: 009, Step: 462, Loss: 0.9666\n",
            "Epoch: 009, Step: 463, Loss: 0.9869\n",
            "Epoch: 009, Step: 464, Loss: 0.9242\n",
            "Epoch: 009, Step: 465, Loss: 0.9837\n",
            "Epoch: 009, Step: 466, Loss: 0.9404\n",
            "Epoch: 009, Step: 467, Loss: 0.8018\n",
            "Epoch: 009, Step: 468, Loss: 0.8050\n",
            "Epoch: 009, Step: 469, Loss: 1.1431\n",
            "Epoch: 009, Step: 470, Loss: 1.0298\n",
            "Epoch: 009, Step: 471, Loss: 1.0622\n",
            "Epoch: 009, Step: 472, Loss: 0.9738\n",
            "Epoch: 009, Step: 473, Loss: 0.9247\n",
            "Epoch: 009, Step: 474, Loss: 1.1666\n",
            "Epoch: 009, Step: 475, Loss: 1.1395\n",
            "Epoch: 009, Step: 476, Loss: 0.9009\n",
            "Epoch: 009, Step: 477, Loss: 1.1727\n",
            "Epoch: 009, Step: 478, Loss: 0.8775\n",
            "Epoch: 009, Step: 479, Loss: 0.8845\n",
            "Epoch: 009, Step: 480, Loss: 1.0128\n",
            "Epoch: 009, Step: 481, Loss: 0.9262\n",
            "Epoch: 009, Step: 482, Loss: 0.9569\n",
            "Epoch: 009, Step: 483, Loss: 1.2083\n",
            "Epoch: 009, Step: 484, Loss: 0.9893\n",
            "Epoch: 009, Step: 485, Loss: 0.8905\n",
            "Epoch: 009, Step: 486, Loss: 1.1061\n",
            "Epoch: 009, Step: 487, Loss: 1.0836\n",
            "Epoch: 009, Step: 488, Loss: 0.9728\n",
            "Epoch: 009, Step: 489, Loss: 0.8615\n",
            "Epoch: 009, Step: 490, Loss: 0.9804\n",
            "Epoch: 009, Step: 491, Loss: 1.0190\n",
            "Epoch: 009, Step: 492, Loss: 0.9870\n",
            "Epoch: 009, Step: 493, Loss: 1.0242\n",
            "Epoch: 009, Step: 494, Loss: 1.0729\n",
            "Epoch: 009, Step: 495, Loss: 0.9620\n",
            "Epoch: 009, Step: 496, Loss: 1.0608\n",
            "Epoch: 009, Step: 497, Loss: 0.9923\n",
            "Epoch: 009, Step: 498, Loss: 0.9840\n",
            "Epoch: 009, Step: 499, Loss: 1.0167\n",
            "Epoch: 009, Step: 500, Loss: 1.0712\n",
            "Epoch: 009, Step: 501, Loss: 0.9227\n",
            "Epoch: 009, Step: 502, Loss: 0.9867\n",
            "Epoch: 009, Step: 503, Loss: 0.8532\n",
            "Epoch: 009, Step: 504, Loss: 1.0614\n",
            "Epoch: 009, Step: 505, Loss: 0.7468\n",
            "Epoch: 009, Step: 506, Loss: 0.9812\n",
            "Epoch: 009, Step: 507, Loss: 0.9163\n",
            "Epoch: 009, Step: 508, Loss: 1.0552\n",
            "Epoch: 009, Step: 509, Loss: 0.9859\n",
            "Epoch: 009, Step: 510, Loss: 0.9837\n",
            "Epoch: 009, Step: 511, Loss: 0.9798\n",
            "Epoch: 009, Step: 512, Loss: 0.9395\n",
            "Epoch: 009, Step: 513, Loss: 0.9635\n",
            "Epoch: 009, Step: 514, Loss: 0.9479\n",
            "Epoch: 009, Step: 515, Loss: 0.9450\n",
            "Epoch: 009, Step: 516, Loss: 1.0256\n",
            "Epoch: 009, Step: 517, Loss: 0.9905\n",
            "Epoch: 009, Step: 518, Loss: 0.9765\n",
            "Epoch: 009, Step: 519, Loss: 0.8522\n",
            "Epoch: 009, Step: 520, Loss: 1.0224\n",
            "Epoch: 009, Step: 521, Loss: 1.0120\n",
            "Epoch: 009, Step: 522, Loss: 0.9640\n",
            "Epoch: 009, Step: 523, Loss: 0.9756\n",
            "Epoch: 009, Step: 524, Loss: 0.8232\n",
            "Epoch: 009, Step: 525, Loss: 0.9994\n",
            "Epoch: 009, Step: 526, Loss: 1.1751\n",
            "Epoch: 009, Step: 527, Loss: 0.9148\n",
            "Epoch: 009, Step: 528, Loss: 0.9132\n",
            "Epoch: 009, Step: 529, Loss: 0.9804\n",
            "Epoch: 009, Step: 530, Loss: 0.9003\n",
            "Epoch: 009, Step: 531, Loss: 1.0820\n",
            "Epoch: 009, Step: 532, Loss: 1.0487\n",
            "Epoch: 009, Step: 533, Loss: 0.8980\n",
            "Epoch: 009, Step: 534, Loss: 0.9583\n",
            "Epoch: 009, Step: 535, Loss: 0.8314\n",
            "Epoch: 009, Step: 536, Loss: 1.0347\n",
            "Epoch: 009, Step: 537, Loss: 1.0305\n",
            "Epoch: 009, Step: 538, Loss: 0.9226\n",
            "Epoch: 009, Step: 539, Loss: 0.9977\n",
            "Epoch: 009, Step: 540, Loss: 0.9474\n",
            "Epoch: 009, Step: 541, Loss: 0.9031\n",
            "Epoch: 009, Step: 542, Loss: 1.1457\n",
            "Epoch: 009, Step: 543, Loss: 0.9089\n",
            "Epoch: 009, Step: 544, Loss: 0.7989\n",
            "Epoch: 009, Step: 545, Loss: 0.8740\n",
            "Epoch: 009, Step: 546, Loss: 1.0331\n",
            "Epoch: 009, Step: 547, Loss: 0.9625\n",
            "Epoch: 009, Step: 548, Loss: 0.9877\n",
            "Epoch: 009, Step: 549, Loss: 1.1047\n",
            "Epoch: 009, Step: 550, Loss: 0.8852\n",
            "Epoch: 009, Step: 551, Loss: 0.8780\n",
            "Epoch: 009, Step: 552, Loss: 0.8493\n",
            "Epoch: 009, Step: 553, Loss: 0.8843\n",
            "Epoch: 009, Step: 554, Loss: 0.8583\n",
            "Epoch: 009, Step: 555, Loss: 0.9767\n",
            "Epoch: 009, Step: 556, Loss: 1.0912\n",
            "Epoch: 009, Step: 557, Loss: 1.0257\n",
            "Epoch: 009, Step: 558, Loss: 1.0127\n",
            "Epoch: 009, Step: 559, Loss: 0.9555\n",
            "Epoch: 009, Step: 560, Loss: 0.9217\n",
            "Epoch: 009, Step: 561, Loss: 0.9668\n",
            "Epoch: 009, Step: 562, Loss: 0.8290\n",
            "Epoch: 009, Step: 563, Loss: 0.8546\n",
            "Epoch: 009, Step: 564, Loss: 0.8880\n",
            "Epoch: 009, Step: 565, Loss: 1.1574\n",
            "Epoch: 009, Step: 566, Loss: 0.9959\n",
            "Epoch: 009, Step: 567, Loss: 0.9399\n",
            "Epoch: 009, Step: 568, Loss: 1.0401\n",
            "Epoch: 009, Step: 569, Loss: 1.1337\n",
            "Epoch: 009, Step: 570, Loss: 0.9790\n",
            "Epoch: 009, Step: 571, Loss: 0.8980\n",
            "Epoch: 009, Step: 572, Loss: 0.9044\n",
            "Epoch: 009, Step: 573, Loss: 0.9622\n",
            "Epoch: 009, Step: 574, Loss: 0.9126\n",
            "Epoch: 009, Step: 575, Loss: 0.9974\n",
            "Epoch: 009, Step: 576, Loss: 1.2361\n",
            "Epoch: 009, Step: 577, Loss: 0.8421\n",
            "Epoch: 009, Step: 578, Loss: 0.9519\n",
            "Epoch: 009, Step: 579, Loss: 1.1413\n",
            "Epoch: 009, Step: 580, Loss: 0.8879\n",
            "Epoch: 009, Step: 581, Loss: 0.9956\n",
            "Epoch: 009, Step: 582, Loss: 0.7639\n",
            "Epoch: 009, Step: 583, Loss: 0.8669\n",
            "Epoch: 009, Step: 584, Loss: 0.9732\n",
            "Epoch: 009, Step: 585, Loss: 0.9322\n",
            "Epoch: 009, Step: 586, Loss: 0.8249\n",
            "Epoch: 009, Step: 587, Loss: 1.1827\n",
            "Epoch: 009, Step: 588, Loss: 0.8562\n",
            "Epoch: 009, Step: 589, Loss: 1.0576\n",
            "Epoch: 009, Step: 590, Loss: 0.8822\n",
            "Epoch: 009, Step: 591, Loss: 0.9525\n",
            "Epoch: 009, Step: 592, Loss: 1.0107\n",
            "Epoch: 009, Step: 593, Loss: 0.9801\n",
            "Epoch: 009, Step: 594, Loss: 0.9972\n",
            "Epoch: 009, Step: 595, Loss: 0.9091\n",
            "Epoch: 009, Step: 596, Loss: 0.9835\n",
            "Epoch: 009, Step: 597, Loss: 1.0607\n",
            "Epoch: 009, Step: 598, Loss: 0.9533\n",
            "Epoch: 009, Step: 599, Loss: 1.0702\n",
            "Epoch: 009, Step: 600, Loss: 1.0388\n",
            "Epoch: 009, Step: 601, Loss: 0.9893\n",
            "Epoch: 009, Step: 602, Loss: 1.0405\n",
            "Epoch: 009, Step: 603, Loss: 0.9437\n",
            "Epoch: 009, Step: 604, Loss: 0.8603\n",
            "Epoch: 009, Step: 605, Loss: 0.9973\n",
            "Epoch: 009, Step: 606, Loss: 0.9243\n",
            "Epoch: 009, Step: 607, Loss: 0.8337\n",
            "Epoch: 009, Step: 608, Loss: 0.9271\n",
            "Epoch: 009, Step: 609, Loss: 0.9439\n",
            "Epoch: 009, Step: 610, Loss: 0.8637\n",
            "Epoch: 009, Step: 611, Loss: 0.8147\n",
            "Epoch: 009, Step: 612, Loss: 0.8364\n",
            "Epoch: 009, Step: 613, Loss: 0.8600\n",
            "Epoch: 009, Step: 614, Loss: 1.0315\n",
            "Epoch: 009, Step: 615, Loss: 1.0098\n",
            "Epoch: 009, Step: 616, Loss: 0.9405\n",
            "Epoch: 009, Step: 617, Loss: 0.9248\n",
            "Epoch: 009, Step: 618, Loss: 0.7358\n",
            "Epoch: 009, Step: 619, Loss: 1.1303\n",
            "Epoch: 009, Step: 620, Loss: 1.0072\n",
            "Epoch: 009, Step: 621, Loss: 0.9598\n",
            "Epoch: 009, Step: 622, Loss: 0.9420\n",
            "Epoch: 009, Step: 623, Loss: 0.9707\n",
            "Epoch: 009, Step: 624, Loss: 0.8989\n",
            "Epoch: 009, Step: 625, Loss: 1.0769\n",
            "Epoch: 009, Step: 626, Loss: 1.0887\n",
            "Epoch: 009, Step: 627, Loss: 1.0101\n",
            "Epoch: 009, Step: 628, Loss: 0.9495\n",
            "Epoch: 009, Step: 629, Loss: 1.0829\n",
            "Epoch: 009, Step: 630, Loss: 0.8815\n",
            "Epoch: 009, Step: 631, Loss: 1.0327\n",
            "Epoch: 009, Step: 632, Loss: 0.8616\n",
            "Epoch: 009, Step: 633, Loss: 0.8833\n",
            "Epoch: 009, Step: 634, Loss: 0.8309\n",
            "Epoch: 009, Step: 635, Loss: 0.8794\n",
            "Epoch: 009, Step: 636, Loss: 0.8646\n",
            "Epoch: 009, Step: 637, Loss: 0.9463\n",
            "Epoch: 009, Step: 638, Loss: 0.9263\n",
            "Epoch: 009, Step: 639, Loss: 0.8871\n",
            "Epoch: 009, Step: 640, Loss: 1.0682\n",
            "Epoch: 009, Step: 641, Loss: 0.8361\n",
            "Epoch: 009, Step: 642, Loss: 0.9923\n",
            "Epoch: 009, Step: 643, Loss: 0.9282\n",
            "Epoch: 009, Step: 644, Loss: 1.0705\n",
            "Epoch: 009, Step: 645, Loss: 0.9638\n",
            "Epoch: 009, Step: 646, Loss: 0.8953\n",
            "Epoch: 009, Step: 647, Loss: 0.8611\n",
            "Epoch: 009, Step: 648, Loss: 0.8052\n",
            "Epoch: 009, Step: 649, Loss: 0.9068\n",
            "Epoch: 009, Step: 650, Loss: 0.8528\n",
            "Epoch: 009, Step: 651, Loss: 0.8899\n",
            "Epoch: 009, Step: 652, Loss: 0.8374\n",
            "Epoch: 009, Step: 653, Loss: 0.9498\n",
            "Epoch: 009, Step: 654, Loss: 1.0569\n",
            "Epoch: 009, Step: 655, Loss: 0.8694\n",
            "Epoch: 009, Step: 656, Loss: 1.0267\n",
            "Epoch: 009, Step: 657, Loss: 0.8992\n",
            "Epoch: 009, Step: 658, Loss: 0.9384\n",
            "Epoch: 009, Step: 659, Loss: 0.7841\n",
            "Epoch: 009, Step: 660, Loss: 1.0633\n",
            "Epoch: 009, Step: 661, Loss: 0.9456\n",
            "Epoch: 009, Step: 662, Loss: 0.9129\n",
            "Epoch: 009, Step: 663, Loss: 0.9498\n",
            "Epoch: 009, Step: 664, Loss: 0.8609\n",
            "Epoch: 009, Step: 665, Loss: 0.9815\n",
            "Epoch: 009, Step: 666, Loss: 0.9370\n",
            "Epoch: 009, Step: 667, Loss: 0.8830\n",
            "Epoch: 009, Step: 668, Loss: 0.9509\n",
            "Epoch: 009, Step: 669, Loss: 0.9541\n",
            "Epoch: 009, Step: 670, Loss: 1.0181\n",
            "Epoch: 009, Step: 671, Loss: 0.9610\n",
            "Epoch: 009, Step: 672, Loss: 0.9692\n",
            "Epoch: 009, Step: 673, Loss: 1.0162\n",
            "Epoch: 009, Step: 674, Loss: 0.8437\n",
            "Epoch: 009, Step: 675, Loss: 1.0299\n",
            "Epoch: 009, Step: 676, Loss: 0.8071\n",
            "Epoch: 009, Step: 677, Loss: 0.8904\n",
            "Epoch: 009, Step: 678, Loss: 1.3393\n",
            "Epoch: 009, Step: 679, Loss: 0.9726\n",
            "Epoch: 009, Step: 680, Loss: 0.9436\n",
            "Epoch: 009, Step: 681, Loss: 0.9775\n",
            "Epoch: 009, Step: 682, Loss: 0.9216\n",
            "Epoch: 009, Step: 683, Loss: 0.8523\n",
            "Epoch: 009, Step: 684, Loss: 0.9145\n",
            "Epoch: 009, Step: 685, Loss: 0.9258\n",
            "Epoch: 009, Step: 686, Loss: 0.8079\n",
            "Epoch: 009, Step: 687, Loss: 1.0180\n",
            "Epoch: 009, Step: 688, Loss: 0.9253\n",
            "Epoch: 009, Step: 689, Loss: 0.9925\n",
            "Epoch: 009, Step: 690, Loss: 0.8375\n",
            "Epoch: 009, Step: 691, Loss: 0.8284\n",
            "Epoch: 009, Step: 692, Loss: 1.0785\n",
            "Epoch: 009, Step: 693, Loss: 0.8787\n",
            "Epoch: 009, Step: 694, Loss: 0.7874\n",
            "Epoch: 009, Step: 695, Loss: 0.9601\n",
            "Epoch: 009, Step: 696, Loss: 0.8333\n",
            "Epoch: 009, Step: 697, Loss: 0.8561\n",
            "Epoch: 009, Step: 698, Loss: 1.0099\n",
            "Epoch: 009, Step: 699, Loss: 1.0021\n",
            "Epoch: 009, Step: 700, Loss: 0.8969\n",
            "Epoch: 009, Step: 701, Loss: 0.9626\n",
            "Epoch: 009, Step: 702, Loss: 0.9438\n",
            "Epoch: 009, Step: 703, Loss: 0.9008\n",
            "Epoch: 009, Step: 704, Loss: 0.8472\n",
            "Epoch: 009, Step: 705, Loss: 0.8892\n",
            "Epoch: 009, Step: 706, Loss: 1.1116\n",
            "Epoch: 009, Step: 707, Loss: 0.9999\n",
            "Epoch: 009, Step: 708, Loss: 1.0262\n",
            "Epoch: 009, Step: 709, Loss: 0.9509\n",
            "Epoch: 009, Step: 710, Loss: 1.0538\n",
            "Epoch: 009, Step: 711, Loss: 1.0257\n",
            "Epoch: 009, Step: 712, Loss: 0.9538\n",
            "Epoch: 009, Step: 713, Loss: 0.9758\n",
            "Epoch: 009, Step: 714, Loss: 0.8234\n",
            "Epoch: 009, Step: 715, Loss: 0.9189\n",
            "Epoch: 009, Step: 716, Loss: 0.9991\n",
            "Epoch: 009, Step: 717, Loss: 1.1488\n",
            "Epoch: 009, Step: 718, Loss: 0.9953\n",
            "Epoch: 009, Step: 719, Loss: 0.8759\n",
            "Epoch: 009, Step: 720, Loss: 0.8730\n",
            "Epoch: 009, Step: 721, Loss: 0.8513\n",
            "Epoch: 009, Step: 722, Loss: 0.7873\n",
            "Epoch: 009, Step: 723, Loss: 0.9061\n",
            "Epoch: 009, Step: 724, Loss: 0.7903\n",
            "Epoch: 009, Step: 725, Loss: 0.9574\n",
            "Epoch: 009, Step: 726, Loss: 1.0914\n",
            "Epoch: 009, Step: 727, Loss: 0.8768\n",
            "Epoch: 009, Step: 728, Loss: 0.9933\n",
            "Epoch: 009, Step: 729, Loss: 0.9126\n",
            "Epoch: 009, Step: 730, Loss: 1.0366\n",
            "Epoch: 009, Step: 731, Loss: 0.9055\n",
            "Epoch: 009, Step: 732, Loss: 0.8618\n",
            "Epoch: 009, Step: 733, Loss: 1.1665\n",
            "Epoch: 009, Step: 734, Loss: 0.8623\n",
            "Epoch: 009, Step: 735, Loss: 1.1011\n",
            "Epoch: 009, Step: 736, Loss: 1.0383\n",
            "Epoch: 009, Step: 737, Loss: 1.1189\n",
            "Epoch: 009, Step: 738, Loss: 0.9687\n",
            "Epoch: 009, Step: 739, Loss: 0.9186\n",
            "Epoch: 009, Step: 740, Loss: 0.9530\n",
            "Epoch: 009, Step: 741, Loss: 1.0580\n",
            "Epoch: 009, Step: 742, Loss: 0.9247\n",
            "Epoch: 009, Step: 743, Loss: 1.0439\n",
            "Epoch: 009, Step: 744, Loss: 0.9772\n",
            "Epoch: 009, Step: 745, Loss: 0.9380\n",
            "Epoch: 009, Step: 746, Loss: 1.0712\n",
            "Epoch: 009, Step: 747, Loss: 0.9414\n",
            "Epoch: 009, Step: 748, Loss: 0.8156\n",
            "Epoch: 009, Step: 749, Loss: 0.9589\n",
            "Epoch: 009, Step: 750, Loss: 0.9654\n",
            "Epoch: 009, Step: 751, Loss: 0.8058\n",
            "Epoch: 009, Step: 752, Loss: 1.1017\n",
            "Epoch: 009, Step: 753, Loss: 0.9027\n",
            "Epoch: 009, Step: 754, Loss: 0.9949\n",
            "Epoch: 009, Step: 755, Loss: 0.9988\n",
            "Epoch: 009, Step: 756, Loss: 0.8519\n",
            "Epoch: 009, Step: 757, Loss: 0.8650\n",
            "Epoch: 009, Step: 758, Loss: 0.8433\n",
            "Epoch: 009, Step: 759, Loss: 1.0230\n",
            "Epoch: 009, Step: 760, Loss: 0.8832\n",
            "Epoch: 009, Step: 761, Loss: 0.8425\n",
            "Epoch: 009, Step: 762, Loss: 0.8290\n",
            "Epoch: 009, Step: 763, Loss: 0.9442\n",
            "Epoch: 009, Step: 764, Loss: 0.9054\n",
            "Epoch: 009, Step: 765, Loss: 0.9091\n",
            "Epoch: 009, Step: 766, Loss: 0.7172\n",
            "Epoch: 009, Step: 767, Loss: 0.9101\n",
            "Epoch: 009, Step: 768, Loss: 1.1314\n",
            "Epoch: 009, Step: 769, Loss: 0.9554\n",
            "Epoch: 009, Step: 770, Loss: 0.9845\n",
            "Epoch: 009, Step: 771, Loss: 0.8996\n",
            "Epoch: 009, Step: 772, Loss: 0.8707\n",
            "Epoch: 009, Step: 773, Loss: 0.9081\n",
            "Epoch: 009, Step: 774, Loss: 0.9441\n",
            "Epoch: 009, Step: 775, Loss: 0.9196\n",
            "Epoch: 009, Step: 776, Loss: 0.8212\n",
            "Epoch: 009, Step: 777, Loss: 1.0190\n",
            "Epoch: 009, Step: 778, Loss: 1.0031\n",
            "Epoch: 009, Step: 779, Loss: 0.9756\n",
            "Epoch: 009, Step: 780, Loss: 0.8614\n",
            "Epoch: 009, Step: 781, Loss: 0.9978\n",
            "Epoch: 009, Step: 782, Loss: 0.9573\n",
            "Epoch: 009, Step: 783, Loss: 0.8352\n",
            "Epoch: 009, Step: 784, Loss: 1.1492\n",
            "Epoch: 009, Step: 785, Loss: 0.9235\n",
            "Epoch: 009, Step: 786, Loss: 1.0581\n",
            "Epoch: 009, Step: 787, Loss: 0.8650\n",
            "Epoch: 009, Step: 788, Loss: 0.9006\n",
            "Epoch: 009, Step: 789, Loss: 0.8004\n",
            "Epoch: 009, Step: 790, Loss: 0.9269\n",
            "Epoch: 009, Step: 791, Loss: 0.9084\n",
            "Epoch: 009, Step: 792, Loss: 0.8907\n",
            "Epoch: 009, Step: 793, Loss: 0.7962\n",
            "Epoch: 009, Step: 794, Loss: 0.8431\n",
            "Epoch: 009, Step: 795, Loss: 0.8516\n",
            "Epoch: 009, Step: 796, Loss: 1.0248\n",
            "Epoch: 009, Step: 797, Loss: 0.9010\n",
            "Epoch: 009, Step: 798, Loss: 0.8652\n",
            "Epoch: 009, Step: 799, Loss: 1.0893\n",
            "Epoch: 009, Step: 800, Loss: 0.9159\n",
            "Epoch: 009, Step: 801, Loss: 1.1706\n",
            "Epoch: 009, Step: 802, Loss: 1.0750\n",
            "Epoch: 009, Step: 803, Loss: 0.9960\n",
            "Epoch: 009, Step: 804, Loss: 0.9775\n",
            "Epoch: 009, Step: 805, Loss: 1.0810\n",
            "Epoch: 009, Step: 806, Loss: 0.7797\n",
            "Epoch: 009, Step: 807, Loss: 0.9327\n",
            "Epoch: 009, Step: 808, Loss: 1.0678\n",
            "Epoch: 009, Step: 809, Loss: 0.8123\n",
            "Epoch: 009, Step: 810, Loss: 0.9591\n",
            "Epoch: 009, Step: 811, Loss: 0.9969\n",
            "Epoch: 009, Step: 812, Loss: 1.1003\n",
            "Epoch: 009, Step: 813, Loss: 1.1555\n",
            "Epoch: 009, Step: 814, Loss: 1.0973\n",
            "Epoch: 009, Step: 815, Loss: 0.9528\n",
            "Epoch: 009, Step: 816, Loss: 0.9131\n",
            "Epoch: 009, Step: 817, Loss: 1.0650\n",
            "Epoch: 009, Step: 818, Loss: 0.9865\n",
            "Epoch: 009, Step: 819, Loss: 0.7941\n",
            "Epoch: 009, Step: 820, Loss: 0.9266\n",
            "Epoch: 009, Step: 821, Loss: 0.9988\n",
            "Epoch: 009, Step: 822, Loss: 0.8855\n",
            "Epoch: 009, Step: 823, Loss: 1.1434\n",
            "Epoch: 009, Step: 824, Loss: 0.8649\n",
            "Epoch: 009, Step: 825, Loss: 0.9804\n",
            "Epoch: 009, Step: 826, Loss: 0.9163\n",
            "Epoch: 009, Step: 827, Loss: 0.9084\n",
            "Epoch: 009, Step: 828, Loss: 0.9012\n",
            "Epoch: 009, Step: 829, Loss: 0.9958\n",
            "Epoch: 009, Step: 830, Loss: 1.0321\n",
            "Epoch: 009, Step: 831, Loss: 1.0292\n",
            "Epoch: 009, Step: 832, Loss: 0.9261\n",
            "Epoch: 009, Step: 833, Loss: 1.0319\n",
            "Epoch: 009, Step: 834, Loss: 0.9269\n",
            "Epoch: 009, Step: 835, Loss: 1.1510\n",
            "Epoch: 009, Step: 836, Loss: 0.9763\n",
            "Epoch: 009, Step: 837, Loss: 1.0142\n",
            "Epoch: 009, Step: 838, Loss: 0.9486\n",
            "Epoch: 009, Step: 839, Loss: 0.9419\n",
            "Epoch: 009, Step: 840, Loss: 0.7615\n",
            "Epoch: 009, Step: 841, Loss: 0.9171\n",
            "Epoch: 009, Step: 842, Loss: 0.9959\n",
            "Epoch: 009, Step: 843, Loss: 0.9265\n",
            "Epoch: 009, Step: 844, Loss: 1.0099\n",
            "Epoch: 009, Step: 845, Loss: 0.8922\n",
            "Epoch: 009, Step: 846, Loss: 0.9489\n",
            "Epoch: 009, Step: 847, Loss: 0.9712\n",
            "Epoch: 009, Step: 848, Loss: 0.9813\n",
            "Epoch: 009, Step: 849, Loss: 1.0038\n",
            "Epoch: 009, Step: 850, Loss: 0.7740\n",
            "Epoch: 009, Step: 851, Loss: 0.8650\n",
            "Epoch: 009, Step: 852, Loss: 0.7739\n",
            "Epoch: 009, Step: 853, Loss: 1.0990\n",
            "Epoch: 009, Step: 854, Loss: 1.0623\n",
            "Epoch: 009, Step: 855, Loss: 1.0334\n",
            "Epoch: 009, Step: 856, Loss: 0.9139\n",
            "Epoch: 009, Step: 857, Loss: 0.8834\n",
            "Epoch: 009, Step: 858, Loss: 1.0815\n",
            "Epoch: 009, Step: 859, Loss: 1.0317\n",
            "Epoch: 009, Step: 860, Loss: 1.0160\n",
            "Epoch: 009, Step: 861, Loss: 0.8809\n",
            "Epoch: 009, Step: 862, Loss: 0.8642\n",
            "Epoch: 009, Step: 863, Loss: 0.9835\n",
            "Epoch: 009, Step: 864, Loss: 0.9962\n",
            "Epoch: 009, Step: 865, Loss: 0.7870\n",
            "Epoch: 009, Step: 866, Loss: 0.9469\n",
            "Epoch: 009, Step: 867, Loss: 1.0584\n",
            "Epoch: 009, Step: 868, Loss: 0.8981\n",
            "Epoch: 009, Step: 869, Loss: 0.8439\n",
            "Epoch: 009, Step: 870, Loss: 1.3265\n",
            "Epoch: 009, Step: 871, Loss: 0.8202\n",
            "Epoch: 009, Step: 872, Loss: 0.9783\n",
            "Epoch: 009, Step: 873, Loss: 1.0266\n",
            "Epoch: 009, Step: 874, Loss: 0.9315\n",
            "Epoch: 009, Step: 875, Loss: 1.0923\n",
            "Epoch: 009, Step: 876, Loss: 0.8741\n",
            "Epoch: 009, Step: 877, Loss: 1.1176\n",
            "Epoch: 009, Step: 878, Loss: 0.8211\n",
            "Epoch: 009, Step: 879, Loss: 0.9992\n",
            "Epoch: 009, Step: 880, Loss: 0.9170\n",
            "Epoch: 009, Step: 881, Loss: 0.9099\n",
            "Epoch: 009, Step: 882, Loss: 0.9973\n",
            "Epoch: 009, Step: 883, Loss: 0.8682\n",
            "Epoch: 009, Step: 884, Loss: 0.8755\n",
            "Epoch: 009, Step: 885, Loss: 0.9382\n",
            "Epoch: 009, Step: 886, Loss: 1.0109\n",
            "Epoch: 009, Step: 887, Loss: 1.0244\n",
            "Epoch: 009, Step: 888, Loss: 1.0376\n",
            "Epoch: 009, Step: 889, Loss: 0.8910\n",
            "Epoch: 009, Step: 890, Loss: 0.9204\n",
            "Epoch: 009, Step: 891, Loss: 0.8880\n",
            "Epoch: 009, Step: 892, Loss: 0.9091\n",
            "Epoch: 009, Step: 893, Loss: 0.8317\n",
            "Epoch: 009, Step: 894, Loss: 0.8385\n",
            "Epoch: 009, Step: 895, Loss: 1.0669\n",
            "Epoch: 009, Step: 896, Loss: 0.9985\n",
            "Epoch: 009, Step: 897, Loss: 0.9956\n",
            "Epoch: 009, Step: 898, Loss: 0.9566\n",
            "Epoch: 009, Step: 899, Loss: 0.9302\n",
            "Epoch: 009, Step: 900, Loss: 1.0546\n",
            "Epoch: 009, Step: 901, Loss: 0.9195\n",
            "Epoch: 009, Step: 902, Loss: 1.0192\n",
            "Epoch: 009, Step: 903, Loss: 0.9650\n",
            "Epoch: 009, Step: 904, Loss: 0.9879\n",
            "Epoch: 009, Step: 905, Loss: 0.9527\n",
            "Epoch: 009, Step: 906, Loss: 0.9231\n",
            "Epoch: 009, Step: 907, Loss: 0.9089\n",
            "Epoch: 009, Step: 908, Loss: 0.6949\n",
            "Epoch: 009, Step: 909, Loss: 0.8535\n",
            "Epoch: 009, Step: 910, Loss: 0.7914\n",
            "Epoch: 009, Step: 911, Loss: 0.9302\n",
            "Epoch: 009, Step: 912, Loss: 0.9127\n",
            "Epoch: 009, Step: 913, Loss: 1.0273\n",
            "Epoch: 009, Step: 914, Loss: 0.8760\n",
            "Epoch: 009, Step: 915, Loss: 1.0870\n",
            "Epoch: 009, Step: 916, Loss: 0.8895\n",
            "Epoch: 009, Step: 917, Loss: 0.8929\n",
            "Epoch: 009, Step: 918, Loss: 0.9058\n",
            "Epoch: 009, Step: 919, Loss: 0.9910\n",
            "Epoch: 009, Step: 920, Loss: 0.8443\n",
            "Epoch: 009, Step: 921, Loss: 0.8833\n",
            "Epoch: 009, Step: 922, Loss: 1.1211\n",
            "Epoch: 009, Step: 923, Loss: 0.9479\n",
            "Epoch: 009, Step: 924, Loss: 1.1629\n",
            "Epoch: 009, Step: 925, Loss: 0.8635\n",
            "Epoch: 009, Step: 926, Loss: 0.7981\n",
            "Epoch: 009, Step: 927, Loss: 0.7340\n",
            "Epoch: 009, Step: 928, Loss: 0.8706\n",
            "Epoch: 009, Step: 929, Loss: 0.9131\n",
            "Epoch: 009, Step: 930, Loss: 0.9680\n",
            "Epoch: 009, Step: 931, Loss: 0.9300\n",
            "Epoch: 009, Step: 932, Loss: 0.9318\n",
            "Epoch: 009, Step: 933, Loss: 0.9298\n",
            "Epoch: 009, Step: 934, Loss: 0.9812\n",
            "Epoch: 009, Step: 935, Loss: 0.8846\n",
            "Epoch: 009, Step: 936, Loss: 0.9930\n",
            "Epoch: 009, Step: 937, Loss: 0.9666\n",
            "Epoch: 009, Step: 938, Loss: 0.8828\n",
            "Epoch: 009, Step: 939, Loss: 0.9798\n",
            "Epoch: 009, Step: 940, Loss: 0.9019\n",
            "Epoch: 009, Step: 941, Loss: 0.9589\n",
            "Epoch: 009, Step: 942, Loss: 0.9967\n",
            "Epoch: 009, Step: 943, Loss: 0.9108\n",
            "Epoch: 009, Step: 944, Loss: 0.8108\n",
            "Epoch: 009, Step: 945, Loss: 1.1683\n",
            "Epoch: 009, Step: 946, Loss: 1.1016\n",
            "Epoch: 009, Step: 947, Loss: 0.8653\n",
            "Epoch: 009, Step: 948, Loss: 1.0794\n",
            "Epoch: 009, Step: 949, Loss: 0.9978\n",
            "Epoch: 009, Step: 950, Loss: 0.8473\n",
            "Epoch: 009, Step: 951, Loss: 1.0819\n",
            "Epoch: 009, Step: 952, Loss: 0.9828\n",
            "Epoch: 009, Step: 953, Loss: 1.0075\n",
            "Epoch: 009, Step: 954, Loss: 0.9396\n",
            "Epoch: 009, Step: 955, Loss: 1.1654\n",
            "Epoch: 009, Step: 956, Loss: 0.9393\n",
            "Epoch: 009, Step: 957, Loss: 0.9473\n",
            "Epoch: 009, Step: 958, Loss: 0.9828\n",
            "Epoch: 009, Step: 959, Loss: 1.0649\n",
            "Epoch: 009, Step: 960, Loss: 1.0581\n",
            "Epoch: 009, Step: 961, Loss: 0.8987\n",
            "Epoch: 009, Step: 962, Loss: 1.0837\n",
            "Epoch: 009, Step: 963, Loss: 0.9050\n",
            "Epoch: 009, Step: 964, Loss: 1.0587\n",
            "Epoch: 009, Step: 965, Loss: 1.0308\n",
            "Epoch: 009, Step: 966, Loss: 0.8127\n",
            "Epoch: 009, Step: 967, Loss: 1.0917\n",
            "Epoch: 009, Step: 968, Loss: 0.9473\n",
            "Epoch: 009, Step: 969, Loss: 0.9273\n",
            "Epoch: 009, Step: 970, Loss: 0.9380\n",
            "Epoch: 009, Step: 971, Loss: 0.8816\n",
            "Epoch: 009, Step: 972, Loss: 0.7735\n",
            "Epoch: 009, Step: 973, Loss: 1.0535\n",
            "Epoch: 009, Step: 974, Loss: 0.9184\n",
            "Epoch: 009, Step: 975, Loss: 0.9824\n",
            "Epoch: 009, Step: 976, Loss: 0.9469\n",
            "Epoch: 009, Step: 977, Loss: 0.8557\n",
            "Epoch: 009, Step: 978, Loss: 0.8527\n",
            "Epoch: 009, Step: 979, Loss: 0.9743\n",
            "Epoch: 009, Step: 980, Loss: 0.9261\n",
            "Epoch: 009, Step: 981, Loss: 1.0620\n",
            "Epoch: 009, Step: 982, Loss: 1.0689\n",
            "Epoch: 009, Step: 983, Loss: 0.7433\n",
            "Epoch: 009, Step: 984, Loss: 0.8909\n",
            "Epoch: 009, Step: 985, Loss: 0.9876\n",
            "Epoch: 009, Step: 986, Loss: 0.8762\n",
            "Epoch: 009, Step: 987, Loss: 1.0100\n",
            "Epoch: 009, Step: 988, Loss: 1.1619\n",
            "Epoch: 009, Step: 989, Loss: 0.9725\n",
            "Epoch: 009, Step: 990, Loss: 0.9135\n",
            "Epoch: 009, Step: 991, Loss: 0.9907\n",
            "Epoch: 009, Step: 992, Loss: 0.8772\n",
            "Epoch: 009, Step: 993, Loss: 1.1009\n",
            "Epoch: 009, Step: 994, Loss: 1.0805\n",
            "Epoch: 009, Step: 995, Loss: 0.9648\n",
            "Epoch: 009, Step: 996, Loss: 0.9128\n",
            "Epoch: 009, Step: 997, Loss: 0.8870\n",
            "Epoch: 009, Step: 998, Loss: 0.8199\n",
            "Epoch: 009, Step: 999, Loss: 0.7733\n",
            "Epoch: 009, Step: 1000, Loss: 0.8338\n",
            "Epoch: 009, Step: 1001, Loss: 0.7484\n",
            "Epoch: 009, Step: 1002, Loss: 0.8707\n",
            "Epoch: 009, Step: 1003, Loss: 0.9381\n",
            "Epoch: 009, Step: 1004, Loss: 0.8936\n",
            "Epoch: 009, Step: 1005, Loss: 0.9942\n",
            "Epoch: 009, Step: 1006, Loss: 0.9319\n",
            "Epoch: 009, Step: 1007, Loss: 1.0822\n",
            "Epoch: 009, Step: 1008, Loss: 0.8587\n",
            "Epoch: 009, Step: 1009, Loss: 0.9672\n",
            "Epoch: 009, Step: 1010, Loss: 1.0818\n",
            "Epoch: 009, Step: 1011, Loss: 0.9638\n",
            "Epoch: 009, Step: 1012, Loss: 0.9634\n",
            "Epoch: 009, Step: 1013, Loss: 1.0424\n",
            "Epoch: 009, Step: 1014, Loss: 1.0471\n",
            "Epoch: 009, Step: 1015, Loss: 0.8663\n",
            "Epoch: 009, Step: 1016, Loss: 0.8087\n",
            "Epoch: 009, Step: 1017, Loss: 1.0242\n",
            "Epoch: 009, Step: 1018, Loss: 1.0248\n",
            "Epoch: 009, Step: 1019, Loss: 0.9939\n",
            "Epoch: 009, Step: 1020, Loss: 1.0041\n",
            "Epoch: 009, Step: 1021, Loss: 0.9910\n",
            "Epoch: 009, Step: 1022, Loss: 1.0005\n",
            "Epoch: 009, Step: 1023, Loss: 1.0301\n",
            "Epoch: 009, Step: 1024, Loss: 0.9977\n",
            "Epoch: 009, Step: 1025, Loss: 0.8828\n",
            "Epoch: 009, Step: 1026, Loss: 0.8005\n",
            "Epoch: 009, Step: 1027, Loss: 0.9960\n",
            "Epoch: 009, Step: 1028, Loss: 1.1787\n",
            "Epoch: 009, Step: 1029, Loss: 1.0284\n",
            "Epoch: 009, Step: 1030, Loss: 0.9405\n",
            "Epoch: 009, Step: 1031, Loss: 0.9703\n",
            "Epoch: 009, Step: 1032, Loss: 1.0353\n",
            "Epoch: 009, Step: 1033, Loss: 0.8576\n",
            "Epoch: 009, Step: 1034, Loss: 0.9880\n",
            "Epoch: 009, Step: 1035, Loss: 1.0166\n",
            "Epoch: 009, Step: 1036, Loss: 0.7904\n",
            "Epoch: 009, Step: 1037, Loss: 0.8500\n",
            "Epoch: 009, Step: 1038, Loss: 0.8584\n",
            "Epoch: 009, Step: 1039, Loss: 1.1461\n",
            "Epoch: 009, Step: 1040, Loss: 0.9647\n",
            "Epoch: 009, Step: 1041, Loss: 1.0576\n",
            "Epoch: 009, Step: 1042, Loss: 0.9005\n",
            "Epoch: 009, Step: 1043, Loss: 0.8325\n",
            "Epoch: 009, Step: 1044, Loss: 0.8766\n",
            "Epoch: 009, Step: 1045, Loss: 1.3312\n",
            "Epoch: 009, Step: 1046, Loss: 0.9226\n",
            "Epoch: 009, Step: 1047, Loss: 1.0322\n",
            "Epoch: 009, Step: 1048, Loss: 1.0518\n",
            "Epoch: 009, Step: 1049, Loss: 0.8864\n",
            "Epoch: 009, Step: 1050, Loss: 0.9652\n",
            "Epoch: 009, Step: 1051, Loss: 1.0339\n",
            "Epoch: 009, Step: 1052, Loss: 0.9768\n",
            "Epoch: 009, Step: 1053, Loss: 0.9816\n",
            "Epoch: 009, Step: 1054, Loss: 1.0371\n",
            "Epoch: 009, Step: 1055, Loss: 0.9741\n",
            "Epoch: 009, Step: 1056, Loss: 1.0259\n",
            "Epoch: 009, Step: 1057, Loss: 0.9073\n",
            "Epoch: 009, Step: 1058, Loss: 0.9508\n",
            "Epoch: 009, Step: 1059, Loss: 1.0236\n",
            "Epoch: 009, Step: 1060, Loss: 1.0552\n",
            "Epoch: 009, Step: 1061, Loss: 1.0138\n",
            "Epoch: 009, Step: 1062, Loss: 1.0218\n",
            "Epoch: 009, Step: 1063, Loss: 1.0120\n",
            "Epoch: 009, Step: 1064, Loss: 0.9262\n",
            "Epoch: 009, Step: 1065, Loss: 1.0155\n",
            "Epoch: 009, Step: 1066, Loss: 0.8820\n",
            "Epoch: 009, Step: 1067, Loss: 0.8659\n",
            "Epoch: 009, Step: 1068, Loss: 0.9179\n",
            "Epoch: 009, Step: 1069, Loss: 1.0865\n",
            "Epoch: 009, Step: 1070, Loss: 0.8815\n",
            "Epoch: 009, Step: 1071, Loss: 0.8667\n",
            "Epoch: 009, Step: 1072, Loss: 0.9954\n",
            "Epoch: 009, Step: 1073, Loss: 0.9182\n",
            "Epoch: 009, Step: 1074, Loss: 1.0572\n",
            "Epoch: 009, Step: 1075, Loss: 0.9900\n",
            "Epoch: 009, Step: 1076, Loss: 0.9193\n",
            "Epoch: 009, Step: 1077, Loss: 0.9857\n",
            "Epoch: 009, Step: 1078, Loss: 1.1015\n",
            "Epoch: 009, Step: 1079, Loss: 0.9168\n",
            "Epoch: 009, Step: 1080, Loss: 1.0700\n",
            "Epoch: 009, Step: 1081, Loss: 0.9429\n",
            "Epoch: 009, Step: 1082, Loss: 1.1247\n",
            "Epoch: 009, Step: 1083, Loss: 0.9137\n",
            "Epoch: 009, Step: 1084, Loss: 0.8565\n",
            "Epoch: 009, Step: 1085, Loss: 1.0423\n",
            "Epoch: 009, Step: 1086, Loss: 1.0342\n",
            "Epoch: 009, Step: 1087, Loss: 0.9212\n",
            "Epoch: 009, Step: 1088, Loss: 0.8699\n",
            "Epoch: 009, Step: 1089, Loss: 1.0372\n",
            "Epoch: 009, Step: 1090, Loss: 0.9118\n",
            "Epoch: 009, Step: 1091, Loss: 0.9974\n",
            "Epoch: 009, Step: 1092, Loss: 0.9266\n",
            "Epoch: 009, Step: 1093, Loss: 0.9975\n",
            "Epoch: 009, Step: 1094, Loss: 1.0271\n",
            "Epoch: 009, Step: 1095, Loss: 0.9466\n",
            "Epoch: 009, Step: 1096, Loss: 1.0635\n",
            "Epoch: 009, Step: 1097, Loss: 0.9629\n",
            "Epoch: 009, Step: 1098, Loss: 0.9973\n",
            "Epoch: 009, Step: 1099, Loss: 0.8871\n",
            "Epoch: 009, Step: 1100, Loss: 0.9538\n",
            "Epoch: 009, Step: 1101, Loss: 0.8379\n",
            "Epoch: 009, Step: 1102, Loss: 1.0538\n",
            "Epoch: 009, Step: 1103, Loss: 1.1112\n",
            "Epoch: 009, Step: 1104, Loss: 0.9461\n",
            "Epoch: 009, Step: 1105, Loss: 1.0390\n",
            "Epoch: 009, Step: 1106, Loss: 1.0774\n",
            "Epoch: 009, Step: 1107, Loss: 1.0812\n",
            "Epoch: 009, Step: 1108, Loss: 1.1242\n",
            "Epoch: 009, Step: 1109, Loss: 1.0863\n",
            "Epoch: 009, Step: 1110, Loss: 0.9023\n",
            "Epoch: 009, Step: 1111, Loss: 0.8392\n",
            "Epoch: 009, Step: 1112, Loss: 1.0188\n",
            "Epoch: 009, Step: 1113, Loss: 0.9940\n",
            "Epoch: 009, Step: 1114, Loss: 1.0250\n",
            "Epoch: 009, Step: 1115, Loss: 1.1307\n",
            "Epoch: 009, Step: 1116, Loss: 0.8858\n",
            "Epoch: 009, Step: 1117, Loss: 0.8421\n",
            "Epoch: 009, Step: 1118, Loss: 1.1308\n",
            "Epoch: 009, Step: 1119, Loss: 1.0204\n",
            "Epoch: 009, Step: 1120, Loss: 0.8790\n",
            "Epoch: 009, Step: 1121, Loss: 1.0429\n",
            "Epoch: 009, Step: 1122, Loss: 1.0756\n",
            "Epoch: 009, Step: 1123, Loss: 1.2249\n",
            "Epoch: 009, Step: 1124, Loss: 1.0111\n",
            "Epoch: 009, Step: 1125, Loss: 1.0407\n",
            "Epoch: 009, Step: 1126, Loss: 0.9627\n",
            "Epoch: 009, Step: 1127, Loss: 0.9387\n",
            "Epoch: 009, Step: 1128, Loss: 1.0105\n",
            "Epoch: 009, Step: 1129, Loss: 0.9050\n",
            "Epoch: 009, Step: 1130, Loss: 1.0968\n",
            "Epoch: 009, Step: 1131, Loss: 0.9622\n",
            "Epoch: 009, Step: 1132, Loss: 1.0471\n",
            "Epoch: 009, Step: 1133, Loss: 0.9733\n",
            "Epoch: 009, Step: 1134, Loss: 1.0842\n",
            "Epoch: 009, Step: 1135, Loss: 0.8732\n",
            "Epoch: 009, Step: 1136, Loss: 0.9811\n",
            "Epoch: 009, Step: 1137, Loss: 0.9969\n",
            "Epoch: 009, Step: 1138, Loss: 1.0671\n",
            "Epoch: 009, Step: 1139, Loss: 0.8677\n",
            "Epoch: 009, Step: 1140, Loss: 0.8259\n",
            "Epoch: 009, Step: 1141, Loss: 0.8772\n",
            "Epoch: 009, Step: 1142, Loss: 0.9894\n",
            "Epoch: 009, Step: 1143, Loss: 0.9164\n",
            "Epoch: 009, Step: 1144, Loss: 0.8747\n",
            "Epoch: 009, Step: 1145, Loss: 0.9508\n",
            "Epoch: 009, Step: 1146, Loss: 0.8535\n",
            "Epoch: 009, Step: 1147, Loss: 0.9351\n",
            "Epoch: 009, Step: 1148, Loss: 0.8621\n",
            "Epoch: 009, Step: 1149, Loss: 0.9817\n",
            "Epoch: 009, Step: 1150, Loss: 0.8370\n",
            "Epoch: 009, Step: 1151, Loss: 1.1552\n",
            "Epoch: 009, Step: 1152, Loss: 0.8874\n",
            "Epoch: 009, Step: 1153, Loss: 0.8534\n",
            "Epoch: 009, Step: 1154, Loss: 0.9856\n",
            "Epoch: 009, Step: 1155, Loss: 0.8921\n",
            "Epoch: 009, Step: 1156, Loss: 0.8205\n",
            "Epoch: 009, Step: 1157, Loss: 0.8552\n",
            "Epoch: 009, Step: 1158, Loss: 0.8939\n",
            "Epoch: 009, Step: 1159, Loss: 1.0974\n",
            "Epoch: 009, Step: 1160, Loss: 1.1389\n",
            "Epoch: 009, Step: 1161, Loss: 0.8748\n",
            "Epoch: 009, Step: 1162, Loss: 1.0498\n",
            "Epoch: 009, Step: 1163, Loss: 1.1002\n",
            "Epoch: 009, Step: 1164, Loss: 0.8750\n",
            "Epoch: 009, Step: 1165, Loss: 1.1204\n",
            "Epoch: 009, Step: 1166, Loss: 0.8439\n",
            "Epoch: 009, Step: 1167, Loss: 0.9566\n",
            "Epoch: 009, Step: 1168, Loss: 0.8313\n",
            "Epoch: 009, Step: 1169, Loss: 1.0489\n",
            "Epoch: 009, Step: 1170, Loss: 1.0545\n",
            "Epoch: 009, Step: 1171, Loss: 1.0419\n",
            "Epoch: 009, Step: 1172, Loss: 0.9646\n",
            "Epoch: 009, Step: 1173, Loss: 0.8769\n",
            "Epoch: 009, Step: 1174, Loss: 0.9308\n",
            "Epoch: 009, Step: 1175, Loss: 0.9386\n",
            "Epoch: 009, Step: 1176, Loss: 0.9252\n",
            "Epoch: 009, Step: 1177, Loss: 0.8916\n",
            "Epoch: 009, Step: 1178, Loss: 0.8035\n",
            "Epoch: 009, Step: 1179, Loss: 0.8168\n",
            "Epoch: 009, Step: 1180, Loss: 1.0026\n",
            "Epoch: 009, Step: 1181, Loss: 0.9786\n",
            "Epoch: 009, Step: 1182, Loss: 0.9362\n",
            "Epoch: 009, Step: 1183, Loss: 1.0270\n",
            "Epoch: 009, Step: 1184, Loss: 0.9456\n",
            "Epoch: 009, Step: 1185, Loss: 0.8530\n",
            "Epoch: 009, Step: 1186, Loss: 1.0040\n",
            "Epoch: 009, Step: 1187, Loss: 0.9493\n",
            "Epoch: 009, Step: 1188, Loss: 0.7530\n",
            "Epoch: 009, Step: 1189, Loss: 1.1146\n",
            "Epoch: 009, Step: 1190, Loss: 0.9411\n",
            "Epoch: 009, Step: 1191, Loss: 0.8637\n",
            "Epoch: 009, Step: 1192, Loss: 0.9239\n",
            "Epoch: 009, Step: 1193, Loss: 0.8507\n",
            "Epoch: 009, Step: 1194, Loss: 1.1176\n",
            "Epoch: 009, Step: 1195, Loss: 1.0358\n",
            "Epoch: 009, Step: 1196, Loss: 0.9967\n",
            "Epoch: 009, Step: 1197, Loss: 0.9538\n",
            "Epoch: 009, Step: 1198, Loss: 1.1060\n",
            "Epoch: 009, Step: 1199, Loss: 0.8667\n",
            "Epoch: 009, Step: 1200, Loss: 1.0928\n",
            "Epoch: 009, Step: 1201, Loss: 1.0422\n",
            "Epoch: 009, Step: 1202, Loss: 1.0260\n",
            "Epoch: 009, Step: 1203, Loss: 1.0690\n",
            "Epoch: 009, Step: 1204, Loss: 1.0951\n",
            "Epoch: 009, Step: 1205, Loss: 0.9416\n",
            "Epoch: 009, Step: 1206, Loss: 0.9769\n",
            "Epoch: 009, Step: 1207, Loss: 0.8589\n",
            "Epoch: 009, Step: 1208, Loss: 0.9777\n",
            "Epoch: 009, Step: 1209, Loss: 0.8591\n",
            "Epoch: 009, Step: 1210, Loss: 0.9361\n",
            "Epoch: 009, Step: 1211, Loss: 0.8952\n",
            "Epoch: 009, Step: 1212, Loss: 0.9428\n",
            "Epoch: 009, Step: 1213, Loss: 1.1592\n",
            "Epoch: 009, Step: 1214, Loss: 0.9404\n",
            "Epoch: 009, Step: 1215, Loss: 1.0526\n",
            "Epoch: 009, Step: 1216, Loss: 0.9466\n",
            "Epoch: 009, Step: 1217, Loss: 0.9425\n",
            "Epoch: 009, Step: 1218, Loss: 1.0161\n",
            "Epoch: 009, Step: 1219, Loss: 1.0622\n",
            "Epoch: 009, Step: 1220, Loss: 1.0835\n",
            "Epoch: 009, Step: 1221, Loss: 1.0367\n",
            "Epoch: 009, Step: 1222, Loss: 0.9537\n",
            "Epoch: 009, Step: 1223, Loss: 0.9540\n",
            "Epoch: 009, Step: 1224, Loss: 0.8693\n",
            "Epoch: 009, Step: 1225, Loss: 1.1715\n",
            "Epoch: 009, Step: 1226, Loss: 0.9153\n",
            "Epoch: 009, Step: 1227, Loss: 1.0310\n",
            "Epoch: 009, Step: 1228, Loss: 0.9345\n",
            "Epoch: 009, Step: 1229, Loss: 1.0140\n",
            "Epoch: 009, Step: 1230, Loss: 1.0022\n",
            "Epoch: 009, Step: 1231, Loss: 0.9060\n",
            "Epoch: 009, Step: 1232, Loss: 1.0867\n",
            "Epoch: 009, Step: 1233, Loss: 0.9934\n",
            "Epoch: 009, Step: 1234, Loss: 0.9880\n",
            "Epoch: 009, Step: 1235, Loss: 0.8508\n",
            "Epoch: 009, Step: 1236, Loss: 1.0699\n",
            "Epoch: 009, Step: 1237, Loss: 0.8345\n",
            "Epoch: 009, Step: 1238, Loss: 0.8520\n",
            "Epoch: 009, Step: 1239, Loss: 1.1218\n",
            "Epoch: 009, Step: 1240, Loss: 0.9357\n",
            "Epoch: 009, Step: 1241, Loss: 0.9183\n",
            "Epoch: 009, Step: 1242, Loss: 0.8739\n",
            "Epoch: 009, Step: 1243, Loss: 0.8541\n",
            "Epoch: 009, Step: 1244, Loss: 0.8965\n",
            "Epoch: 009, Step: 1245, Loss: 0.9102\n",
            "Epoch: 009, Step: 1246, Loss: 0.9803\n",
            "Epoch: 009, Step: 1247, Loss: 0.8892\n",
            "Epoch: 009, Step: 1248, Loss: 0.8115\n",
            "Epoch: 009, Step: 1249, Loss: 0.9972\n",
            "Epoch: 009, Step: 1250, Loss: 0.9727\n",
            "Epoch: 009, Step: 1251, Loss: 0.8554\n",
            "Epoch: 009, Step: 1252, Loss: 0.7740\n",
            "Epoch: 009, Step: 1253, Loss: 0.9509\n",
            "Epoch: 009, Step: 1254, Loss: 0.9267\n",
            "Epoch: 009, Step: 1255, Loss: 0.8351\n",
            "Epoch: 009, Step: 1256, Loss: 0.9439\n",
            "Epoch: 009, Step: 1257, Loss: 0.9168\n",
            "Epoch: 009, Step: 1258, Loss: 1.0013\n",
            "Epoch: 009, Step: 1259, Loss: 0.9044\n",
            "Epoch: 009, Step: 1260, Loss: 1.0032\n",
            "Epoch: 009, Step: 1261, Loss: 1.1131\n",
            "Epoch: 009, Step: 1262, Loss: 0.9682\n",
            "Epoch: 009, Step: 1263, Loss: 0.8034\n",
            "Epoch: 009, Step: 1264, Loss: 0.9540\n",
            "Epoch: 009, Step: 1265, Loss: 1.0891\n",
            "Epoch: 009, Step: 1266, Loss: 0.9333\n",
            "Epoch: 009, Step: 1267, Loss: 0.8962\n",
            "Epoch: 009, Step: 1268, Loss: 1.1154\n",
            "Epoch: 009, Step: 1269, Loss: 1.0260\n",
            "Epoch: 009, Step: 1270, Loss: 0.9118\n",
            "Epoch: 009, Step: 1271, Loss: 1.0027\n",
            "Epoch: 009, Step: 1272, Loss: 0.9726\n",
            "Epoch: 009, Step: 1273, Loss: 0.9713\n",
            "Epoch: 009, Step: 1274, Loss: 0.9408\n",
            "Epoch: 009, Step: 1275, Loss: 0.9227\n",
            "Epoch: 009, Step: 1276, Loss: 0.9979\n",
            "Epoch: 009, Step: 1277, Loss: 0.7465\n",
            "Epoch: 009, Step: 1278, Loss: 1.0579\n",
            "Epoch: 009, Step: 1279, Loss: 0.9486\n",
            "Epoch: 009, Step: 1280, Loss: 0.8204\n",
            "Epoch: 009, Step: 1281, Loss: 0.8880\n",
            "Epoch: 009, Step: 1282, Loss: 1.0223\n",
            "Epoch: 009, Step: 1283, Loss: 1.0294\n",
            "Epoch: 009, Step: 1284, Loss: 0.8743\n",
            "Epoch: 009, Step: 1285, Loss: 0.9502\n",
            "Epoch: 009, Step: 1286, Loss: 0.9452\n",
            "Epoch: 009, Step: 1287, Loss: 0.9768\n",
            "Epoch: 009, Step: 1288, Loss: 1.0010\n",
            "Epoch: 009, Step: 1289, Loss: 0.9350\n",
            "Epoch: 009, Step: 1290, Loss: 1.0834\n",
            "Epoch: 009, Step: 1291, Loss: 1.0841\n",
            "Epoch: 009, Step: 1292, Loss: 1.0052\n",
            "Epoch: 009, Step: 1293, Loss: 0.9336\n",
            "Epoch: 009, Step: 1294, Loss: 0.9529\n",
            "Epoch: 009, Step: 1295, Loss: 1.0252\n",
            "Epoch: 009, Step: 1296, Loss: 0.9536\n",
            "Epoch: 009, Step: 1297, Loss: 0.8099\n",
            "Epoch: 009, Step: 1298, Loss: 0.9955\n",
            "Epoch: 009, Step: 1299, Loss: 1.0388\n",
            "Epoch: 009, Step: 1300, Loss: 0.8736\n",
            "Epoch: 009, Step: 1301, Loss: 0.8287\n",
            "Epoch: 009, Step: 1302, Loss: 1.0158\n",
            "Epoch: 009, Step: 1303, Loss: 1.0050\n",
            "Epoch: 009, Step: 1304, Loss: 0.9904\n",
            "Epoch: 009, Step: 1305, Loss: 0.8355\n",
            "Epoch: 009, Step: 1306, Loss: 1.0256\n",
            "Epoch: 009, Step: 1307, Loss: 1.0144\n",
            "Epoch: 009, Step: 1308, Loss: 0.8299\n",
            "Epoch: 009, Step: 1309, Loss: 0.9445\n",
            "Epoch: 009, Step: 1310, Loss: 0.8889\n",
            "Epoch: 009, Step: 1311, Loss: 0.8847\n",
            "Epoch: 009, Step: 1312, Loss: 0.8459\n",
            "Epoch: 009, Step: 1313, Loss: 0.8829\n",
            "Epoch: 009, Step: 1314, Loss: 1.1170\n",
            "Epoch: 009, Step: 1315, Loss: 0.9391\n",
            "Epoch: 009, Step: 1316, Loss: 0.9637\n",
            "Epoch: 009, Step: 1317, Loss: 0.9163\n",
            "Epoch: 009, Step: 1318, Loss: 0.9459\n",
            "Epoch: 009, Step: 1319, Loss: 0.9640\n",
            "Epoch: 009, Step: 1320, Loss: 0.9632\n",
            "Epoch: 009, Step: 1321, Loss: 0.9919\n",
            "Epoch: 009, Step: 1322, Loss: 0.8751\n",
            "Epoch: 009, Step: 1323, Loss: 0.8078\n",
            "Epoch: 009, Step: 1324, Loss: 0.8278\n",
            "Epoch: 009, Step: 1325, Loss: 0.9918\n",
            "Epoch: 009, Step: 1326, Loss: 0.9272\n",
            "Epoch: 009, Step: 1327, Loss: 0.9792\n",
            "Epoch: 009, Step: 1328, Loss: 1.0313\n",
            "Epoch: 009, Step: 1329, Loss: 0.8040\n",
            "Epoch: 009, Step: 1330, Loss: 0.8999\n",
            "Epoch: 009, Step: 1331, Loss: 0.9884\n",
            "Epoch: 009, Step: 1332, Loss: 0.8757\n",
            "Epoch: 009, Step: 1333, Loss: 1.0420\n",
            "Epoch: 009, Step: 1334, Loss: 1.0383\n",
            "Epoch: 009, Step: 1335, Loss: 0.9125\n",
            "Epoch: 009, Step: 1336, Loss: 0.9489\n",
            "Epoch: 009, Step: 1337, Loss: 0.9960\n",
            "Epoch: 009, Step: 1338, Loss: 0.9880\n",
            "Epoch: 009, Step: 1339, Loss: 0.7965\n",
            "Epoch: 009, Step: 1340, Loss: 0.8873\n",
            "Epoch: 009, Step: 1341, Loss: 0.9641\n",
            "Epoch: 009, Step: 1342, Loss: 1.1385\n",
            "Epoch: 009, Step: 1343, Loss: 0.8287\n",
            "Epoch: 009, Step: 1344, Loss: 1.1518\n",
            "Epoch: 009, Step: 1345, Loss: 0.9111\n",
            "Epoch: 009, Step: 1346, Loss: 0.8570\n",
            "Epoch: 009, Step: 1347, Loss: 1.0131\n",
            "Epoch: 009, Step: 1348, Loss: 0.8423\n",
            "Epoch: 009, Step: 1349, Loss: 0.8908\n",
            "Epoch: 009, Step: 1350, Loss: 0.7604\n",
            "Epoch: 009, Step: 1351, Loss: 0.9365\n",
            "Epoch: 009, Step: 1352, Loss: 0.9641\n",
            "Epoch: 009, Step: 1353, Loss: 1.0657\n",
            "Epoch: 009, Step: 1354, Loss: 0.9317\n",
            "Epoch: 009, Step: 1355, Loss: 0.9561\n",
            "Epoch: 009, Step: 1356, Loss: 0.8219\n",
            "Epoch: 009, Step: 1357, Loss: 0.9866\n",
            "Epoch: 009, Step: 1358, Loss: 0.9248\n",
            "Epoch: 009, Step: 1359, Loss: 0.8490\n",
            "Epoch: 009, Step: 1360, Loss: 1.1403\n",
            "Epoch: 009, Step: 1361, Loss: 1.0375\n",
            "Epoch: 009, Step: 1362, Loss: 1.0001\n",
            "Epoch: 009, Step: 1363, Loss: 0.9920\n",
            "Epoch: 009, Step: 1364, Loss: 0.9088\n",
            "Epoch: 009, Step: 1365, Loss: 0.9592\n",
            "Epoch: 009, Step: 1366, Loss: 0.9403\n",
            "Epoch: 009, Step: 1367, Loss: 0.9770\n",
            "Epoch: 009, Step: 1368, Loss: 1.1010\n",
            "Epoch: 009, Step: 1369, Loss: 0.8854\n",
            "Epoch: 009, Step: 1370, Loss: 0.9749\n",
            "Epoch: 009, Step: 1371, Loss: 0.8634\n",
            "Epoch: 009, Step: 1372, Loss: 1.0346\n",
            "Epoch: 009, Step: 1373, Loss: 0.9378\n",
            "Epoch: 009, Step: 1374, Loss: 1.1085\n",
            "Epoch: 009, Step: 1375, Loss: 0.8951\n",
            "Epoch: 009, Step: 1376, Loss: 0.9909\n",
            "Epoch: 009, Step: 1377, Loss: 0.9090\n",
            "Epoch: 009, Step: 1378, Loss: 0.9984\n",
            "Epoch: 009, Step: 1379, Loss: 0.9699\n",
            "Epoch: 009, Step: 1380, Loss: 0.9821\n",
            "Epoch: 009, Step: 1381, Loss: 0.7770\n",
            "Epoch: 009, Step: 1382, Loss: 0.8891\n",
            "Epoch: 009, Step: 1383, Loss: 0.9952\n",
            "Epoch: 009, Step: 1384, Loss: 1.0308\n",
            "Epoch: 009, Step: 1385, Loss: 0.9565\n",
            "Epoch: 009, Step: 1386, Loss: 0.8500\n",
            "Epoch: 009, Step: 1387, Loss: 1.1044\n",
            "Epoch: 009, Step: 1388, Loss: 1.0263\n",
            "Epoch: 009, Step: 1389, Loss: 0.9291\n",
            "Epoch: 009, Step: 1390, Loss: 0.8985\n",
            "Epoch: 009, Step: 1391, Loss: 0.9327\n",
            "Epoch: 009, Step: 1392, Loss: 0.7149\n",
            "Epoch: 009, Step: 1393, Loss: 0.8279\n",
            "Epoch: 009, Step: 1394, Loss: 1.1556\n",
            "Epoch: 009, Step: 1395, Loss: 0.8851\n",
            "Epoch: 009, Step: 1396, Loss: 0.9316\n",
            "Epoch: 009, Step: 1397, Loss: 1.0302\n",
            "Epoch: 009, Step: 1398, Loss: 0.9923\n",
            "Epoch: 009, Step: 1399, Loss: 0.8363\n",
            "Epoch: 009, Step: 1400, Loss: 0.8622\n",
            "Epoch: 009, Step: 1401, Loss: 0.9477\n",
            "Epoch: 009, Step: 1402, Loss: 0.9633\n",
            "Epoch: 009, Step: 1403, Loss: 1.1040\n",
            "Epoch: 009, Step: 1404, Loss: 1.0126\n",
            "Epoch: 009, Step: 1405, Loss: 1.1034\n",
            "Epoch: 009, Step: 1406, Loss: 1.0550\n",
            "Epoch: 009, Step: 1407, Loss: 0.9909\n",
            "Epoch: 009, Step: 1408, Loss: 0.8768\n",
            "Epoch: 009, Step: 1409, Loss: 1.0014\n",
            "Epoch: 009, Step: 1410, Loss: 1.0072\n",
            "Epoch: 009, Step: 1411, Loss: 0.9841\n",
            "Epoch: 009, Step: 1412, Loss: 0.9847\n",
            "Epoch: 009, Step: 1413, Loss: 0.8618\n",
            "Epoch: 009, Step: 1414, Loss: 0.9652\n",
            "Epoch: 009, Step: 1415, Loss: 0.6933\n",
            "Epoch: 009, Step: 1416, Loss: 1.0504\n",
            "Epoch: 009, Step: 1417, Loss: 0.9420\n",
            "Epoch: 009, Step: 1418, Loss: 0.8719\n",
            "Epoch: 009, Step: 1419, Loss: 0.9001\n",
            "Epoch: 009, Step: 1420, Loss: 0.8541\n",
            "Epoch: 009, Step: 1421, Loss: 0.7443\n",
            "Epoch: 009, Step: 1422, Loss: 0.8785\n",
            "Epoch: 009, Step: 1423, Loss: 1.0523\n",
            "Epoch: 009, Step: 1424, Loss: 1.0619\n",
            "Epoch: 009, Step: 1425, Loss: 1.1261\n",
            "Epoch: 009, Step: 1426, Loss: 1.0060\n",
            "Epoch: 009, Step: 1427, Loss: 0.8967\n",
            "Epoch: 009, Step: 1428, Loss: 1.0601\n",
            "Epoch: 009, Step: 1429, Loss: 0.9264\n",
            "Epoch: 009, Step: 1430, Loss: 0.9849\n",
            "Epoch: 009, Step: 1431, Loss: 0.9544\n",
            "Epoch: 009, Step: 1432, Loss: 0.9215\n",
            "Epoch: 009, Step: 1433, Loss: 1.0896\n",
            "Epoch: 009, Step: 1434, Loss: 1.1653\n",
            "Epoch: 009, Step: 1435, Loss: 1.0633\n",
            "Epoch: 009, Step: 1436, Loss: 0.9252\n",
            "Epoch: 009, Step: 1437, Loss: 1.0904\n",
            "Epoch: 009, Step: 1438, Loss: 0.9852\n",
            "Epoch: 009, Step: 1439, Loss: 0.9447\n",
            "Epoch: 009, Step: 1440, Loss: 0.8916\n",
            "Epoch: 009, Step: 1441, Loss: 1.0826\n",
            "Epoch: 009, Step: 1442, Loss: 1.0135\n",
            "Epoch: 009, Step: 1443, Loss: 0.9888\n",
            "Epoch: 009, Step: 1444, Loss: 0.9429\n",
            "Epoch: 009, Step: 1445, Loss: 1.0420\n",
            "Epoch: 009, Step: 1446, Loss: 0.8775\n",
            "Epoch: 009, Step: 1447, Loss: 0.9682\n",
            "Epoch: 009, Step: 1448, Loss: 0.8907\n",
            "Epoch: 009, Step: 1449, Loss: 0.9922\n",
            "Epoch: 009, Step: 1450, Loss: 0.8750\n",
            "Epoch: 009, Step: 1451, Loss: 0.8693\n",
            "Epoch: 009, Step: 1452, Loss: 0.9037\n",
            "Epoch: 009, Step: 1453, Loss: 0.9921\n",
            "Epoch: 009, Step: 1454, Loss: 1.0708\n",
            "Epoch: 009, Step: 1455, Loss: 1.0165\n",
            "Epoch: 009, Step: 1456, Loss: 1.0328\n",
            "Epoch: 009, Step: 1457, Loss: 0.8797\n",
            "Epoch: 009, Step: 1458, Loss: 0.8909\n",
            "Epoch: 009, Step: 1459, Loss: 1.1040\n",
            "Epoch: 009, Step: 1460, Loss: 0.9333\n",
            "Epoch: 009, Step: 1461, Loss: 1.0120\n",
            "Epoch: 009, Step: 1462, Loss: 0.9596\n",
            "Epoch: 009, Step: 1463, Loss: 0.8887\n",
            "Epoch: 009, Step: 1464, Loss: 0.8257\n",
            "Epoch: 009, Step: 1465, Loss: 0.9509\n",
            "Epoch: 009, Step: 1466, Loss: 1.1155\n",
            "Epoch: 009, Step: 1467, Loss: 0.8815\n",
            "Epoch: 009, Step: 1468, Loss: 0.9155\n",
            "Epoch: 009, Step: 1469, Loss: 1.1260\n",
            "Epoch: 009, Step: 1470, Loss: 0.9206\n",
            "Epoch: 009, Step: 1471, Loss: 0.8346\n",
            "Epoch: 009, Step: 1472, Loss: 1.0268\n",
            "Epoch: 009, Step: 1473, Loss: 1.1105\n",
            "Epoch: 009, Step: 1474, Loss: 0.9578\n",
            "Epoch: 009, Step: 1475, Loss: 0.9091\n",
            "Epoch: 009, Step: 1476, Loss: 0.9939\n",
            "Epoch: 009, Step: 1477, Loss: 1.1223\n",
            "Epoch: 009, Step: 1478, Loss: 0.9938\n",
            "Epoch: 009, Step: 1479, Loss: 0.8750\n",
            "Epoch: 009, Step: 1480, Loss: 0.7908\n",
            "Epoch: 009, Step: 1481, Loss: 0.8955\n",
            "Epoch: 009, Step: 1482, Loss: 0.9681\n",
            "Epoch: 009, Step: 1483, Loss: 1.0621\n",
            "Epoch: 009, Step: 1484, Loss: 1.0837\n",
            "Epoch: 009, Step: 1485, Loss: 1.0720\n",
            "Epoch: 009, Step: 1486, Loss: 0.9835\n",
            "Epoch: 009, Step: 1487, Loss: 0.9981\n",
            "Epoch: 009, Step: 1488, Loss: 0.9795\n",
            "Epoch: 009, Step: 1489, Loss: 1.0361\n",
            "Epoch: 009, Step: 1490, Loss: 0.9775\n",
            "Epoch: 009, Step: 1491, Loss: 0.9391\n",
            "Epoch: 009, Step: 1492, Loss: 1.1311\n",
            "Epoch: 009, Step: 1493, Loss: 0.9220\n",
            "Epoch: 009, Step: 1494, Loss: 0.9970\n",
            "Epoch: 009, Step: 1495, Loss: 0.8123\n",
            "Epoch: 009, Step: 1496, Loss: 0.8103\n",
            "Epoch: 009, Step: 1497, Loss: 1.0145\n",
            "Epoch: 009, Step: 1498, Loss: 0.9000\n",
            "Epoch: 009, Step: 1499, Loss: 1.0031\n",
            "Epoch: 009, Step: 1500, Loss: 0.9458\n",
            "Epoch: 009, Step: 1501, Loss: 1.0203\n",
            "Epoch: 009, Step: 1502, Loss: 0.9351\n",
            "Epoch: 009, Step: 1503, Loss: 0.9190\n",
            "Epoch: 009, Step: 1504, Loss: 0.9021\n",
            "Epoch: 009, Step: 1505, Loss: 0.9588\n",
            "Epoch: 009, Step: 1506, Loss: 0.9576\n",
            "Epoch: 009, Step: 1507, Loss: 1.1891\n",
            "Epoch: 009, Step: 1508, Loss: 0.9871\n",
            "Epoch: 009, Step: 1509, Loss: 0.9393\n",
            "Epoch: 009, Step: 1510, Loss: 0.9417\n",
            "Epoch: 009, Step: 1511, Loss: 0.9481\n",
            "Epoch: 009, Step: 1512, Loss: 0.8849\n",
            "Epoch: 009, Step: 1513, Loss: 0.9160\n",
            "Epoch: 009, Step: 1514, Loss: 0.9032\n",
            "Epoch: 009, Step: 1515, Loss: 1.0409\n",
            "Epoch: 009, Step: 1516, Loss: 0.7645\n",
            "Epoch: 009, Step: 1517, Loss: 1.0014\n",
            "Epoch: 009, Step: 1518, Loss: 0.8938\n",
            "Epoch: 009, Step: 1519, Loss: 1.1227\n",
            "Epoch: 009, Step: 1520, Loss: 0.9419\n",
            "Epoch: 009, Step: 1521, Loss: 0.8035\n",
            "Epoch: 009, Step: 1522, Loss: 0.9047\n",
            "Epoch: 009, Step: 1523, Loss: 0.9923\n",
            "Epoch: 009, Step: 1524, Loss: 0.8528\n",
            "Epoch: 009, Step: 1525, Loss: 1.0854\n",
            "Epoch: 009, Step: 1526, Loss: 1.0165\n",
            "Epoch: 009, Step: 1527, Loss: 0.9448\n",
            "Epoch: 009, Step: 1528, Loss: 0.9168\n",
            "Epoch: 009, Step: 1529, Loss: 0.9705\n",
            "Epoch: 009, Step: 1530, Loss: 0.8235\n",
            "Epoch: 009, Step: 1531, Loss: 0.9963\n",
            "Epoch: 009, Step: 1532, Loss: 0.8890\n",
            "Epoch: 009, Step: 1533, Loss: 0.9331\n",
            "Epoch: 009, Step: 1534, Loss: 1.0214\n",
            "Epoch: 009, Step: 1535, Loss: 0.9361\n",
            "Epoch: 009, Step: 1536, Loss: 1.0681\n",
            "Epoch: 009, Step: 1537, Loss: 0.9814\n",
            "Epoch: 009, Step: 1538, Loss: 0.8854\n",
            "Epoch: 009, Step: 1539, Loss: 0.9292\n",
            "Epoch: 009, Step: 1540, Loss: 0.7730\n",
            "Epoch: 009, Step: 1541, Loss: 1.0160\n",
            "Epoch: 009, Step: 1542, Loss: 0.9164\n",
            "Epoch: 009, Step: 1543, Loss: 1.0427\n",
            "Epoch: 009, Step: 1544, Loss: 0.9517\n",
            "Epoch: 009, Step: 1545, Loss: 1.0727\n",
            "Epoch: 009, Step: 1546, Loss: 0.9848\n",
            "Epoch: 009, Step: 1547, Loss: 1.0143\n",
            "Epoch: 009, Step: 1548, Loss: 0.8775\n",
            "Epoch: 009, Step: 1549, Loss: 1.0161\n",
            "Epoch: 009, Step: 1550, Loss: 0.8819\n",
            "Epoch: 009, Step: 1551, Loss: 0.8989\n",
            "Epoch: 009, Step: 1552, Loss: 0.8163\n",
            "Epoch: 009, Step: 1553, Loss: 1.1038\n",
            "Epoch: 009, Step: 1554, Loss: 0.8697\n",
            "Epoch: 009, Step: 1555, Loss: 0.8285\n",
            "Epoch: 009, Step: 1556, Loss: 1.0142\n",
            "Epoch: 009, Step: 1557, Loss: 0.9290\n",
            "Epoch: 009, Step: 1558, Loss: 0.7397\n",
            "Epoch: 009, Step: 1559, Loss: 1.0536\n",
            "Epoch: 009, Step: 1560, Loss: 1.0670\n",
            "Epoch: 009, Step: 1561, Loss: 0.7821\n",
            "Epoch: 009, Step: 1562, Loss: 0.9989\n",
            "Epoch: 009, Step: 1563, Loss: 0.9961\n",
            "Epoch: 009, Step: 1564, Loss: 1.0759\n",
            "Epoch: 009, Step: 1565, Loss: 1.0029\n",
            "Epoch: 009, Step: 1566, Loss: 0.8677\n",
            "Epoch: 009, Step: 1567, Loss: 1.0002\n",
            "Epoch: 009, Step: 1568, Loss: 1.0258\n",
            "Epoch: 009, Step: 1569, Loss: 1.0175\n",
            "Epoch: 009, Step: 1570, Loss: 1.0392\n",
            "Epoch: 009, Step: 1571, Loss: 0.9612\n",
            "Epoch: 009, Step: 1572, Loss: 0.9619\n",
            "Epoch: 009, Step: 1573, Loss: 0.8890\n",
            "Epoch: 009, Step: 1574, Loss: 0.9000\n",
            "Epoch: 009, Step: 1575, Loss: 0.9667\n",
            "Epoch: 009, Step: 1576, Loss: 0.9739\n",
            "Epoch: 009, Step: 1577, Loss: 1.0350\n",
            "Epoch: 009, Step: 1578, Loss: 1.0113\n",
            "Epoch: 009, Step: 1579, Loss: 0.8600\n",
            "Epoch: 009, Step: 1580, Loss: 0.9727\n",
            "Epoch: 009, Step: 1581, Loss: 0.8841\n",
            "Epoch: 009, Step: 1582, Loss: 0.7362\n",
            "Epoch: 009, Step: 1583, Loss: 0.8552\n",
            "Epoch: 009, Step: 1584, Loss: 0.8088\n",
            "Epoch: 009, Step: 1585, Loss: 0.9961\n",
            "Epoch: 009, Step: 1586, Loss: 1.0417\n",
            "Epoch: 009, Step: 1587, Loss: 0.9478\n",
            "Epoch: 009, Step: 1588, Loss: 0.9817\n",
            "Epoch: 009, Step: 1589, Loss: 1.0130\n",
            "Epoch: 009, Step: 1590, Loss: 0.9431\n",
            "Epoch: 009, Step: 1591, Loss: 0.9021\n",
            "Epoch: 009, Step: 1592, Loss: 0.9108\n",
            "Epoch: 009, Step: 1593, Loss: 1.0299\n",
            "Epoch: 009, Step: 1594, Loss: 0.8851\n",
            "Epoch: 009, Step: 1595, Loss: 0.9437\n",
            "Epoch: 009, Step: 1596, Loss: 0.9448\n",
            "Epoch: 009, Step: 1597, Loss: 0.8982\n",
            "Epoch: 009, Step: 1598, Loss: 0.8666\n",
            "Epoch: 009, Step: 1599, Loss: 1.0630\n",
            "Epoch: 009, Step: 1600, Loss: 0.8524\n",
            "Epoch: 009, Step: 1601, Loss: 0.8716\n",
            "Epoch: 009, Step: 1602, Loss: 1.0249\n",
            "Epoch: 009, Step: 1603, Loss: 1.0027\n",
            "Epoch: 009, Step: 1604, Loss: 0.9853\n",
            "Epoch: 009, Step: 1605, Loss: 0.8848\n",
            "Epoch: 009, Step: 1606, Loss: 1.0052\n",
            "Epoch: 009, Step: 1607, Loss: 1.1659\n",
            "Epoch: 009, Step: 1608, Loss: 0.9594\n",
            "Epoch: 009, Step: 1609, Loss: 1.0533\n",
            "Epoch: 009, Step: 1610, Loss: 1.0060\n",
            "Epoch: 009, Step: 1611, Loss: 1.1208\n",
            "Epoch: 009, Step: 1612, Loss: 0.8590\n",
            "Epoch: 009, Step: 1613, Loss: 0.8629\n",
            "Epoch: 009, Step: 1614, Loss: 0.7445\n",
            "Epoch: 009, Step: 1615, Loss: 0.9814\n",
            "Epoch: 009, Step: 1616, Loss: 0.8928\n",
            "Epoch: 009, Step: 1617, Loss: 1.0345\n",
            "Epoch: 009, Step: 1618, Loss: 1.0059\n",
            "Epoch: 009, Step: 1619, Loss: 1.0588\n",
            "Epoch: 009, Step: 1620, Loss: 0.9406\n",
            "Epoch: 009, Step: 1621, Loss: 0.9171\n",
            "Epoch: 009, Step: 1622, Loss: 0.9357\n",
            "Epoch: 009, Step: 1623, Loss: 0.8947\n",
            "Epoch: 009, Step: 1624, Loss: 0.9705\n",
            "Epoch: 009, Step: 1625, Loss: 0.9324\n",
            "Epoch: 009, Step: 1626, Loss: 1.0098\n",
            "Epoch: 009, Step: 1627, Loss: 0.9316\n",
            "Epoch: 009, Step: 1628, Loss: 0.9709\n",
            "Epoch: 009, Step: 1629, Loss: 1.0247\n",
            "Epoch: 009, Step: 1630, Loss: 1.0093\n",
            "Epoch: 009, Step: 1631, Loss: 0.9190\n",
            "Epoch: 009, Step: 1632, Loss: 0.9835\n",
            "Epoch: 009, Step: 1633, Loss: 0.9464\n",
            "Epoch: 009, Step: 1634, Loss: 0.8921\n",
            "Epoch: 009, Step: 1635, Loss: 0.9145\n",
            "Epoch: 009, Step: 1636, Loss: 0.8472\n",
            "Epoch: 009, Step: 1637, Loss: 0.9096\n",
            "Epoch: 009, Step: 1638, Loss: 0.8870\n",
            "Epoch: 009, Step: 1639, Loss: 1.2357\n",
            "Epoch: 009, Step: 1640, Loss: 1.0393\n",
            "Epoch: 009, Step: 1641, Loss: 0.8985\n",
            "Epoch: 009, Step: 1642, Loss: 0.9532\n",
            "Epoch: 009, Step: 1643, Loss: 0.9347\n",
            "Epoch: 009, Step: 1644, Loss: 0.9770\n",
            "Epoch: 009, Step: 1645, Loss: 0.9638\n",
            "Epoch: 009, Step: 1646, Loss: 0.9209\n",
            "Epoch: 009, Step: 1647, Loss: 1.0460\n",
            "Epoch: 009, Step: 1648, Loss: 0.8178\n",
            "Epoch: 009, Step: 1649, Loss: 1.0861\n",
            "Epoch: 009, Step: 1650, Loss: 1.0583\n",
            "Epoch: 009, Step: 1651, Loss: 0.9009\n",
            "Epoch: 009, Step: 1652, Loss: 0.9605\n",
            "Epoch: 009, Step: 1653, Loss: 0.7825\n",
            "Epoch: 009, Step: 1654, Loss: 1.0191\n",
            "Epoch: 009, Step: 1655, Loss: 0.9084\n",
            "Epoch: 009, Step: 1656, Loss: 1.0209\n",
            "Epoch: 009, Step: 1657, Loss: 0.9260\n",
            "Epoch: 009, Step: 1658, Loss: 0.9669\n",
            "Epoch: 009, Step: 1659, Loss: 0.9635\n",
            "Epoch: 009, Step: 1660, Loss: 0.8570\n",
            "Epoch: 009, Step: 1661, Loss: 1.1240\n",
            "Epoch: 009, Step: 1662, Loss: 0.9763\n",
            "Epoch: 009, Step: 1663, Loss: 0.8801\n",
            "Epoch: 009, Step: 1664, Loss: 1.0496\n",
            "Epoch: 009, Step: 1665, Loss: 0.9117\n",
            "Epoch: 009, Step: 1666, Loss: 0.9074\n",
            "Epoch: 009, Step: 1667, Loss: 1.0507\n",
            "Epoch: 009, Step: 1668, Loss: 1.1163\n",
            "Epoch: 009, Step: 1669, Loss: 0.9474\n",
            "Epoch: 009, Step: 1670, Loss: 0.9632\n",
            "Epoch: 009, Step: 1671, Loss: 1.1389\n",
            "Epoch: 009, Step: 1672, Loss: 0.9696\n",
            "Epoch: 009, Step: 1673, Loss: 0.8801\n",
            "Epoch: 009, Step: 1674, Loss: 0.8913\n",
            "Epoch: 009, Step: 1675, Loss: 0.9302\n",
            "Epoch: 009, Step: 1676, Loss: 1.0977\n",
            "Epoch: 009, Step: 1677, Loss: 1.0019\n",
            "Epoch: 009, Step: 1678, Loss: 0.8557\n",
            "Epoch: 009, Step: 1679, Loss: 0.9290\n",
            "Epoch: 009, Step: 1680, Loss: 1.0803\n",
            "Epoch: 009, Step: 1681, Loss: 0.8779\n",
            "Epoch: 009, Step: 1682, Loss: 0.9653\n",
            "Epoch: 009, Step: 1683, Loss: 1.0852\n",
            "Epoch: 009, Step: 1684, Loss: 0.9484\n",
            "Epoch: 009, Step: 1685, Loss: 0.8487\n",
            "Epoch: 009, Step: 1686, Loss: 0.9684\n",
            "Epoch: 009, Step: 1687, Loss: 0.9377\n",
            "Epoch: 009, Step: 1688, Loss: 1.0040\n",
            "Epoch: 009, Step: 1689, Loss: 0.7600\n",
            "Epoch: 009, Step: 1690, Loss: 1.0073\n",
            "Epoch: 009, Step: 1691, Loss: 0.9992\n",
            "Epoch: 009, Step: 1692, Loss: 0.9354\n",
            "Epoch: 009, Step: 1693, Loss: 0.9388\n",
            "Epoch: 009, Step: 1694, Loss: 0.9678\n",
            "Epoch: 009, Step: 1695, Loss: 0.9905\n",
            "Epoch: 009, Step: 1696, Loss: 0.8972\n",
            "Epoch: 009, Step: 1697, Loss: 0.9739\n",
            "Epoch: 009, Step: 1698, Loss: 0.9272\n",
            "Epoch: 009, Step: 1699, Loss: 0.9225\n",
            "Epoch: 009, Step: 1700, Loss: 0.8723\n",
            "Epoch: 009, Step: 1701, Loss: 0.9029\n",
            "Epoch: 009, Step: 1702, Loss: 0.9018\n",
            "Epoch: 009, Step: 1703, Loss: 0.7943\n",
            "Epoch: 009, Step: 1704, Loss: 0.9675\n",
            "Epoch: 009, Step: 1705, Loss: 0.9557\n",
            "Epoch: 009, Step: 1706, Loss: 1.0120\n",
            "Epoch: 009, Step: 1707, Loss: 0.7903\n",
            "Epoch: 009, Step: 1708, Loss: 1.1186\n",
            "Epoch: 009, Step: 1709, Loss: 0.8957\n",
            "Epoch: 009, Step: 1710, Loss: 0.9291\n",
            "Epoch: 009, Step: 1711, Loss: 0.9611\n",
            "Epoch: 009, Step: 1712, Loss: 0.9807\n",
            "Epoch: 009, Step: 1713, Loss: 0.9388\n",
            "Epoch: 009, Step: 1714, Loss: 0.8406\n",
            "Epoch: 009, Step: 1715, Loss: 0.8301\n",
            "Epoch: 009, Step: 1716, Loss: 1.1746\n",
            "Epoch: 009, Step: 1717, Loss: 1.0199\n",
            "Epoch: 009, Step: 1718, Loss: 1.0074\n",
            "Epoch: 009, Step: 1719, Loss: 0.8176\n",
            "Epoch: 009, Step: 1720, Loss: 0.8815\n",
            "Epoch: 009, Step: 1721, Loss: 0.9174\n",
            "Epoch: 009, Step: 1722, Loss: 0.8903\n",
            "Epoch: 009, Step: 1723, Loss: 0.9395\n",
            "Epoch: 009, Step: 1724, Loss: 0.9832\n",
            "Epoch: 009, Step: 1725, Loss: 1.0890\n",
            "Epoch: 009, Step: 1726, Loss: 0.9030\n",
            "Epoch: 009, Step: 1727, Loss: 0.9327\n",
            "Epoch: 009, Step: 1728, Loss: 0.8896\n",
            "Epoch: 009, Step: 1729, Loss: 0.9477\n",
            "Epoch: 009, Step: 1730, Loss: 0.8638\n",
            "Epoch: 009, Step: 1731, Loss: 0.8754\n",
            "Epoch: 009, Step: 1732, Loss: 0.9877\n",
            "Epoch: 009, Step: 1733, Loss: 0.8047\n",
            "Epoch: 009, Step: 1734, Loss: 1.0175\n",
            "Epoch: 009, Step: 1735, Loss: 1.0469\n",
            "Epoch: 009, Step: 1736, Loss: 0.9437\n",
            "Epoch: 009, Step: 1737, Loss: 1.0469\n",
            "Epoch: 009, Step: 1738, Loss: 0.9049\n",
            "Epoch: 009, Step: 1739, Loss: 0.9939\n",
            "Epoch: 009, Step: 1740, Loss: 0.8511\n",
            "Epoch: 009, Step: 1741, Loss: 0.9037\n",
            "Epoch: 009, Step: 1742, Loss: 0.8793\n",
            "Epoch: 009, Step: 1743, Loss: 0.8530\n",
            "Epoch: 009, Step: 1744, Loss: 0.9963\n",
            "Epoch: 009, Step: 1745, Loss: 1.0652\n",
            "Epoch: 009, Step: 1746, Loss: 0.9871\n",
            "Epoch: 009, Step: 1747, Loss: 0.9720\n",
            "Epoch: 009, Step: 1748, Loss: 1.1791\n",
            "Epoch: 009, Step: 1749, Loss: 0.8551\n",
            "Epoch: 009, Step: 1750, Loss: 0.9770\n",
            "Epoch: 009, Step: 1751, Loss: 1.0226\n",
            "Epoch: 009, Step: 1752, Loss: 1.1004\n",
            "Epoch: 009, Step: 1753, Loss: 1.0633\n",
            "Epoch: 009, Step: 1754, Loss: 0.8466\n",
            "Epoch: 009, Step: 1755, Loss: 0.9372\n",
            "Epoch: 009, Step: 1756, Loss: 1.0180\n",
            "Epoch: 009, Step: 1757, Loss: 0.9298\n",
            "Epoch: 009, Step: 1758, Loss: 0.8150\n",
            "Epoch: 009, Step: 1759, Loss: 0.8771\n",
            "Epoch: 009, Step: 1760, Loss: 0.9838\n",
            "Epoch: 009, Step: 1761, Loss: 1.0480\n",
            "Epoch: 009, Step: 1762, Loss: 1.0405\n",
            "Epoch: 009, Step: 1763, Loss: 0.9232\n",
            "Epoch: 009, Step: 1764, Loss: 0.8761\n",
            "Epoch: 009, Step: 1765, Loss: 1.2648\n",
            "Epoch: 009, Step: 1766, Loss: 0.8757\n",
            "Epoch: 009, Step: 1767, Loss: 0.8591\n",
            "Epoch: 009, Step: 1768, Loss: 1.0865\n",
            "Epoch: 009, Step: 1769, Loss: 0.9540\n",
            "Epoch: 009, Step: 1770, Loss: 0.8665\n",
            "Epoch: 009, Step: 1771, Loss: 0.7827\n",
            "Epoch: 009, Step: 1772, Loss: 0.9167\n",
            "Epoch: 009, Step: 1773, Loss: 0.9484\n",
            "Epoch: 009, Step: 1774, Loss: 0.9135\n",
            "Epoch: 009, Step: 1775, Loss: 0.8050\n",
            "Epoch: 009, Step: 1776, Loss: 0.9876\n",
            "Epoch: 009, Step: 1777, Loss: 0.8440\n",
            "Epoch: 009, Step: 1778, Loss: 0.8801\n",
            "Epoch: 009, Step: 1779, Loss: 0.8597\n",
            "Epoch: 009, Step: 1780, Loss: 1.0029\n",
            "Epoch: 009, Step: 1781, Loss: 0.9233\n",
            "Epoch: 009, Step: 1782, Loss: 0.8721\n",
            "Epoch: 009, Step: 1783, Loss: 0.9458\n",
            "Epoch: 009, Step: 1784, Loss: 0.9618\n",
            "Epoch: 009, Step: 1785, Loss: 0.8130\n",
            "Epoch: 009, Step: 1786, Loss: 0.9638\n",
            "Epoch: 009, Step: 1787, Loss: 0.9646\n",
            "Epoch: 009, Step: 1788, Loss: 0.8924\n",
            "Epoch: 009, Step: 1789, Loss: 1.0102\n",
            "Epoch: 009, Step: 1790, Loss: 0.9136\n",
            "Epoch: 009, Step: 1791, Loss: 0.9167\n",
            "Epoch: 009, Step: 1792, Loss: 1.1824\n",
            "Epoch: 009, Step: 1793, Loss: 0.9530\n",
            "Epoch: 009, Step: 1794, Loss: 0.9769\n",
            "Epoch: 009, Step: 1795, Loss: 1.0552\n",
            "Epoch: 009, Step: 1796, Loss: 0.9930\n",
            "Epoch: 009, Step: 1797, Loss: 0.9207\n",
            "Epoch: 009, Step: 1798, Loss: 0.9335\n",
            "Epoch: 009, Step: 1799, Loss: 1.1013\n",
            "Epoch: 009, Step: 1800, Loss: 0.9748\n",
            "Epoch: 009, Step: 1801, Loss: 0.8684\n",
            "Epoch: 009, Step: 1802, Loss: 0.9008\n",
            "Epoch: 009, Step: 1803, Loss: 0.9835\n",
            "Epoch: 009, Step: 1804, Loss: 0.9446\n",
            "Epoch: 009, Step: 1805, Loss: 0.8506\n",
            "Epoch: 009, Step: 1806, Loss: 1.0811\n",
            "Epoch: 009, Step: 1807, Loss: 0.8885\n",
            "Epoch: 009, Step: 1808, Loss: 0.8722\n",
            "Epoch: 009, Step: 1809, Loss: 1.0459\n",
            "Epoch: 009, Step: 1810, Loss: 1.1037\n",
            "Epoch: 009, Step: 1811, Loss: 1.1757\n",
            "Epoch: 009, Step: 1812, Loss: 0.8975\n",
            "Epoch: 009, Step: 1813, Loss: 0.9211\n",
            "Epoch: 009, Step: 1814, Loss: 0.9353\n",
            "Epoch: 009, Step: 1815, Loss: 0.8768\n",
            "Epoch: 009, Step: 1816, Loss: 0.9224\n",
            "Epoch: 009, Step: 1817, Loss: 0.7819\n",
            "Epoch: 009, Step: 1818, Loss: 1.0567\n",
            "Epoch: 009, Step: 1819, Loss: 0.9306\n",
            "Epoch: 009, Step: 1820, Loss: 0.8536\n",
            "Epoch: 009, Step: 1821, Loss: 0.9830\n",
            "Epoch: 009, Step: 1822, Loss: 0.8609\n",
            "Epoch: 009, Step: 1823, Loss: 0.9775\n",
            "Epoch: 009, Step: 1824, Loss: 0.9009\n",
            "Epoch: 009, Step: 1825, Loss: 1.0012\n",
            "Epoch: 009, Step: 1826, Loss: 1.0908\n",
            "Epoch: 009, Step: 1827, Loss: 1.0596\n",
            "Epoch: 009, Step: 1828, Loss: 1.2025\n",
            "Epoch: 009, Step: 1829, Loss: 1.0408\n",
            "Epoch: 009, Step: 1830, Loss: 0.8478\n",
            "Epoch: 009, Step: 1831, Loss: 0.9799\n",
            "Epoch: 009, Step: 1832, Loss: 1.0751\n",
            "Epoch: 009, Step: 1833, Loss: 0.8933\n",
            "Epoch: 009, Step: 1834, Loss: 0.9815\n",
            "Epoch: 009, Step: 1835, Loss: 0.9794\n",
            "Epoch: 009, Step: 1836, Loss: 0.9780\n",
            "Epoch: 009, Step: 1837, Loss: 0.9048\n",
            "Epoch: 009, Step: 1838, Loss: 0.9909\n",
            "Epoch: 009, Step: 1839, Loss: 0.9473\n",
            "Epoch: 009, Step: 1840, Loss: 0.9744\n",
            "Epoch: 009, Step: 1841, Loss: 0.7887\n",
            "Epoch: 009, Step: 1842, Loss: 0.8857\n",
            "Epoch: 009, Step: 1843, Loss: 1.0769\n",
            "Epoch: 009, Step: 1844, Loss: 1.0052\n",
            "Epoch: 009, Step: 1845, Loss: 0.9240\n",
            "Epoch: 009, Step: 1846, Loss: 0.9280\n",
            "Epoch: 009, Step: 1847, Loss: 1.0010\n",
            "Epoch: 009, Step: 1848, Loss: 0.9982\n",
            "Epoch: 009, Step: 1849, Loss: 0.9830\n",
            "Epoch: 009, Step: 1850, Loss: 0.9920\n",
            "Epoch: 009, Step: 1851, Loss: 0.8590\n",
            "Epoch: 009, Step: 1852, Loss: 0.9194\n",
            "Epoch: 009, Step: 1853, Loss: 1.0315\n",
            "Epoch: 009, Step: 1854, Loss: 1.0917\n",
            "Epoch: 009, Step: 1855, Loss: 1.1510\n",
            "Epoch: 009, Step: 1856, Loss: 0.9683\n",
            "Epoch: 009, Step: 1857, Loss: 0.8391\n",
            "Epoch: 009, Step: 1858, Loss: 1.0259\n",
            "Epoch: 009, Step: 1859, Loss: 0.9677\n",
            "Epoch: 009, Step: 1860, Loss: 0.9253\n",
            "Epoch: 009, Step: 1861, Loss: 0.9798\n",
            "Epoch: 009, Step: 1862, Loss: 0.9429\n",
            "Epoch: 009, Step: 1863, Loss: 1.0518\n",
            "Epoch: 009, Step: 1864, Loss: 0.8568\n",
            "Epoch: 009, Step: 1865, Loss: 0.8623\n",
            "Epoch: 009, Step: 1866, Loss: 1.0582\n",
            "Epoch: 009, Step: 1867, Loss: 0.9711\n",
            "Epoch: 009, Step: 1868, Loss: 0.8594\n",
            "Epoch: 009, Step: 1869, Loss: 1.0582\n",
            "Epoch: 009, Step: 1870, Loss: 0.9374\n",
            "Epoch: 009, Step: 1871, Loss: 0.9835\n",
            "Epoch: 009, Step: 1872, Loss: 1.0072\n",
            "Epoch: 009, Step: 1873, Loss: 0.7782\n",
            "Epoch: 009, Step: 1874, Loss: 0.9230\n",
            "Epoch: 009, Step: 1875, Loss: 0.8487\n",
            "Epoch: 009, Step: 1876, Loss: 1.0942\n",
            "Epoch: 009, Step: 1877, Loss: 0.8307\n",
            "Epoch: 009, Step: 1878, Loss: 0.9146\n",
            "Epoch: 009, Step: 1879, Loss: 0.8188\n",
            "Epoch: 009, Step: 1880, Loss: 0.8663\n",
            "Epoch: 009, Step: 1881, Loss: 1.1692\n",
            "Epoch: 009, Step: 1882, Loss: 0.8353\n",
            "Epoch: 009, Step: 1883, Loss: 1.0242\n",
            "Epoch: 009, Step: 1884, Loss: 1.0051\n",
            "Epoch: 009, Step: 1885, Loss: 0.9406\n",
            "Epoch: 009, Step: 1886, Loss: 1.0983\n",
            "Epoch: 009, Step: 1887, Loss: 0.8825\n",
            "Epoch: 009, Step: 1888, Loss: 0.9457\n",
            "Epoch: 009, Step: 1889, Loss: 0.9919\n",
            "Epoch: 009, Step: 1890, Loss: 1.0178\n",
            "Epoch: 009, Step: 1891, Loss: 0.9372\n",
            "Epoch: 009, Step: 1892, Loss: 1.1240\n",
            "Epoch: 009, Step: 1893, Loss: 1.0355\n",
            "Epoch: 009, Step: 1894, Loss: 1.0403\n",
            "Epoch: 009, Step: 1895, Loss: 0.9796\n",
            "Epoch: 009, Step: 1896, Loss: 1.0080\n",
            "Epoch: 009, Step: 1897, Loss: 0.9084\n",
            "Epoch: 009, Step: 1898, Loss: 1.0031\n",
            "Epoch: 009, Step: 1899, Loss: 0.9920\n",
            "Epoch: 009, Step: 1900, Loss: 0.9263\n",
            "Epoch: 009, Step: 1901, Loss: 0.9218\n",
            "Epoch: 009, Step: 1902, Loss: 0.9461\n",
            "Epoch: 009, Step: 1903, Loss: 0.8448\n",
            "Epoch: 009, Step: 1904, Loss: 1.0148\n",
            "Epoch: 009, Step: 1905, Loss: 1.0398\n",
            "Epoch: 009, Step: 1906, Loss: 0.9029\n",
            "Epoch: 009, Step: 1907, Loss: 0.9427\n",
            "Epoch: 009, Step: 1908, Loss: 1.0124\n",
            "Epoch: 009, Step: 1909, Loss: 0.9371\n",
            "Epoch: 009, Step: 1910, Loss: 0.8384\n",
            "Epoch: 009, Step: 1911, Loss: 0.9989\n",
            "Epoch: 009, Step: 1912, Loss: 1.0056\n",
            "Epoch: 009, Step: 1913, Loss: 0.8545\n",
            "Epoch: 009, Step: 1914, Loss: 0.9063\n",
            "Epoch: 009, Step: 1915, Loss: 0.9492\n",
            "Epoch: 009, Step: 1916, Loss: 0.8941\n",
            "Epoch: 009, Step: 1917, Loss: 0.9632\n",
            "Epoch: 009, Step: 1918, Loss: 0.9248\n",
            "Epoch: 009, Step: 1919, Loss: 0.9730\n",
            "Epoch: 009, Step: 1920, Loss: 1.1255\n",
            "Epoch: 009, Step: 1921, Loss: 0.8501\n",
            "Epoch: 009, Step: 1922, Loss: 0.9137\n",
            "Epoch: 009, Step: 1923, Loss: 1.0830\n",
            "Epoch: 009, Step: 1924, Loss: 0.8800\n",
            "Epoch: 009, Step: 1925, Loss: 0.9928\n",
            "Epoch: 009, Step: 1926, Loss: 0.8394\n",
            "Epoch: 009, Step: 1927, Loss: 1.1464\n",
            "Epoch: 009, Step: 1928, Loss: 0.9712\n",
            "Epoch: 009, Step: 1929, Loss: 0.8744\n",
            "Epoch: 009, Step: 1930, Loss: 0.9381\n",
            "Epoch: 009, Step: 1931, Loss: 0.8808\n",
            "Epoch: 009, Step: 1932, Loss: 0.9917\n",
            "Epoch: 009, Step: 1933, Loss: 1.1226\n",
            "Epoch: 009, Step: 1934, Loss: 0.8386\n",
            "Epoch: 009, Step: 1935, Loss: 0.8674\n",
            "Epoch: 009, Step: 1936, Loss: 0.8127\n",
            "Epoch: 009, Step: 1937, Loss: 0.9795\n",
            "Epoch: 009, Step: 1938, Loss: 0.8874\n",
            "Epoch: 009, Step: 1939, Loss: 0.8405\n",
            "Epoch: 009, Step: 1940, Loss: 0.9357\n",
            "Epoch: 009, Step: 1941, Loss: 0.7359\n",
            "Epoch: 009, Step: 1942, Loss: 0.9309\n",
            "Epoch: 009, Step: 1943, Loss: 0.8029\n",
            "Epoch: 009, Step: 1944, Loss: 0.9600\n",
            "Epoch: 009, Step: 1945, Loss: 0.9622\n",
            "Epoch: 009, Step: 1946, Loss: 1.0520\n",
            "Epoch: 009, Step: 1947, Loss: 0.8817\n",
            "Epoch: 009, Step: 1948, Loss: 1.0480\n",
            "Epoch: 009, Step: 1949, Loss: 1.0925\n",
            "Epoch: 009, Step: 1950, Loss: 0.9022\n",
            "Epoch: 009, Step: 1951, Loss: 0.9746\n",
            "Epoch: 009, Step: 1952, Loss: 1.0489\n",
            "Epoch: 009, Step: 1953, Loss: 0.7893\n",
            "Epoch: 009, Step: 1954, Loss: 0.9114\n",
            "Epoch: 009, Step: 1955, Loss: 0.9748\n",
            "Epoch: 009, Step: 1956, Loss: 0.8311\n",
            "Epoch: 009, Step: 1957, Loss: 1.0552\n",
            "Epoch: 009, Step: 1958, Loss: 0.9052\n",
            "Epoch: 009, Step: 1959, Loss: 1.0447\n",
            "Epoch: 009, Step: 1960, Loss: 1.0040\n",
            "Epoch: 009, Step: 1961, Loss: 0.9738\n",
            "Epoch: 009, Step: 1962, Loss: 0.8572\n",
            "Epoch: 009, Step: 1963, Loss: 0.9535\n",
            "Epoch: 009, Step: 1964, Loss: 0.9564\n",
            "Epoch: 009, Step: 1965, Loss: 0.9875\n",
            "Epoch: 009, Step: 1966, Loss: 0.9648\n",
            "Epoch: 009, Step: 1967, Loss: 0.9348\n",
            "Epoch: 009, Step: 1968, Loss: 0.8141\n",
            "Epoch: 009, Step: 1969, Loss: 0.9483\n",
            "Epoch: 009, Step: 1970, Loss: 0.8842\n",
            "Epoch: 009, Step: 1971, Loss: 1.0615\n",
            "Epoch: 009, Step: 1972, Loss: 1.0898\n",
            "Epoch: 009, Step: 1973, Loss: 0.9050\n",
            "Epoch: 009, Step: 1974, Loss: 1.0237\n",
            "Epoch: 009, Step: 1975, Loss: 0.8597\n",
            "Epoch: 009, Step: 1976, Loss: 0.8636\n",
            "Epoch: 009, Step: 1977, Loss: 0.8120\n",
            "Epoch: 009, Step: 1978, Loss: 0.9358\n",
            "Epoch: 009, Step: 1979, Loss: 1.0003\n",
            "Epoch: 009, Step: 1980, Loss: 0.9488\n",
            "Epoch: 009, Step: 1981, Loss: 1.0224\n",
            "Epoch: 009, Step: 1982, Loss: 0.8933\n",
            "Epoch: 009, Step: 1983, Loss: 0.9167\n",
            "Epoch: 009, Step: 1984, Loss: 0.8994\n",
            "Epoch: 009, Step: 1985, Loss: 1.0159\n",
            "Epoch: 009, Step: 1986, Loss: 1.0663\n",
            "Epoch: 009, Step: 1987, Loss: 0.7793\n",
            "Epoch: 009, Step: 1988, Loss: 0.9141\n",
            "Epoch: 009, Step: 1989, Loss: 0.9812\n",
            "Epoch: 009, Step: 1990, Loss: 0.9885\n",
            "Epoch: 009, Step: 1991, Loss: 0.9716\n",
            "Epoch: 009, Step: 1992, Loss: 0.8214\n",
            "Epoch: 009, Step: 1993, Loss: 0.8676\n",
            "Epoch: 009, Step: 1994, Loss: 0.9085\n",
            "Epoch: 009, Step: 1995, Loss: 1.0770\n",
            "Epoch: 009, Step: 1996, Loss: 0.9096\n",
            "Epoch: 009, Step: 1997, Loss: 0.9997\n",
            "Epoch: 009, Step: 1998, Loss: 1.0031\n",
            "Epoch: 009, Step: 1999, Loss: 0.8644\n",
            "Epoch: 009, Step: 2000, Loss: 1.0568\n",
            "Epoch: 009, Step: 2001, Loss: 0.9473\n",
            "Epoch: 009, Step: 2002, Loss: 0.8331\n",
            "Epoch: 009, Step: 2003, Loss: 1.0505\n",
            "Epoch: 009, Step: 2004, Loss: 0.9646\n",
            "Epoch: 009, Step: 2005, Loss: 0.9280\n",
            "Epoch: 009, Step: 2006, Loss: 0.9380\n",
            "Epoch: 009, Step: 2007, Loss: 0.9721\n",
            "Epoch: 009, Step: 2008, Loss: 0.8680\n",
            "Epoch: 009, Step: 2009, Loss: 0.9844\n",
            "Epoch: 009, Step: 2010, Loss: 0.9414\n",
            "Epoch: 009, Step: 2011, Loss: 1.0570\n",
            "Epoch: 009, Step: 2012, Loss: 1.0466\n",
            "Epoch: 009, Step: 2013, Loss: 0.9484\n",
            "Epoch: 009, Step: 2014, Loss: 1.0925\n",
            "Epoch: 009, Step: 2015, Loss: 0.8030\n",
            "Epoch: 009, Step: 2016, Loss: 0.8166\n",
            "Epoch: 009, Step: 2017, Loss: 0.9814\n",
            "Epoch: 009, Step: 2018, Loss: 1.0275\n",
            "Epoch: 009, Step: 2019, Loss: 0.8974\n",
            "Epoch: 009, Step: 2020, Loss: 1.0619\n",
            "Epoch: 009, Step: 2021, Loss: 0.8997\n",
            "Epoch: 009, Step: 2022, Loss: 0.8455\n",
            "Epoch: 009, Step: 2023, Loss: 0.8959\n",
            "Epoch: 009, Step: 2024, Loss: 1.0128\n",
            "Epoch: 009, Step: 2025, Loss: 0.9809\n",
            "Epoch: 009, Step: 2026, Loss: 0.9900\n",
            "Epoch: 009, Step: 2027, Loss: 0.8456\n",
            "Epoch: 009, Step: 2028, Loss: 0.8591\n",
            "Epoch: 009, Step: 2029, Loss: 0.9901\n",
            "Epoch: 009, Step: 2030, Loss: 1.0381\n",
            "Epoch: 009, Step: 2031, Loss: 1.0110\n",
            "Epoch: 009, Step: 2032, Loss: 1.0430\n",
            "Epoch: 009, Step: 2033, Loss: 0.9649\n",
            "Epoch: 009, Step: 2034, Loss: 1.0000\n",
            "Epoch: 009, Step: 2035, Loss: 0.9317\n",
            "Epoch: 009, Step: 2036, Loss: 1.0429\n",
            "Epoch: 009, Step: 2037, Loss: 0.8837\n",
            "Epoch: 009, Step: 2038, Loss: 1.0912\n",
            "Epoch: 009, Step: 2039, Loss: 0.8610\n",
            "Epoch: 009, Step: 2040, Loss: 0.9502\n",
            "Epoch: 009, Step: 2041, Loss: 0.9553\n",
            "Epoch: 009, Step: 2042, Loss: 0.9866\n",
            "Epoch: 009, Step: 2043, Loss: 0.9700\n",
            "Epoch: 009, Step: 2044, Loss: 0.8973\n",
            "Epoch: 009, Step: 2045, Loss: 1.0043\n",
            "Epoch: 009, Step: 2046, Loss: 0.9844\n",
            "Epoch: 009, Step: 2047, Loss: 1.0584\n",
            "Epoch: 009, Step: 2048, Loss: 0.9419\n",
            "Epoch: 009, Step: 2049, Loss: 0.9164\n",
            "Epoch: 009, Step: 2050, Loss: 0.8068\n",
            "Epoch: 009, Step: 2051, Loss: 0.8333\n",
            "Epoch: 009, Step: 2052, Loss: 0.9450\n",
            "Epoch: 009, Step: 2053, Loss: 1.0216\n",
            "Epoch: 009, Step: 2054, Loss: 0.9632\n",
            "Epoch: 009, Step: 2055, Loss: 0.9022\n",
            "Epoch: 009, Step: 2056, Loss: 0.9776\n",
            "Epoch: 009, Step: 2057, Loss: 1.1242\n",
            "Epoch: 009, Step: 2058, Loss: 0.9694\n",
            "Epoch: 009, Step: 2059, Loss: 0.8386\n",
            "Epoch: 009, Step: 2060, Loss: 0.9530\n",
            "Epoch: 009, Step: 2061, Loss: 1.0144\n",
            "Epoch: 009, Step: 2062, Loss: 0.8821\n",
            "Epoch: 009, Step: 2063, Loss: 0.8719\n",
            "Epoch: 009, Step: 2064, Loss: 1.0147\n",
            "Epoch: 009, Step: 2065, Loss: 1.0843\n",
            "Epoch: 009, Step: 2066, Loss: 0.9031\n",
            "Epoch: 009, Step: 2067, Loss: 0.8860\n",
            "Epoch: 009, Step: 2068, Loss: 1.0687\n",
            "Epoch: 009, Step: 2069, Loss: 0.9133\n",
            "Epoch: 009, Step: 2070, Loss: 1.0298\n",
            "Epoch: 009, Step: 2071, Loss: 0.9847\n",
            "Epoch: 009, Step: 2072, Loss: 0.9164\n",
            "Epoch: 009, Step: 2073, Loss: 0.9867\n",
            "Epoch: 009, Step: 2074, Loss: 0.9472\n",
            "Epoch: 009, Step: 2075, Loss: 0.9407\n",
            "Epoch: 009, Step: 2076, Loss: 0.8155\n",
            "Epoch: 009, Step: 2077, Loss: 1.0130\n",
            "Epoch: 009, Step: 2078, Loss: 0.8356\n",
            "Epoch: 009, Step: 2079, Loss: 0.9483\n",
            "Epoch: 009, Step: 2080, Loss: 0.9620\n",
            "Epoch: 009, Step: 2081, Loss: 0.8428\n",
            "Epoch: 009, Step: 2082, Loss: 1.1167\n",
            "Epoch: 009, Step: 2083, Loss: 1.0489\n",
            "Epoch: 009, Step: 2084, Loss: 0.8929\n",
            "Epoch: 009, Step: 2085, Loss: 0.8220\n",
            "Epoch: 009, Step: 2086, Loss: 0.8184\n",
            "Epoch: 009, Step: 2087, Loss: 0.9134\n",
            "Epoch: 009, Step: 2088, Loss: 0.9521\n",
            "Epoch: 009, Step: 2089, Loss: 0.9343\n",
            "Epoch: 009, Step: 2090, Loss: 1.0732\n",
            "Epoch: 009, Step: 2091, Loss: 1.0452\n",
            "Epoch: 009, Step: 2092, Loss: 0.9393\n",
            "Epoch: 009, Step: 2093, Loss: 0.9294\n",
            "Epoch: 009, Step: 2094, Loss: 1.0543\n",
            "Epoch: 009, Step: 2095, Loss: 0.9669\n",
            "Epoch: 009, Step: 2096, Loss: 0.9541\n",
            "Epoch: 009, Step: 2097, Loss: 1.0177\n",
            "Epoch: 009, Step: 2098, Loss: 0.9185\n",
            "Epoch: 009, Step: 2099, Loss: 0.9803\n",
            "Epoch: 009, Step: 2100, Loss: 0.9290\n",
            "Epoch: 009, Step: 2101, Loss: 0.9991\n",
            "Epoch: 009, Step: 2102, Loss: 0.9571\n",
            "Epoch: 009, Step: 2103, Loss: 0.8877\n",
            "Epoch: 009, Step: 2104, Loss: 0.7843\n",
            "Epoch: 009, Step: 2105, Loss: 0.9120\n",
            "Epoch: 009, Step: 2106, Loss: 0.8880\n",
            "Epoch: 009, Step: 2107, Loss: 0.9654\n",
            "Epoch: 009, Step: 2108, Loss: 1.0132\n",
            "Epoch: 009, Step: 2109, Loss: 0.9307\n",
            "Epoch: 009, Step: 2110, Loss: 0.8054\n",
            "Epoch: 009, Step: 2111, Loss: 0.9186\n",
            "Epoch: 009, Step: 2112, Loss: 0.9543\n",
            "Epoch: 009, Step: 2113, Loss: 1.0558\n",
            "Epoch: 009, Step: 2114, Loss: 0.9672\n",
            "Epoch: 009, Step: 2115, Loss: 0.9923\n",
            "Epoch: 009, Step: 2116, Loss: 0.8753\n",
            "Epoch: 009, Step: 2117, Loss: 1.1131\n",
            "Epoch: 009, Step: 2118, Loss: 0.8502\n",
            "Epoch: 009, Step: 2119, Loss: 0.9160\n",
            "Epoch: 009, Step: 2120, Loss: 1.0057\n",
            "Epoch: 009, Step: 2121, Loss: 1.0825\n",
            "Epoch: 009, Step: 2122, Loss: 0.9545\n",
            "Epoch: 009, Step: 2123, Loss: 1.0790\n",
            "Epoch: 009, Step: 2124, Loss: 1.0607\n",
            "Epoch: 009, Step: 2125, Loss: 1.0158\n",
            "Epoch: 009, Step: 2126, Loss: 0.8974\n",
            "Epoch: 009, Step: 2127, Loss: 1.0378\n",
            "Epoch: 009, Step: 2128, Loss: 1.0324\n",
            "Epoch: 009, Step: 2129, Loss: 1.0212\n",
            "Epoch: 009, Step: 2130, Loss: 0.9798\n",
            "Epoch: 009, Step: 2131, Loss: 1.0014\n",
            "Epoch: 009, Step: 2132, Loss: 0.9808\n",
            "Epoch: 009, Step: 2133, Loss: 0.9996\n",
            "Epoch: 009, Step: 2134, Loss: 1.1424\n",
            "Epoch: 009, Step: 2135, Loss: 1.0294\n",
            "Epoch: 009, Step: 2136, Loss: 0.8018\n",
            "Epoch: 009, Step: 2137, Loss: 0.9016\n",
            "Epoch: 009, Step: 2138, Loss: 0.8214\n",
            "Epoch: 009, Step: 2139, Loss: 0.8765\n",
            "Epoch: 009, Step: 2140, Loss: 1.0689\n",
            "Epoch: 009, Step: 2141, Loss: 0.8853\n",
            "Epoch: 009, Step: 2142, Loss: 0.9939\n",
            "Epoch: 009, Step: 2143, Loss: 1.0532\n",
            "Epoch: 009, Step: 2144, Loss: 0.9605\n",
            "Epoch: 009, Step: 2145, Loss: 0.9236\n",
            "Epoch: 009, Step: 2146, Loss: 0.8196\n",
            "Epoch: 009, Step: 2147, Loss: 0.9516\n",
            "Epoch: 009, Step: 2148, Loss: 0.9826\n",
            "Epoch: 009, Step: 2149, Loss: 0.9732\n",
            "Epoch: 009, Step: 2150, Loss: 0.9715\n",
            "Epoch: 009, Step: 2151, Loss: 0.9914\n",
            "Epoch: 009, Step: 2152, Loss: 1.0640\n",
            "Epoch: 009, Step: 2153, Loss: 1.0738\n",
            "Epoch: 009, Step: 2154, Loss: 0.8426\n",
            "Epoch: 009, Step: 2155, Loss: 1.0386\n",
            "Epoch: 009, Step: 2156, Loss: 1.0437\n",
            "Epoch: 009, Step: 2157, Loss: 0.9238\n",
            "Epoch: 009, Step: 2158, Loss: 0.8172\n",
            "Epoch: 009, Step: 2159, Loss: 0.9061\n",
            "Epoch: 009, Step: 2160, Loss: 0.9687\n",
            "Epoch: 009, Step: 2161, Loss: 0.9996\n",
            "Epoch: 009, Step: 2162, Loss: 1.0265\n",
            "Epoch: 009, Step: 2163, Loss: 0.7737\n",
            "Epoch: 009, Step: 2164, Loss: 0.8645\n",
            "Epoch: 009, Step: 2165, Loss: 1.0685\n",
            "Epoch: 009, Step: 2166, Loss: 1.0631\n",
            "Epoch: 009, Step: 2167, Loss: 0.9520\n",
            "Epoch: 009, Step: 2168, Loss: 0.9616\n",
            "Epoch: 009, Step: 2169, Loss: 1.0380\n",
            "Epoch: 009, Step: 2170, Loss: 0.9558\n",
            "Epoch: 009, Step: 2171, Loss: 1.0148\n",
            "Epoch: 009, Step: 2172, Loss: 0.7785\n",
            "Epoch: 009, Step: 2173, Loss: 1.1455\n",
            "Epoch: 009, Step: 2174, Loss: 1.2350\n",
            "Epoch: 009, Step: 2175, Loss: 0.9790\n",
            "Epoch: 009, Step: 2176, Loss: 1.0099\n",
            "Epoch: 009, Step: 2177, Loss: 0.8695\n",
            "Epoch: 009, Step: 2178, Loss: 0.9842\n",
            "Epoch: 009, Step: 2179, Loss: 0.9830\n",
            "Epoch: 009, Step: 2180, Loss: 0.9991\n",
            "Epoch: 009, Step: 2181, Loss: 0.8849\n",
            "Epoch: 009, Step: 2182, Loss: 0.9336\n",
            "Epoch: 009, Step: 2183, Loss: 0.8868\n",
            "Epoch: 009, Step: 2184, Loss: 0.9540\n",
            "Epoch: 009, Step: 2185, Loss: 0.9234\n",
            "Epoch: 009, Step: 2186, Loss: 0.8578\n",
            "Epoch: 009, Step: 2187, Loss: 0.8807\n",
            "Epoch: 009, Step: 2188, Loss: 0.9995\n",
            "Epoch: 009, Step: 2189, Loss: 1.0612\n",
            "Epoch: 009, Step: 2190, Loss: 0.9408\n",
            "Epoch: 009, Step: 2191, Loss: 0.9049\n",
            "Epoch: 009, Step: 2192, Loss: 0.8264\n",
            "Epoch: 009, Step: 2193, Loss: 0.9440\n",
            "Epoch: 009, Step: 2194, Loss: 0.9415\n",
            "Epoch: 009, Step: 2195, Loss: 1.0522\n",
            "Epoch: 009, Step: 2196, Loss: 0.9862\n",
            "Epoch: 009, Step: 2197, Loss: 0.9503\n",
            "Epoch: 009, Step: 2198, Loss: 1.0848\n",
            "Epoch: 009, Step: 2199, Loss: 1.0285\n",
            "Epoch: 009, Step: 2200, Loss: 1.0721\n",
            "Epoch: 009, Step: 2201, Loss: 1.0703\n",
            "Epoch: 009, Step: 2202, Loss: 0.9147\n",
            "Epoch: 009, Step: 2203, Loss: 0.7992\n",
            "Epoch: 009, Step: 2204, Loss: 0.9508\n",
            "Epoch: 009, Step: 2205, Loss: 1.1567\n",
            "Epoch: 009, Step: 2206, Loss: 0.9430\n",
            "Epoch: 009, Step: 2207, Loss: 0.9046\n",
            "Epoch: 009, Step: 2208, Loss: 1.1543\n",
            "Epoch: 009, Step: 2209, Loss: 0.9777\n",
            "Epoch: 009, Step: 2210, Loss: 0.8344\n",
            "Epoch: 009, Step: 2211, Loss: 1.0258\n",
            "Epoch: 009, Step: 2212, Loss: 1.0330\n",
            "Epoch: 009, Step: 2213, Loss: 1.0908\n",
            "Epoch: 009, Step: 2214, Loss: 0.9093\n",
            "Epoch: 009, Step: 2215, Loss: 0.8306\n",
            "Epoch: 009, Step: 2216, Loss: 0.9468\n",
            "Epoch: 009, Step: 2217, Loss: 0.9216\n",
            "Epoch: 009, Step: 2218, Loss: 0.9454\n",
            "Epoch: 009, Step: 2219, Loss: 0.9480\n",
            "Epoch: 009, Step: 2220, Loss: 0.8665\n",
            "Epoch: 009, Step: 2221, Loss: 1.0281\n",
            "Epoch: 009, Step: 2222, Loss: 0.8995\n",
            "Epoch: 009, Step: 2223, Loss: 0.8859\n",
            "Epoch: 009, Step: 2224, Loss: 0.9863\n",
            "Epoch: 009, Step: 2225, Loss: 0.8002\n",
            "Epoch: 009, Step: 2226, Loss: 0.9526\n",
            "Epoch: 009, Step: 2227, Loss: 0.9483\n",
            "Epoch: 009, Step: 2228, Loss: 0.9728\n",
            "Epoch: 009, Step: 2229, Loss: 1.0262\n",
            "Epoch: 009, Step: 2230, Loss: 0.9932\n",
            "Epoch: 009, Step: 2231, Loss: 1.0291\n",
            "Epoch: 009, Step: 2232, Loss: 0.8718\n",
            "Epoch: 009, Step: 2233, Loss: 0.8383\n",
            "Epoch: 009, Step: 2234, Loss: 0.8171\n",
            "Epoch: 009, Step: 2235, Loss: 1.0624\n",
            "Epoch: 009, Step: 2236, Loss: 0.8941\n",
            "Epoch: 009, Step: 2237, Loss: 1.1889\n",
            "Epoch: 009, Step: 2238, Loss: 1.1474\n",
            "Epoch: 009, Step: 2239, Loss: 1.0073\n",
            "Epoch: 009, Step: 2240, Loss: 0.9062\n",
            "Epoch: 009, Step: 2241, Loss: 0.8327\n",
            "Epoch: 009, Step: 2242, Loss: 0.7981\n",
            "Epoch: 009, Step: 2243, Loss: 1.1458\n",
            "Epoch: 009, Step: 2244, Loss: 0.8929\n",
            "Epoch: 009, Step: 2245, Loss: 0.8412\n",
            "Epoch: 009, Step: 2246, Loss: 0.9484\n",
            "Epoch: 009, Step: 2247, Loss: 0.9083\n",
            "Epoch: 009, Step: 2248, Loss: 1.0148\n",
            "Epoch: 009, Step: 2249, Loss: 1.0149\n",
            "Epoch: 009, Step: 2250, Loss: 0.9117\n",
            "Epoch: 009, Step: 2251, Loss: 0.8484\n",
            "Epoch: 009, Step: 2252, Loss: 0.7872\n",
            "Epoch: 009, Step: 2253, Loss: 0.9522\n",
            "Epoch: 009, Step: 2254, Loss: 0.7897\n",
            "Epoch: 009, Step: 2255, Loss: 1.1630\n",
            "Epoch: 009, Step: 2256, Loss: 0.9861\n",
            "Epoch: 009, Step: 2257, Loss: 1.0070\n",
            "Epoch: 009, Step: 2258, Loss: 0.9099\n",
            "Epoch: 009, Step: 2259, Loss: 0.9997\n",
            "Epoch: 009, Step: 2260, Loss: 0.8957\n",
            "Epoch: 009, Step: 2261, Loss: 0.9983\n",
            "Epoch: 009, Step: 2262, Loss: 1.0589\n",
            "Epoch: 009, Step: 2263, Loss: 0.9688\n",
            "Epoch: 009, Step: 2264, Loss: 0.7520\n",
            "Epoch: 009, Step: 2265, Loss: 0.9310\n",
            "Epoch: 009, Step: 2266, Loss: 0.8948\n",
            "Epoch: 009, Step: 2267, Loss: 1.0807\n",
            "Epoch: 009, Step: 2268, Loss: 0.9906\n",
            "Epoch: 009, Step: 2269, Loss: 1.0058\n",
            "Epoch: 009, Step: 2270, Loss: 0.9163\n",
            "Epoch: 009, Step: 2271, Loss: 1.0014\n",
            "Epoch: 009, Step: 2272, Loss: 0.9956\n",
            "Epoch: 009, Step: 2273, Loss: 0.9655\n",
            "Epoch: 009, Step: 2274, Loss: 0.9447\n",
            "Epoch: 009, Step: 2275, Loss: 1.1119\n",
            "Epoch: 009, Step: 2276, Loss: 0.8200\n",
            "Epoch: 009, Step: 2277, Loss: 0.8916\n",
            "Epoch: 009, Step: 2278, Loss: 0.9566\n",
            "Epoch: 009, Step: 2279, Loss: 1.0832\n",
            "Epoch: 009, Step: 2280, Loss: 1.0042\n",
            "Epoch: 009, Step: 2281, Loss: 1.0916\n",
            "Epoch: 009, Step: 2282, Loss: 0.8749\n",
            "Epoch: 009, Step: 2283, Loss: 0.9542\n",
            "Epoch: 009, Step: 2284, Loss: 0.9710\n",
            "Epoch: 009, Step: 2285, Loss: 0.9223\n",
            "Epoch: 009, Step: 2286, Loss: 1.0626\n",
            "Epoch: 009, Step: 2287, Loss: 1.0096\n",
            "Epoch: 009, Step: 2288, Loss: 0.9900\n",
            "Epoch: 009, Step: 2289, Loss: 0.9746\n",
            "Epoch: 009, Step: 2290, Loss: 0.9614\n",
            "Epoch: 009, Step: 2291, Loss: 0.8799\n",
            "Epoch: 009, Step: 2292, Loss: 0.9134\n",
            "Epoch: 009, Step: 2293, Loss: 0.8745\n",
            "Epoch: 009, Step: 2294, Loss: 1.0365\n",
            "Epoch: 009, Step: 2295, Loss: 0.9564\n",
            "Epoch: 009, Step: 2296, Loss: 0.8143\n",
            "Epoch: 009, Step: 2297, Loss: 0.7623\n",
            "Epoch: 009, Step: 2298, Loss: 0.9769\n",
            "Epoch: 009, Step: 2299, Loss: 1.0061\n",
            "Epoch: 009, Step: 2300, Loss: 0.9902\n",
            "Epoch: 009, Step: 2301, Loss: 1.0426\n",
            "Epoch: 009, Step: 2302, Loss: 0.9206\n",
            "Epoch: 009, Step: 2303, Loss: 0.8644\n",
            "Epoch: 009, Step: 2304, Loss: 1.0460\n",
            "Epoch: 009, Step: 2305, Loss: 0.9966\n",
            "Epoch: 009, Step: 2306, Loss: 0.9606\n",
            "Epoch: 009, Step: 2307, Loss: 0.9306\n",
            "Epoch: 009, Step: 2308, Loss: 0.8992\n",
            "Epoch: 009, Step: 2309, Loss: 0.7649\n",
            "Epoch: 009, Step: 2310, Loss: 1.1692\n",
            "Epoch: 009, Step: 2311, Loss: 1.1658\n",
            "Epoch: 009, Step: 2312, Loss: 0.9719\n",
            "Epoch: 009, Step: 2313, Loss: 1.1565\n",
            "Epoch: 009, Step: 2314, Loss: 1.0421\n",
            "Epoch: 009, Step: 2315, Loss: 0.9928\n",
            "Epoch: 009, Step: 2316, Loss: 1.0580\n",
            "Epoch: 009, Step: 2317, Loss: 0.9216\n",
            "Epoch: 009, Step: 2318, Loss: 1.0540\n",
            "Epoch: 009, Step: 2319, Loss: 0.8661\n",
            "Epoch: 009, Step: 2320, Loss: 0.8733\n",
            "Epoch: 009, Step: 2321, Loss: 1.0003\n",
            "Epoch: 009, Step: 2322, Loss: 0.8630\n",
            "Epoch: 009, Step: 2323, Loss: 0.9136\n",
            "Epoch: 009, Step: 2324, Loss: 0.9307\n",
            "Epoch: 009, Step: 2325, Loss: 0.9453\n",
            "Epoch: 009, Step: 2326, Loss: 0.8210\n",
            "Epoch: 009, Step: 2327, Loss: 0.9513\n",
            "Epoch: 009, Step: 2328, Loss: 1.0928\n",
            "Epoch: 009, Step: 2329, Loss: 0.9692\n",
            "Epoch: 009, Step: 2330, Loss: 0.9704\n",
            "Epoch: 009, Step: 2331, Loss: 0.9933\n",
            "Epoch: 009, Step: 2332, Loss: 0.8556\n",
            "Epoch: 009, Step: 2333, Loss: 0.9118\n",
            "Epoch: 009, Step: 2334, Loss: 0.8963\n",
            "Epoch: 009, Step: 2335, Loss: 0.9141\n",
            "Epoch: 009, Step: 2336, Loss: 0.9953\n",
            "Epoch: 009, Step: 2337, Loss: 0.8761\n",
            "Epoch: 009, Step: 2338, Loss: 1.1002\n",
            "Epoch: 009, Step: 2339, Loss: 0.9981\n",
            "Epoch: 009, Step: 2340, Loss: 0.7914\n",
            "Epoch: 009, Step: 2341, Loss: 0.8165\n",
            "Epoch: 009, Step: 2342, Loss: 1.0172\n",
            "Epoch: 009, Step: 2343, Loss: 1.0010\n",
            "Epoch: 009, Step: 2344, Loss: 1.0621\n",
            "Epoch: 009, Step: 2345, Loss: 0.8938\n",
            "Epoch: 009, Step: 2346, Loss: 0.9743\n",
            "Epoch: 009, Step: 2347, Loss: 1.1538\n",
            "Epoch: 009, Step: 2348, Loss: 0.9648\n",
            "Epoch: 009, Step: 2349, Loss: 0.8781\n",
            "Epoch: 009, Step: 2350, Loss: 1.1054\n",
            "Epoch: 009, Step: 2351, Loss: 0.8198\n",
            "Epoch: 009, Step: 2352, Loss: 0.9073\n",
            "Epoch: 009, Step: 2353, Loss: 1.0640\n",
            "Epoch: 009, Step: 2354, Loss: 0.7611\n",
            "Epoch: 009, Step: 2355, Loss: 1.0435\n",
            "Epoch: 009, Step: 2356, Loss: 0.9687\n",
            "Epoch: 009, Step: 2357, Loss: 0.8945\n",
            "Epoch: 009, Step: 2358, Loss: 0.8419\n",
            "Epoch: 009, Step: 2359, Loss: 0.8515\n",
            "Epoch: 009, Step: 2360, Loss: 1.0187\n",
            "Epoch: 009, Step: 2361, Loss: 0.9460\n",
            "Epoch: 009, Step: 2362, Loss: 0.9531\n",
            "Epoch: 009, Step: 2363, Loss: 1.0991\n",
            "Epoch: 009, Step: 2364, Loss: 0.8972\n",
            "Epoch: 009, Step: 2365, Loss: 0.9262\n",
            "Epoch: 009, Step: 2366, Loss: 0.9886\n",
            "Epoch: 009, Step: 2367, Loss: 1.0245\n",
            "Epoch: 009, Step: 2368, Loss: 0.8529\n",
            "Epoch: 009, Step: 2369, Loss: 0.9011\n",
            "Epoch: 009, Step: 2370, Loss: 1.0321\n",
            "Epoch: 009, Step: 2371, Loss: 0.8924\n",
            "Epoch: 009, Step: 2372, Loss: 0.9979\n",
            "Epoch: 009, Step: 2373, Loss: 1.1058\n",
            "Epoch: 009, Step: 2374, Loss: 0.9287\n",
            "Epoch: 009, Step: 2375, Loss: 0.9796\n",
            "Epoch: 009, Step: 2376, Loss: 0.9444\n",
            "Epoch: 009, Step: 2377, Loss: 0.9656\n",
            "Epoch: 009, Step: 2378, Loss: 0.9222\n",
            "Epoch: 009, Step: 2379, Loss: 0.9811\n",
            "Epoch: 009, Step: 2380, Loss: 0.9322\n",
            "Epoch: 009, Step: 2381, Loss: 0.9293\n",
            "Epoch: 009, Step: 2382, Loss: 0.9880\n",
            "Epoch: 009, Step: 2383, Loss: 1.0017\n",
            "Epoch: 009, Step: 2384, Loss: 0.9034\n",
            "Epoch: 009, Step: 2385, Loss: 0.8653\n",
            "Epoch: 009, Step: 2386, Loss: 0.8339\n",
            "Epoch: 009, Step: 2387, Loss: 1.0316\n",
            "Epoch: 009, Step: 2388, Loss: 0.7840\n",
            "Epoch: 009, Step: 2389, Loss: 1.0982\n",
            "Epoch: 009, Step: 2390, Loss: 1.0469\n",
            "Epoch: 009, Step: 2391, Loss: 0.9998\n",
            "Epoch: 009, Step: 2392, Loss: 0.8655\n",
            "Epoch: 009, Step: 2393, Loss: 1.0161\n",
            "Epoch: 009, Step: 2394, Loss: 0.9576\n",
            "Epoch: 009, Step: 2395, Loss: 0.9796\n",
            "Epoch: 009, Step: 2396, Loss: 0.9525\n",
            "Epoch: 009, Step: 2397, Loss: 1.0643\n",
            "Epoch: 009, Step: 2398, Loss: 0.8692\n",
            "Epoch: 009, Step: 2399, Loss: 1.0245\n",
            "Epoch: 009, Step: 2400, Loss: 1.0196\n",
            "Epoch: 009, Step: 2401, Loss: 0.9832\n",
            "Epoch: 009, Step: 2402, Loss: 0.9853\n",
            "Epoch: 009, Step: 2403, Loss: 0.9692\n",
            "Epoch: 009, Step: 2404, Loss: 1.0146\n",
            "Epoch: 009, Step: 2405, Loss: 0.8682\n",
            "Epoch: 009, Step: 2406, Loss: 0.9244\n",
            "Epoch: 009, Step: 2407, Loss: 0.9787\n",
            "Epoch: 009, Step: 2408, Loss: 0.8332\n",
            "Epoch: 009, Step: 2409, Loss: 1.0634\n",
            "Epoch: 009, Step: 2410, Loss: 1.1391\n",
            "Epoch: 009, Step: 2411, Loss: 1.0435\n",
            "Epoch: 009, Step: 2412, Loss: 0.9549\n",
            "Epoch: 009, Step: 2413, Loss: 0.9312\n",
            "Epoch: 009, Step: 2414, Loss: 0.9444\n",
            "Epoch: 009, Step: 2415, Loss: 0.8182\n",
            "Epoch: 009, Step: 2416, Loss: 0.9423\n",
            "Epoch: 009, Step: 2417, Loss: 1.0331\n",
            "Epoch: 009, Step: 2418, Loss: 1.0644\n",
            "Epoch: 009, Step: 2419, Loss: 0.9552\n",
            "Epoch: 009, Step: 2420, Loss: 0.9489\n",
            "Epoch: 009, Step: 2421, Loss: 1.0896\n",
            "Epoch: 009, Step: 2422, Loss: 0.9126\n",
            "Epoch: 009, Step: 2423, Loss: 0.9052\n",
            "Epoch: 009, Step: 2424, Loss: 0.8820\n",
            "Epoch: 009, Step: 2425, Loss: 0.9852\n",
            "Epoch: 009, Step: 2426, Loss: 0.9091\n",
            "Epoch: 009, Step: 2427, Loss: 0.9350\n",
            "Epoch: 009, Step: 2428, Loss: 0.9466\n",
            "Epoch: 009, Step: 2429, Loss: 0.8617\n",
            "Epoch: 009, Step: 2430, Loss: 1.0033\n",
            "Epoch: 009, Step: 2431, Loss: 1.1740\n",
            "Epoch: 009, Step: 2432, Loss: 0.8398\n",
            "Epoch: 009, Step: 2433, Loss: 0.9072\n",
            "Epoch: 009, Step: 2434, Loss: 1.0712\n",
            "Epoch: 009, Step: 2435, Loss: 0.9104\n",
            "Epoch: 009, Step: 2436, Loss: 0.8552\n",
            "Epoch: 009, Step: 2437, Loss: 0.8933\n",
            "Epoch: 009, Step: 2438, Loss: 1.0979\n",
            "Epoch: 009, Step: 2439, Loss: 1.0139\n",
            "Epoch: 009, Step: 2440, Loss: 0.9172\n",
            "Epoch: 009, Step: 2441, Loss: 0.9276\n",
            "Epoch: 009, Step: 2442, Loss: 0.9196\n",
            "Epoch: 009, Step: 2443, Loss: 1.1317\n",
            "Epoch: 009, Step: 2444, Loss: 0.9451\n",
            "Epoch: 009, Step: 2445, Loss: 0.9123\n",
            "Epoch: 009, Step: 2446, Loss: 0.9817\n",
            "Epoch: 009, Step: 2447, Loss: 0.9071\n",
            "Epoch: 009, Step: 2448, Loss: 0.9911\n",
            "Epoch: 009, Step: 2449, Loss: 0.9029\n",
            "Epoch: 009, Step: 2450, Loss: 1.0250\n",
            "Epoch: 009, Step: 2451, Loss: 0.9929\n",
            "Epoch: 009, Step: 2452, Loss: 0.9536\n",
            "Epoch: 009, Step: 2453, Loss: 0.9286\n",
            "Epoch: 009, Step: 2454, Loss: 0.9573\n",
            "Epoch: 009, Step: 2455, Loss: 0.9181\n",
            "Epoch: 009, Step: 2456, Loss: 0.8785\n",
            "Epoch: 009, Step: 2457, Loss: 0.9109\n",
            "Epoch: 009, Step: 2458, Loss: 0.8956\n",
            "Epoch: 009, Step: 2459, Loss: 0.9282\n",
            "Epoch: 009, Step: 2460, Loss: 0.8388\n",
            "Epoch: 009, Step: 2461, Loss: 1.0804\n",
            "Epoch: 009, Step: 2462, Loss: 1.0022\n",
            "Epoch: 009, Step: 2463, Loss: 1.0040\n",
            "Epoch: 009, Step: 2464, Loss: 0.8980\n",
            "Epoch: 009, Step: 2465, Loss: 0.8431\n",
            "Epoch: 009, Step: 2466, Loss: 0.9131\n",
            "Epoch: 009, Step: 2467, Loss: 0.9022\n",
            "Epoch: 009, Step: 2468, Loss: 0.9304\n",
            "Epoch: 009, Step: 2469, Loss: 1.0042\n",
            "Epoch: 009, Step: 2470, Loss: 1.1245\n",
            "Epoch: 009, Step: 2471, Loss: 0.8719\n",
            "Epoch: 009, Step: 2472, Loss: 1.0479\n",
            "Epoch: 009, Step: 2473, Loss: 0.9193\n",
            "Epoch: 009, Step: 2474, Loss: 0.7552\n",
            "Epoch: 009, Step: 2475, Loss: 1.0069\n",
            "Epoch: 009, Step: 2476, Loss: 0.9516\n",
            "Epoch: 009, Step: 2477, Loss: 0.9146\n",
            "Epoch: 009, Step: 2478, Loss: 1.0522\n",
            "Epoch: 009, Step: 2479, Loss: 1.0127\n",
            "Epoch: 009, Step: 2480, Loss: 0.8649\n",
            "Epoch: 009, Step: 2481, Loss: 0.8837\n",
            "Epoch: 009, Step: 2482, Loss: 1.0755\n",
            "Epoch: 009, Step: 2483, Loss: 0.8723\n",
            "Epoch: 009, Step: 2484, Loss: 0.9643\n",
            "Epoch: 009, Step: 2485, Loss: 0.9964\n",
            "Epoch: 009, Step: 2486, Loss: 1.0798\n",
            "Epoch: 009, Step: 2487, Loss: 1.0428\n",
            "Epoch: 009, Step: 2488, Loss: 0.8886\n",
            "Epoch: 009, Step: 2489, Loss: 0.9991\n",
            "Epoch: 009, Step: 2490, Loss: 1.0569\n",
            "Epoch: 009, Step: 2491, Loss: 0.9103\n",
            "Epoch: 009, Step: 2492, Loss: 0.9547\n",
            "Epoch: 009, Step: 2493, Loss: 0.9563\n",
            "Epoch: 009, Step: 2494, Loss: 0.9775\n",
            "Epoch: 009, Step: 2495, Loss: 0.9518\n",
            "Epoch: 009, Step: 2496, Loss: 0.9811\n",
            "Epoch: 009, Step: 2497, Loss: 1.2566\n",
            "Epoch: 009, Step: 2498, Loss: 0.9797\n",
            "Epoch: 009, Step: 2499, Loss: 1.0446\n",
            "Epoch: 009, Step: 2500, Loss: 0.9900\n",
            "Epoch: 009, Step: 2501, Loss: 0.9113\n",
            "Epoch: 009, Step: 2502, Loss: 0.9282\n",
            "Epoch: 009, Step: 2503, Loss: 0.9900\n",
            "Epoch: 009, Step: 2504, Loss: 1.0628\n",
            "Epoch: 009, Step: 2505, Loss: 0.8936\n",
            "Epoch: 009, Step: 2506, Loss: 0.8383\n",
            "Epoch: 009, Step: 2507, Loss: 1.0024\n",
            "Epoch: 009, Step: 2508, Loss: 0.9289\n",
            "Epoch: 009, Step: 2509, Loss: 0.8614\n",
            "Epoch: 009, Step: 2510, Loss: 0.8585\n",
            "Epoch: 009, Step: 2511, Loss: 0.9629\n",
            "Epoch: 009, Step: 2512, Loss: 0.9173\n",
            "Epoch: 009, Step: 2513, Loss: 0.9537\n",
            "Epoch: 009, Step: 2514, Loss: 1.0006\n",
            "Epoch: 009, Step: 2515, Loss: 0.8958\n",
            "Epoch: 009, Step: 2516, Loss: 1.0427\n",
            "Epoch: 009, Step: 2517, Loss: 0.9195\n",
            "Epoch: 009, Step: 2518, Loss: 1.0157\n",
            "Epoch: 009, Step: 2519, Loss: 0.9980\n",
            "Epoch: 009, Step: 2520, Loss: 0.9365\n",
            "Epoch: 009, Step: 2521, Loss: 0.9890\n",
            "Epoch: 009, Step: 2522, Loss: 0.8698\n",
            "Epoch: 009, Step: 2523, Loss: 0.9391\n",
            "Epoch: 009, Step: 2524, Loss: 1.0483\n",
            "Epoch: 009, Step: 2525, Loss: 0.9826\n",
            "Epoch: 009, Step: 2526, Loss: 1.0865\n",
            "Epoch: 009, Step: 2527, Loss: 0.8297\n",
            "Epoch: 009, Step: 2528, Loss: 0.9132\n",
            "Epoch: 009, Step: 2529, Loss: 0.9326\n",
            "Epoch: 009, Step: 2530, Loss: 0.9711\n",
            "Epoch: 009, Step: 2531, Loss: 0.9465\n",
            "Epoch: 009, Step: 2532, Loss: 0.9545\n",
            "Epoch: 009, Step: 2533, Loss: 1.1230\n",
            "Epoch: 009, Step: 2534, Loss: 0.9407\n",
            "Epoch: 009, Step: 2535, Loss: 0.9159\n",
            "Epoch: 009, Step: 2536, Loss: 0.8334\n",
            "Epoch: 009, Step: 2537, Loss: 1.0070\n",
            "Epoch: 009, Step: 2538, Loss: 1.0484\n",
            "Epoch: 009, Step: 2539, Loss: 0.9356\n",
            "Epoch: 009, Step: 2540, Loss: 0.8121\n",
            "Epoch: 009, Step: 2541, Loss: 0.8855\n",
            "Epoch: 009, Step: 2542, Loss: 0.9688\n",
            "Epoch: 009, Step: 2543, Loss: 0.8426\n",
            "Epoch: 009, Step: 2544, Loss: 0.7502\n",
            "Epoch: 009, Step: 2545, Loss: 0.8947\n",
            "Epoch: 009, Step: 2546, Loss: 0.8875\n",
            "Epoch: 009, Step: 2547, Loss: 0.8141\n",
            "Epoch: 009, Step: 2548, Loss: 1.0735\n",
            "Epoch: 009, Step: 2549, Loss: 0.9450\n",
            "Epoch: 009, Step: 2550, Loss: 1.0417\n",
            "Epoch: 009, Step: 2551, Loss: 0.9366\n",
            "Epoch: 009, Step: 2552, Loss: 0.8934\n",
            "Epoch: 009, Step: 2553, Loss: 1.0772\n",
            "Epoch: 009, Step: 2554, Loss: 0.9809\n",
            "Epoch: 009, Step: 2555, Loss: 1.0131\n",
            "Epoch: 009, Step: 2556, Loss: 0.8996\n",
            "Epoch: 009, Step: 2557, Loss: 0.8727\n",
            "Epoch: 009, Step: 2558, Loss: 0.8826\n",
            "Epoch: 009, Step: 2559, Loss: 0.8776\n",
            "Epoch: 009, Step: 2560, Loss: 1.0115\n",
            "Epoch: 009, Step: 2561, Loss: 0.8229\n",
            "Epoch: 009, Step: 2562, Loss: 1.1595\n",
            "Epoch: 009, Step: 2563, Loss: 0.8681\n",
            "Epoch: 009, Step: 2564, Loss: 0.9789\n",
            "Epoch: 009, Step: 2565, Loss: 0.9665\n",
            "Epoch: 009, Step: 2566, Loss: 0.9854\n",
            "Epoch: 009, Step: 2567, Loss: 0.8378\n",
            "Epoch: 009, Step: 2568, Loss: 0.8889\n",
            "Epoch: 009, Step: 2569, Loss: 0.9637\n",
            "Epoch: 009, Step: 2570, Loss: 0.9618\n",
            "Epoch: 009, Step: 2571, Loss: 0.9715\n",
            "Epoch: 009, Step: 2572, Loss: 0.9357\n",
            "Epoch: 009, Step: 2573, Loss: 1.2195\n",
            "Epoch: 009, Step: 2574, Loss: 1.0940\n",
            "Epoch: 009, Step: 2575, Loss: 0.9605\n",
            "Epoch: 009, Step: 2576, Loss: 0.9280\n",
            "Epoch: 009, Step: 2577, Loss: 1.0108\n",
            "Epoch: 009, Step: 2578, Loss: 0.9080\n",
            "Epoch: 009, Step: 2579, Loss: 0.9243\n",
            "Epoch: 009, Step: 2580, Loss: 1.0559\n",
            "Epoch: 009, Step: 2581, Loss: 1.0254\n",
            "Epoch: 009, Step: 2582, Loss: 0.8400\n",
            "Epoch: 009, Step: 2583, Loss: 1.0438\n",
            "Epoch: 009, Step: 2584, Loss: 1.0294\n",
            "Epoch: 009, Step: 2585, Loss: 0.9596\n",
            "Epoch: 009, Step: 2586, Loss: 0.9187\n",
            "Epoch: 009, Step: 2587, Loss: 0.9535\n",
            "Epoch: 009, Step: 2588, Loss: 0.9056\n",
            "Epoch: 009, Step: 2589, Loss: 0.9130\n",
            "Epoch: 009, Step: 2590, Loss: 0.8758\n",
            "Epoch: 009, Step: 2591, Loss: 0.8822\n",
            "Epoch: 009, Step: 2592, Loss: 0.9581\n",
            "Epoch: 009, Step: 2593, Loss: 0.9119\n",
            "Epoch: 009, Step: 2594, Loss: 0.9676\n",
            "Epoch: 009, Step: 2595, Loss: 0.8680\n",
            "Epoch: 009, Step: 2596, Loss: 0.9010\n",
            "Epoch: 009, Step: 2597, Loss: 1.0252\n",
            "Epoch: 009, Step: 2598, Loss: 0.8937\n",
            "Epoch: 009, Step: 2599, Loss: 0.9768\n",
            "Epoch: 009, Step: 2600, Loss: 0.8800\n",
            "Epoch: 009, Step: 2601, Loss: 0.8575\n",
            "Epoch: 009, Step: 2602, Loss: 1.0588\n",
            "Epoch: 009, Step: 2603, Loss: 0.6908\n",
            "Epoch: 009, Step: 2604, Loss: 0.8523\n",
            "Epoch: 009, Step: 2605, Loss: 0.8224\n",
            "Epoch: 009, Step: 2606, Loss: 1.0559\n",
            "Epoch: 009, Step: 2607, Loss: 0.9035\n",
            "Epoch: 009, Step: 2608, Loss: 0.9465\n",
            "Epoch: 009, Step: 2609, Loss: 1.0273\n",
            "Epoch: 009, Step: 2610, Loss: 0.8994\n",
            "Epoch: 009, Step: 2611, Loss: 1.0192\n",
            "Epoch: 009, Step: 2612, Loss: 0.9692\n",
            "Epoch: 009, Step: 2613, Loss: 0.9952\n",
            "Epoch: 009, Step: 2614, Loss: 1.0818\n",
            "Epoch: 009, Step: 2615, Loss: 0.9249\n",
            "Epoch: 009, Step: 2616, Loss: 1.0894\n",
            "Epoch: 009, Step: 2617, Loss: 1.0999\n",
            "Epoch: 009, Step: 2618, Loss: 1.0940\n",
            "Epoch: 009, Step: 2619, Loss: 0.9936\n",
            "Epoch: 009, Step: 2620, Loss: 0.9344\n",
            "Epoch: 009, Step: 2621, Loss: 0.8025\n",
            "Epoch: 009, Step: 2622, Loss: 0.8747\n",
            "Epoch: 009, Step: 2623, Loss: 0.9417\n",
            "Epoch: 009, Step: 2624, Loss: 1.0204\n",
            "Epoch: 009, Step: 2625, Loss: 1.0740\n",
            "Epoch: 009, Step: 2626, Loss: 0.8732\n",
            "Epoch: 009, Step: 2627, Loss: 0.9769\n",
            "Epoch: 009, Step: 2628, Loss: 1.0944\n",
            "Epoch: 009, Step: 2629, Loss: 1.0090\n",
            "Epoch: 009, Step: 2630, Loss: 0.9157\n",
            "Epoch: 009, Step: 2631, Loss: 0.7888\n",
            "Epoch: 009, Step: 2632, Loss: 0.9679\n",
            "Epoch: 009, Step: 2633, Loss: 0.9206\n",
            "Epoch: 009, Step: 2634, Loss: 1.0565\n",
            "Epoch: 009, Step: 2635, Loss: 0.9252\n",
            "Epoch: 009, Step: 2636, Loss: 0.9635\n",
            "Epoch: 009, Step: 2637, Loss: 0.9209\n",
            "Epoch: 009, Step: 2638, Loss: 1.0173\n",
            "Epoch: 009, Step: 2639, Loss: 1.0212\n",
            "Epoch: 009, Step: 2640, Loss: 1.0916\n",
            "Epoch: 009, Step: 2641, Loss: 0.9994\n",
            "Epoch: 009, Step: 2642, Loss: 1.0630\n",
            "Epoch: 009, Step: 2643, Loss: 0.9424\n",
            "Epoch: 009, Step: 2644, Loss: 0.8203\n",
            "Epoch: 009, Step: 2645, Loss: 0.9145\n",
            "Epoch: 009, Step: 2646, Loss: 0.9750\n",
            "Epoch: 009, Step: 2647, Loss: 0.9201\n",
            "Epoch: 009, Step: 2648, Loss: 1.0109\n",
            "Epoch: 009, Step: 2649, Loss: 0.8550\n",
            "Epoch: 009, Step: 2650, Loss: 0.7871\n",
            "Epoch: 009, Step: 2651, Loss: 0.8435\n",
            "Epoch: 009, Step: 2652, Loss: 0.9151\n",
            "Epoch: 009, Step: 2653, Loss: 0.9876\n",
            "Epoch: 009, Step: 2654, Loss: 1.0355\n",
            "Epoch: 009, Step: 2655, Loss: 0.9399\n",
            "Epoch: 009, Step: 2656, Loss: 0.9863\n",
            "Epoch: 009, Step: 2657, Loss: 1.0396\n",
            "Epoch: 009, Step: 2658, Loss: 0.9438\n",
            "Epoch: 009, Step: 2659, Loss: 0.9081\n",
            "Epoch: 009, Step: 2660, Loss: 0.9821\n",
            "Epoch: 009, Step: 2661, Loss: 1.0976\n",
            "Epoch: 009, Step: 2662, Loss: 1.0914\n",
            "Epoch: 009, Step: 2663, Loss: 0.8438\n",
            "Epoch: 009, Step: 2664, Loss: 1.0453\n",
            "Epoch: 009, Step: 2665, Loss: 0.7143\n",
            "Epoch: 009, Step: 2666, Loss: 0.8514\n",
            "Epoch: 009, Step: 2667, Loss: 1.0014\n",
            "Epoch: 009, Step: 2668, Loss: 1.1294\n",
            "Epoch: 009, Step: 2669, Loss: 1.0156\n",
            "Epoch: 009, Step: 2670, Loss: 0.9780\n",
            "Epoch: 009, Step: 2671, Loss: 1.0758\n",
            "Epoch: 009, Step: 2672, Loss: 0.9018\n",
            "Epoch: 009, Step: 2673, Loss: 0.9541\n",
            "Epoch: 009, Step: 2674, Loss: 0.8954\n",
            "Epoch: 009, Step: 2675, Loss: 0.9104\n",
            "Epoch: 009, Step: 2676, Loss: 0.9225\n",
            "Epoch: 009, Step: 2677, Loss: 0.8980\n",
            "Epoch: 009, Step: 2678, Loss: 1.0070\n",
            "Epoch: 009, Step: 2679, Loss: 0.9205\n",
            "Epoch: 009, Step: 2680, Loss: 0.8900\n",
            "Epoch: 009, Step: 2681, Loss: 0.9444\n",
            "Epoch: 009, Step: 2682, Loss: 0.9324\n",
            "Epoch: 009, Step: 2683, Loss: 0.9371\n",
            "Epoch: 009, Step: 2684, Loss: 1.1196\n",
            "Epoch: 009, Step: 2685, Loss: 0.9075\n",
            "Epoch: 009, Step: 2686, Loss: 1.0451\n",
            "Epoch: 009, Step: 2687, Loss: 1.0683\n",
            "Epoch: 009, Step: 2688, Loss: 0.8178\n",
            "Epoch: 009, Step: 2689, Loss: 0.8880\n",
            "Epoch: 009, Step: 2690, Loss: 1.0881\n",
            "Epoch: 009, Step: 2691, Loss: 0.9964\n",
            "Epoch: 009, Step: 2692, Loss: 1.0525\n",
            "Epoch: 009, Step: 2693, Loss: 0.9434\n",
            "Epoch: 009, Step: 2694, Loss: 0.9850\n",
            "Epoch: 009, Step: 2695, Loss: 1.0229\n",
            "Epoch: 009, Step: 2696, Loss: 0.9970\n",
            "Epoch: 009, Step: 2697, Loss: 1.0921\n",
            "Epoch: 009, Step: 2698, Loss: 1.0528\n",
            "Epoch: 009, Step: 2699, Loss: 1.0660\n",
            "Epoch: 009, Step: 2700, Loss: 0.8823\n",
            "Epoch: 009, Step: 2701, Loss: 0.9560\n",
            "Epoch: 009, Step: 2702, Loss: 0.9124\n",
            "Epoch: 009, Step: 2703, Loss: 0.9748\n",
            "Epoch: 009, Step: 2704, Loss: 0.9278\n",
            "Epoch: 009, Step: 2705, Loss: 0.9074\n",
            "Epoch: 009, Step: 2706, Loss: 0.8946\n",
            "Epoch: 009, Step: 2707, Loss: 1.1084\n",
            "Epoch: 009, Step: 2708, Loss: 0.9595\n",
            "Epoch: 009, Step: 2709, Loss: 1.0158\n",
            "Epoch: 009, Step: 2710, Loss: 0.8206\n",
            "Epoch: 009, Step: 2711, Loss: 0.9449\n",
            "Epoch: 009, Step: 2712, Loss: 1.0101\n",
            "Epoch: 009, Step: 2713, Loss: 0.9026\n",
            "Epoch: 009, Step: 2714, Loss: 0.9584\n",
            "Epoch: 009, Step: 2715, Loss: 0.9532\n",
            "Epoch: 009, Step: 2716, Loss: 0.8599\n",
            "Epoch: 009, Step: 2717, Loss: 0.8241\n",
            "Epoch: 009, Step: 2718, Loss: 1.0143\n",
            "Epoch: 009, Step: 2719, Loss: 0.8264\n",
            "Epoch: 009, Step: 2720, Loss: 0.8831\n",
            "Epoch: 009, Step: 2721, Loss: 0.9079\n",
            "Epoch: 009, Step: 2722, Loss: 0.8679\n",
            "Epoch: 009, Step: 2723, Loss: 0.8663\n",
            "Epoch: 009, Step: 2724, Loss: 0.8330\n",
            "Epoch: 009, Step: 2725, Loss: 0.9637\n",
            "Epoch: 009, Step: 2726, Loss: 0.9069\n",
            "Epoch: 009, Step: 2727, Loss: 0.9085\n",
            "Epoch: 009, Step: 2728, Loss: 0.8801\n",
            "Epoch: 009, Step: 2729, Loss: 0.9693\n",
            "Epoch: 009, Step: 2730, Loss: 1.1637\n",
            "Epoch: 009, Step: 2731, Loss: 1.0897\n",
            "Epoch: 009, Step: 2732, Loss: 0.9497\n",
            "Epoch: 009, Step: 2733, Loss: 0.8207\n",
            "Epoch: 009, Step: 2734, Loss: 0.9551\n",
            "Epoch: 009, Step: 2735, Loss: 1.0248\n",
            "Epoch: 009, Step: 2736, Loss: 1.0318\n",
            "Epoch: 009, Step: 2737, Loss: 0.9376\n",
            "Epoch: 009, Step: 2738, Loss: 1.0742\n",
            "Epoch: 009, Step: 2739, Loss: 0.9486\n",
            "Epoch: 009, Step: 2740, Loss: 0.8972\n",
            "Epoch: 009, Step: 2741, Loss: 0.9077\n",
            "Epoch: 009, Step: 2742, Loss: 1.0679\n",
            "Epoch: 009, Step: 2743, Loss: 1.0214\n",
            "Epoch: 009, Step: 2744, Loss: 0.7832\n",
            "Epoch: 009, Step: 2745, Loss: 0.8904\n",
            "Epoch: 009, Step: 2746, Loss: 0.9347\n",
            "Epoch: 009, Step: 2747, Loss: 1.1131\n",
            "Epoch: 009, Step: 2748, Loss: 0.9648\n",
            "Epoch: 009, Step: 2749, Loss: 1.0105\n",
            "Epoch: 009, Step: 2750, Loss: 0.9123\n",
            "Epoch: 009, Step: 2751, Loss: 1.1940\n",
            "Epoch: 009, Step: 2752, Loss: 1.0942\n",
            "Epoch: 009, Step: 2753, Loss: 1.0410\n",
            "Epoch: 009, Step: 2754, Loss: 1.1225\n",
            "Epoch: 009, Step: 2755, Loss: 0.9152\n",
            "Epoch: 009, Step: 2756, Loss: 0.9338\n",
            "Epoch: 009, Step: 2757, Loss: 1.0002\n",
            "Epoch: 009, Step: 2758, Loss: 0.8938\n",
            "Epoch: 009, Step: 2759, Loss: 1.0729\n",
            "Epoch: 009, Step: 2760, Loss: 1.0335\n",
            "Epoch: 009, Step: 2761, Loss: 1.0120\n",
            "Epoch: 009, Step: 2762, Loss: 0.9582\n",
            "Epoch: 009, Step: 2763, Loss: 0.8561\n",
            "Epoch: 009, Step: 2764, Loss: 0.9912\n",
            "Epoch: 009, Step: 2765, Loss: 0.9207\n",
            "Epoch: 009, Step: 2766, Loss: 1.0094\n",
            "Epoch: 009, Step: 2767, Loss: 0.9780\n",
            "Epoch: 009, Step: 2768, Loss: 0.8840\n",
            "Epoch: 009, Step: 2769, Loss: 0.8966\n",
            "Epoch: 009, Step: 2770, Loss: 1.0031\n",
            "Epoch: 009, Step: 2771, Loss: 0.9665\n",
            "Epoch: 009, Step: 2772, Loss: 0.9591\n",
            "Epoch: 009, Step: 2773, Loss: 0.9915\n",
            "Epoch: 009, Step: 2774, Loss: 0.9216\n",
            "Epoch: 009, Step: 2775, Loss: 0.7922\n",
            "Epoch: 009, Step: 2776, Loss: 0.8566\n",
            "Epoch: 009, Step: 2777, Loss: 1.0959\n",
            "Epoch: 009, Step: 2778, Loss: 0.9321\n",
            "Epoch: 009, Step: 2779, Loss: 0.9039\n",
            "Epoch: 009, Step: 2780, Loss: 0.7834\n",
            "Epoch: 009, Step: 2781, Loss: 0.9434\n",
            "Epoch: 009, Step: 2782, Loss: 0.9752\n",
            "Epoch: 009, Step: 2783, Loss: 1.0265\n",
            "Epoch: 009, Step: 2784, Loss: 0.8839\n",
            "Epoch: 009, Step: 2785, Loss: 0.9089\n",
            "Epoch: 009, Step: 2786, Loss: 0.8940\n",
            "Epoch: 009, Step: 2787, Loss: 1.0341\n",
            "Epoch: 009, Step: 2788, Loss: 1.0290\n",
            "Epoch: 009, Step: 2789, Loss: 1.0055\n",
            "Epoch: 009, Step: 2790, Loss: 0.9693\n",
            "Epoch: 009, Step: 2791, Loss: 0.9109\n",
            "Epoch: 009, Step: 2792, Loss: 1.0353\n",
            "Epoch: 009, Step: 2793, Loss: 1.0231\n",
            "Epoch: 009, Step: 2794, Loss: 0.8444\n",
            "Epoch: 009, Step: 2795, Loss: 0.8768\n",
            "Epoch: 009, Step: 2796, Loss: 0.8904\n",
            "Epoch: 009, Step: 2797, Loss: 0.8479\n",
            "Epoch: 009, Step: 2798, Loss: 1.0265\n",
            "Epoch: 009, Step: 2799, Loss: 0.8445\n",
            "Epoch: 009, Step: 2800, Loss: 0.9573\n",
            "Epoch: 009, Step: 2801, Loss: 0.8909\n",
            "Epoch: 009, Step: 2802, Loss: 0.8031\n",
            "Epoch: 009, Step: 2803, Loss: 1.1102\n",
            "Epoch: 009, Step: 2804, Loss: 0.8120\n",
            "Epoch: 009, Step: 2805, Loss: 0.8890\n",
            "Epoch: 009, Step: 2806, Loss: 0.8802\n",
            "Epoch: 009, Step: 2807, Loss: 0.8942\n",
            "Epoch: 009, Step: 2808, Loss: 1.0316\n",
            "Epoch: 009, Step: 2809, Loss: 0.9160\n",
            "Epoch: 009, Step: 2810, Loss: 1.0530\n",
            "Epoch: 009, Step: 2811, Loss: 1.1797\n",
            "Epoch: 009, Step: 2812, Loss: 1.1867\n",
            "Epoch: 009, Step: 2813, Loss: 0.9580\n",
            "Epoch: 009, Step: 2814, Loss: 1.1542\n",
            "Epoch: 009, Step: 2815, Loss: 1.0912\n",
            "Epoch: 009, Step: 2816, Loss: 1.1123\n",
            "Epoch: 009, Step: 2817, Loss: 0.8883\n",
            "Epoch: 009, Step: 2818, Loss: 1.0345\n",
            "Epoch: 009, Step: 2819, Loss: 0.9391\n",
            "Epoch: 009, Step: 2820, Loss: 0.9501\n",
            "Epoch: 009, Step: 2821, Loss: 1.0067\n",
            "Epoch: 009, Step: 2822, Loss: 0.9477\n",
            "Epoch: 009, Step: 2823, Loss: 0.8523\n",
            "Epoch: 009, Step: 2824, Loss: 0.9668\n",
            "Epoch: 009, Step: 2825, Loss: 0.8921\n",
            "Epoch: 009, Step: 2826, Loss: 0.7877\n",
            "Epoch: 009, Step: 2827, Loss: 0.9148\n",
            "Epoch: 009, Step: 2828, Loss: 0.9364\n",
            "Epoch: 009, Step: 2829, Loss: 0.9639\n",
            "Epoch: 009, Step: 2830, Loss: 0.7976\n",
            "Epoch: 009, Step: 2831, Loss: 0.9042\n",
            "Epoch: 009, Step: 2832, Loss: 0.9379\n",
            "Epoch: 009, Step: 2833, Loss: 1.0536\n",
            "Epoch: 009, Step: 2834, Loss: 0.9028\n",
            "Epoch: 009, Step: 2835, Loss: 0.9533\n",
            "Epoch: 009, Step: 2836, Loss: 1.0943\n",
            "Epoch: 009, Step: 2837, Loss: 0.9447\n",
            "Epoch: 009, Step: 2838, Loss: 0.8827\n",
            "Epoch: 009, Step: 2839, Loss: 1.0584\n",
            "Epoch: 009, Step: 2840, Loss: 1.0575\n",
            "Epoch: 009, Step: 2841, Loss: 0.9015\n",
            "Epoch: 009, Step: 2842, Loss: 0.9904\n",
            "Epoch: 009, Step: 2843, Loss: 1.0177\n",
            "Epoch: 009, Step: 2844, Loss: 0.9871\n",
            "Epoch: 009, Step: 2845, Loss: 0.9496\n",
            "Epoch: 009, Step: 2846, Loss: 0.9140\n",
            "Epoch: 009, Step: 2847, Loss: 1.1215\n",
            "Epoch: 009, Step: 2848, Loss: 0.8535\n",
            "Epoch: 009, Step: 2849, Loss: 1.1101\n",
            "Epoch: 009, Step: 2850, Loss: 0.9402\n",
            "Epoch: 009, Step: 2851, Loss: 1.1489\n",
            "Epoch: 009, Step: 2852, Loss: 1.0221\n",
            "Epoch: 009, Step: 2853, Loss: 0.8819\n",
            "Epoch: 009, Step: 2854, Loss: 0.9082\n",
            "Epoch: 009, Step: 2855, Loss: 1.0275\n",
            "Epoch: 009, Step: 2856, Loss: 0.9548\n",
            "Epoch: 009, Step: 2857, Loss: 0.9628\n",
            "Epoch: 009, Step: 2858, Loss: 1.0191\n",
            "Epoch: 009, Step: 2859, Loss: 1.0018\n",
            "Epoch: 009, Step: 2860, Loss: 0.9236\n",
            "Epoch: 009, Step: 2861, Loss: 0.8556\n",
            "Epoch: 009, Step: 2862, Loss: 0.9679\n",
            "Epoch: 009, Step: 2863, Loss: 0.9197\n",
            "Epoch: 009, Step: 2864, Loss: 1.1027\n",
            "Epoch: 009, Step: 2865, Loss: 0.8740\n",
            "Epoch: 009, Step: 2866, Loss: 1.0077\n",
            "Epoch: 009, Step: 2867, Loss: 0.9591\n",
            "Epoch: 009, Step: 2868, Loss: 0.8669\n",
            "Epoch: 009, Step: 2869, Loss: 0.9972\n",
            "Epoch: 009, Step: 2870, Loss: 1.0276\n",
            "Epoch: 009, Step: 2871, Loss: 0.8434\n",
            "Epoch: 009, Step: 2872, Loss: 0.9228\n",
            "Epoch: 009, Step: 2873, Loss: 0.8696\n",
            "Epoch: 009, Step: 2874, Loss: 1.0494\n",
            "Epoch: 009, Step: 2875, Loss: 0.9823\n",
            "Epoch: 009, Step: 2876, Loss: 0.9532\n",
            "Epoch: 009, Step: 2877, Loss: 0.8592\n",
            "Epoch: 009, Step: 2878, Loss: 0.9997\n",
            "Epoch: 009, Step: 2879, Loss: 1.0853\n",
            "Epoch: 009, Step: 2880, Loss: 1.0325\n",
            "Epoch: 009, Step: 2881, Loss: 0.9023\n",
            "Epoch: 009, Step: 2882, Loss: 0.7946\n",
            "Epoch: 009, Step: 2883, Loss: 0.8034\n",
            "Epoch: 009, Step: 2884, Loss: 0.9634\n",
            "Epoch: 009, Step: 2885, Loss: 1.1252\n",
            "Epoch: 009, Step: 2886, Loss: 0.8847\n",
            "Epoch: 009, Step: 2887, Loss: 0.8246\n",
            "Epoch: 009, Step: 2888, Loss: 0.8732\n",
            "Epoch: 009, Step: 2889, Loss: 1.0013\n",
            "Epoch: 009, Step: 2890, Loss: 1.0803\n",
            "Epoch: 009, Step: 2891, Loss: 0.8965\n",
            "Epoch: 009, Step: 2892, Loss: 0.8744\n",
            "Epoch: 009, Step: 2893, Loss: 0.8526\n",
            "Epoch: 009, Step: 2894, Loss: 1.0956\n",
            "Epoch: 009, Step: 2895, Loss: 1.0344\n",
            "Epoch: 009, Step: 2896, Loss: 0.8453\n",
            "Epoch: 009, Step: 2897, Loss: 0.7406\n",
            "Epoch: 009, Step: 2898, Loss: 0.9395\n",
            "Epoch: 009, Step: 2899, Loss: 0.8448\n",
            "Epoch: 009, Step: 2900, Loss: 1.0439\n",
            "Epoch: 009, Step: 2901, Loss: 1.0299\n",
            "Epoch: 009, Step: 2902, Loss: 0.9643\n",
            "Epoch: 009, Step: 2903, Loss: 0.9070\n",
            "Epoch: 009, Step: 2904, Loss: 0.9575\n",
            "Epoch: 009, Step: 2905, Loss: 0.9079\n",
            "Epoch: 009, Step: 2906, Loss: 0.9539\n",
            "Epoch: 009, Step: 2907, Loss: 0.9718\n",
            "Epoch: 009, Step: 2908, Loss: 0.8246\n",
            "Epoch: 009, Step: 2909, Loss: 0.8804\n",
            "Epoch: 009, Step: 2910, Loss: 0.9110\n",
            "Epoch: 009, Step: 2911, Loss: 0.9621\n",
            "Epoch: 009, Step: 2912, Loss: 0.8605\n",
            "Epoch: 009, Step: 2913, Loss: 0.8399\n",
            "Epoch: 009, Step: 2914, Loss: 0.9050\n",
            "Epoch: 009, Step: 2915, Loss: 1.1148\n",
            "Epoch: 009, Step: 2916, Loss: 0.8785\n",
            "Epoch: 009, Step: 2917, Loss: 1.0438\n",
            "Epoch: 009, Step: 2918, Loss: 0.9124\n",
            "Epoch: 009, Step: 2919, Loss: 0.8483\n",
            "Epoch: 009, Step: 2920, Loss: 0.9545\n",
            "Epoch: 009, Step: 2921, Loss: 0.9834\n",
            "Epoch: 009, Step: 2922, Loss: 0.9798\n",
            "Epoch: 009, Step: 2923, Loss: 0.8813\n",
            "Epoch: 009, Step: 2924, Loss: 1.0095\n",
            "Epoch: 009, Step: 2925, Loss: 0.8427\n",
            "Epoch: 009, Step: 2926, Loss: 0.8577\n",
            "Epoch: 009, Step: 2927, Loss: 0.9276\n",
            "Epoch: 009, Step: 2928, Loss: 0.8762\n",
            "Epoch: 009, Step: 2929, Loss: 0.9577\n",
            "Epoch: 009, Step: 2930, Loss: 1.0146\n",
            "Epoch: 009, Step: 2931, Loss: 0.8893\n",
            "Epoch: 009, Step: 2932, Loss: 0.9146\n",
            "Epoch: 009, Step: 2933, Loss: 0.8652\n",
            "Epoch: 009, Step: 2934, Loss: 0.8400\n",
            "Epoch: 009, Step: 2935, Loss: 0.9640\n",
            "Epoch: 009, Step: 2936, Loss: 1.1548\n",
            "Epoch: 009, Step: 2937, Loss: 0.9467\n",
            "Epoch: 009, Step: 2938, Loss: 0.9508\n",
            "Epoch: 009, Step: 2939, Loss: 0.9585\n",
            "Epoch: 009, Step: 2940, Loss: 0.9684\n",
            "Epoch: 009, Step: 2941, Loss: 0.8807\n",
            "Epoch: 009, Step: 2942, Loss: 1.0896\n",
            "Epoch: 009, Step: 2943, Loss: 0.8838\n",
            "Epoch: 009, Step: 2944, Loss: 0.9623\n",
            "Epoch: 009, Step: 2945, Loss: 0.8685\n",
            "Epoch: 009, Step: 2946, Loss: 0.8469\n",
            "Epoch: 009, Step: 2947, Loss: 0.9671\n",
            "Epoch: 009, Step: 2948, Loss: 1.0056\n",
            "Epoch: 009, Step: 2949, Loss: 0.9142\n",
            "Epoch: 009, Step: 2950, Loss: 1.0211\n",
            "Epoch: 009, Step: 2951, Loss: 0.7123\n",
            "Epoch: 009, Step: 2952, Loss: 0.9470\n",
            "Epoch: 009, Step: 2953, Loss: 1.0376\n",
            "Epoch: 009, Step: 2954, Loss: 0.9539\n",
            "Epoch: 009, Step: 2955, Loss: 0.9767\n",
            "Epoch: 009, Step: 2956, Loss: 0.8693\n",
            "Epoch: 009, Step: 2957, Loss: 0.9751\n",
            "Epoch: 009, Step: 2958, Loss: 0.9076\n",
            "Epoch: 009, Step: 2959, Loss: 0.9460\n",
            "Epoch: 009, Step: 2960, Loss: 0.8312\n",
            "Epoch: 009, Step: 2961, Loss: 0.8969\n",
            "Epoch: 009, Step: 2962, Loss: 0.9941\n",
            "Epoch: 009, Step: 2963, Loss: 1.1612\n",
            "Epoch: 009, Step: 2964, Loss: 0.9702\n",
            "Epoch: 009, Step: 2965, Loss: 0.9531\n",
            "Epoch: 009, Step: 2966, Loss: 0.9128\n",
            "Epoch: 009, Step: 2967, Loss: 0.8526\n",
            "Epoch: 009, Step: 2968, Loss: 1.1303\n",
            "Epoch: 009, Step: 2969, Loss: 0.8296\n",
            "Epoch: 009, Step: 2970, Loss: 0.8277\n",
            "Epoch: 009, Step: 2971, Loss: 0.9818\n",
            "Epoch: 009, Step: 2972, Loss: 0.8778\n",
            "Epoch: 009, Step: 2973, Loss: 0.8555\n",
            "Epoch: 009, Step: 2974, Loss: 0.7760\n",
            "Epoch: 009, Step: 2975, Loss: 0.9685\n",
            "Epoch: 009, Step: 2976, Loss: 0.9255\n",
            "Epoch: 009, Step: 2977, Loss: 1.0250\n",
            "Epoch: 009, Step: 2978, Loss: 0.7921\n",
            "Epoch: 009, Step: 2979, Loss: 0.8803\n",
            "Epoch: 009, Step: 2980, Loss: 1.0407\n",
            "Epoch: 009, Step: 2981, Loss: 0.7644\n",
            "Epoch: 009, Step: 2982, Loss: 0.9100\n",
            "Epoch: 009, Step: 2983, Loss: 1.0531\n",
            "Epoch: 009, Step: 2984, Loss: 0.9725\n",
            "Epoch: 009, Step: 2985, Loss: 0.9047\n",
            "Epoch: 009, Step: 2986, Loss: 1.0275\n",
            "Epoch: 009, Step: 2987, Loss: 0.9257\n",
            "Epoch: 009, Step: 2988, Loss: 0.9093\n",
            "Epoch: 009, Step: 2989, Loss: 0.9592\n",
            "Epoch: 009, Step: 2990, Loss: 1.0538\n",
            "Epoch: 009, Step: 2991, Loss: 0.9317\n",
            "Epoch: 009, Step: 2992, Loss: 0.8788\n",
            "Epoch: 009, Step: 2993, Loss: 1.1116\n",
            "Epoch: 009, Step: 2994, Loss: 1.0007\n",
            "Epoch: 009, Step: 2995, Loss: 1.0454\n",
            "Epoch: 009, Step: 2996, Loss: 0.9353\n",
            "Epoch: 009, Step: 2997, Loss: 0.8978\n",
            "Epoch: 009, Step: 2998, Loss: 0.9575\n",
            "Epoch: 009, Step: 2999, Loss: 0.9356\n",
            "Epoch: 009, Step: 3000, Loss: 0.9477\n",
            "Epoch: 009, Step: 3001, Loss: 1.0344\n",
            "Epoch: 009, Step: 3002, Loss: 0.8348\n",
            "Epoch: 009, Step: 3003, Loss: 1.0177\n",
            "Epoch: 009, Step: 3004, Loss: 0.9447\n",
            "Epoch: 009, Step: 3005, Loss: 1.0838\n",
            "Epoch: 009, Step: 3006, Loss: 0.9221\n",
            "Epoch: 009, Step: 3007, Loss: 0.8308\n",
            "Epoch: 009, Step: 3008, Loss: 1.0163\n",
            "Epoch: 009, Step: 3009, Loss: 1.0450\n",
            "Epoch: 009, Step: 3010, Loss: 1.0566\n",
            "Epoch: 009, Step: 3011, Loss: 0.9858\n",
            "Epoch: 009, Step: 3012, Loss: 0.8538\n",
            "Epoch: 009, Step: 3013, Loss: 1.0719\n",
            "Epoch: 009, Step: 3014, Loss: 0.9668\n",
            "Epoch: 009, Step: 3015, Loss: 1.1500\n",
            "Epoch: 009, Step: 3016, Loss: 0.9262\n",
            "Epoch: 009, Step: 3017, Loss: 0.8937\n",
            "Epoch: 009, Step: 3018, Loss: 0.9204\n",
            "Epoch: 009, Step: 3019, Loss: 1.0891\n",
            "Epoch: 009, Step: 3020, Loss: 0.9176\n",
            "Epoch: 009, Step: 3021, Loss: 0.8809\n",
            "Epoch: 009, Step: 3022, Loss: 0.8047\n",
            "Epoch: 009, Step: 3023, Loss: 1.0866\n",
            "Epoch: 009, Step: 3024, Loss: 0.9435\n",
            "Epoch: 009, Step: 3025, Loss: 1.0448\n",
            "Epoch: 009, Step: 3026, Loss: 0.9955\n",
            "Epoch: 009, Step: 3027, Loss: 1.0302\n",
            "Epoch: 009, Step: 3028, Loss: 0.9008\n",
            "Epoch: 009, Step: 3029, Loss: 0.8427\n",
            "Epoch: 009, Step: 3030, Loss: 1.0013\n",
            "Epoch: 009, Step: 3031, Loss: 0.9402\n",
            "Epoch: 009, Step: 3032, Loss: 1.0054\n",
            "Epoch: 009, Step: 3033, Loss: 1.0398\n",
            "Epoch: 009, Step: 3034, Loss: 1.1317\n",
            "Epoch: 009, Step: 3035, Loss: 1.0201\n",
            "Epoch: 009, Step: 3036, Loss: 1.0078\n",
            "Epoch: 009, Step: 3037, Loss: 1.0585\n",
            "Epoch: 009, Step: 3038, Loss: 0.9702\n",
            "Epoch: 009, Step: 3039, Loss: 1.0455\n",
            "Epoch: 009, Step: 3040, Loss: 1.0406\n",
            "Epoch: 009, Step: 3041, Loss: 0.9176\n",
            "Epoch: 009, Step: 3042, Loss: 0.8703\n",
            "Epoch: 009, Step: 3043, Loss: 0.8628\n",
            "Epoch: 009, Step: 3044, Loss: 0.8489\n",
            "Epoch: 009, Step: 3045, Loss: 0.9746\n",
            "Epoch: 009, Step: 3046, Loss: 0.9134\n",
            "Epoch: 009, Step: 3047, Loss: 0.9032\n",
            "Epoch: 009, Step: 3048, Loss: 0.9676\n",
            "Epoch: 009, Step: 3049, Loss: 0.8839\n",
            "Epoch: 009, Step: 3050, Loss: 0.8393\n",
            "Epoch: 009, Step: 3051, Loss: 0.9095\n",
            "Epoch: 009, Step: 3052, Loss: 0.8331\n",
            "Epoch: 009, Step: 3053, Loss: 1.0737\n",
            "Epoch: 009, Step: 3054, Loss: 0.8855\n",
            "Epoch: 009, Step: 3055, Loss: 0.9740\n",
            "Epoch: 009, Step: 3056, Loss: 0.9844\n",
            "Epoch: 009, Step: 3057, Loss: 1.0391\n",
            "Epoch: 009, Step: 3058, Loss: 1.0360\n",
            "Epoch: 009, Step: 3059, Loss: 0.9622\n",
            "Epoch: 009, Step: 3060, Loss: 0.9110\n",
            "Epoch: 009, Step: 3061, Loss: 0.9770\n",
            "Epoch: 009, Step: 3062, Loss: 0.7782\n",
            "Epoch: 009, Step: 3063, Loss: 0.8814\n",
            "Epoch: 009, Step: 3064, Loss: 0.9448\n",
            "Epoch: 009, Step: 3065, Loss: 0.9564\n",
            "Epoch: 009, Step: 3066, Loss: 1.0472\n",
            "Epoch: 009, Step: 3067, Loss: 1.0083\n",
            "Epoch: 009, Step: 3068, Loss: 0.9646\n",
            "Epoch: 009, Step: 3069, Loss: 0.9807\n",
            "Epoch: 009, Step: 3070, Loss: 1.0297\n",
            "Epoch: 009, Step: 3071, Loss: 0.9512\n",
            "Epoch: 009, Step: 3072, Loss: 1.0785\n",
            "Epoch: 009, Step: 3073, Loss: 1.1050\n",
            "Epoch: 009, Step: 3074, Loss: 1.0125\n",
            "Epoch: 009, Step: 3075, Loss: 0.9964\n",
            "Epoch: 009, Step: 3076, Loss: 0.8060\n",
            "Epoch: 009, Step: 3077, Loss: 1.0855\n",
            "Epoch: 009, Step: 3078, Loss: 1.0517\n",
            "Epoch: 009, Step: 3079, Loss: 0.8503\n",
            "Epoch: 009, Step: 3080, Loss: 0.9384\n",
            "Epoch: 009, Step: 3081, Loss: 0.8498\n",
            "Epoch: 009, Step: 3082, Loss: 0.9174\n",
            "Epoch: 009, Step: 3083, Loss: 1.0748\n",
            "Epoch: 009, Step: 3084, Loss: 0.9099\n",
            "Epoch: 009, Step: 3085, Loss: 0.8509\n",
            "Epoch: 009, Step: 3086, Loss: 0.9838\n",
            "Epoch: 009, Step: 3087, Loss: 0.8237\n",
            "Epoch: 009, Step: 3088, Loss: 0.9457\n",
            "Epoch: 009, Step: 3089, Loss: 1.0734\n",
            "Epoch: 009, Step: 3090, Loss: 0.8627\n",
            "Epoch: 009, Step: 3091, Loss: 0.9436\n",
            "Epoch: 009, Step: 3092, Loss: 0.9133\n",
            "Epoch: 009, Step: 3093, Loss: 0.8895\n",
            "Epoch: 009, Step: 3094, Loss: 1.1293\n",
            "Epoch: 009, Step: 3095, Loss: 0.9238\n",
            "Epoch: 009, Step: 3096, Loss: 1.0180\n",
            "Epoch: 009, Step: 3097, Loss: 1.0439\n",
            "Epoch: 009, Step: 3098, Loss: 0.9440\n",
            "Epoch: 009, Step: 3099, Loss: 0.9414\n",
            "Epoch: 009, Step: 3100, Loss: 0.9373\n",
            "Epoch: 009, Step: 3101, Loss: 1.0648\n",
            "Epoch: 009, Step: 3102, Loss: 1.1285\n",
            "Epoch: 009, Step: 3103, Loss: 0.9060\n",
            "Epoch: 009, Step: 3104, Loss: 0.9625\n",
            "Epoch: 009, Step: 3105, Loss: 1.1047\n",
            "Epoch: 009, Step: 3106, Loss: 0.8994\n",
            "Epoch: 009, Step: 3107, Loss: 0.9622\n",
            "Epoch: 009, Step: 3108, Loss: 0.9935\n",
            "Epoch: 009, Step: 3109, Loss: 0.8800\n",
            "Epoch: 009, Step: 3110, Loss: 0.9197\n",
            "Epoch: 009, Step: 3111, Loss: 0.8970\n",
            "Epoch: 009, Step: 3112, Loss: 1.0071\n",
            "Epoch: 009, Step: 3113, Loss: 1.0175\n",
            "Epoch: 009, Step: 3114, Loss: 0.8829\n",
            "Epoch: 009, Step: 3115, Loss: 0.9508\n",
            "Epoch: 009, Step: 3116, Loss: 1.0873\n",
            "Epoch: 009, Step: 3117, Loss: 0.9508\n",
            "Epoch: 009, Step: 3118, Loss: 1.1136\n",
            "Epoch: 009, Step: 3119, Loss: 0.9247\n",
            "Epoch: 009, Step: 3120, Loss: 0.9892\n",
            "Epoch: 009, Step: 3121, Loss: 1.0157\n",
            "Epoch: 009, Step: 3122, Loss: 1.0150\n",
            "Epoch: 009, Step: 3123, Loss: 1.0393\n",
            "Epoch: 009, Step: 3124, Loss: 0.9461\n",
            "Epoch: 009, Step: 3125, Loss: 0.9361\n",
            "Epoch: 009, Step: 3126, Loss: 0.8463\n",
            "Epoch: 009, Step: 3127, Loss: 0.8995\n",
            "Epoch: 009, Step: 3128, Loss: 0.9232\n",
            "Epoch: 009, Step: 3129, Loss: 0.9552\n",
            "Epoch: 009, Step: 3130, Loss: 0.9588\n",
            "Epoch: 009, Step: 3131, Loss: 0.9466\n",
            "Epoch: 009, Step: 3132, Loss: 0.9875\n",
            "Epoch: 009, Step: 3133, Loss: 0.9965\n",
            "Epoch: 009, Step: 3134, Loss: 1.0518\n",
            "Epoch: 009, Step: 3135, Loss: 0.9996\n",
            "Epoch: 009, Step: 3136, Loss: 1.0237\n",
            "Epoch: 009, Step: 3137, Loss: 0.8953\n",
            "Epoch: 009, Step: 3138, Loss: 1.0718\n",
            "Epoch: 009, Step: 3139, Loss: 1.1233\n",
            "Epoch: 009, Step: 3140, Loss: 0.8888\n",
            "Epoch: 009, Step: 3141, Loss: 0.9925\n",
            "Epoch: 009, Step: 3142, Loss: 0.9797\n",
            "Epoch: 009, Step: 3143, Loss: 1.0662\n",
            "Epoch: 009, Step: 3144, Loss: 1.0113\n",
            "Epoch: 009, Step: 3145, Loss: 1.0219\n",
            "Epoch: 009, Step: 3146, Loss: 1.0705\n",
            "Epoch: 009, Step: 3147, Loss: 0.9203\n",
            "Epoch: 009, Step: 3148, Loss: 0.8550\n",
            "Epoch: 009, Step: 3149, Loss: 0.9275\n",
            "Epoch: 009, Step: 3150, Loss: 0.9793\n",
            "Epoch: 009, Step: 3151, Loss: 0.9418\n",
            "Epoch: 009, Step: 3152, Loss: 0.9085\n",
            "Epoch: 009, Step: 3153, Loss: 0.9273\n",
            "Epoch: 009, Step: 3154, Loss: 0.8874\n",
            "Epoch: 009, Step: 3155, Loss: 1.1365\n",
            "Epoch: 009, Step: 3156, Loss: 0.9673\n",
            "Epoch: 009, Step: 3157, Loss: 0.9436\n",
            "Epoch: 009, Step: 3158, Loss: 0.9784\n",
            "Epoch: 009, Step: 3159, Loss: 0.9453\n",
            "Epoch: 009, Step: 3160, Loss: 1.0312\n",
            "Epoch: 009, Step: 3161, Loss: 0.9683\n",
            "Epoch: 009, Step: 3162, Loss: 1.0365\n",
            "Epoch: 009, Step: 3163, Loss: 0.9315\n",
            "Epoch: 009, Step: 3164, Loss: 1.0058\n",
            "Epoch: 009, Step: 3165, Loss: 0.8563\n",
            "Epoch: 009, Step: 3166, Loss: 1.0030\n",
            "Epoch: 009, Step: 3167, Loss: 0.9565\n",
            "Epoch: 009, Step: 3168, Loss: 1.1016\n",
            "Epoch: 009, Step: 3169, Loss: 0.9533\n",
            "Epoch: 009, Step: 3170, Loss: 0.8006\n",
            "Epoch: 009, Step: 3171, Loss: 1.1433\n",
            "Epoch: 009, Step: 3172, Loss: 1.0481\n",
            "Epoch: 009, Step: 3173, Loss: 1.0092\n",
            "Epoch: 009, Step: 3174, Loss: 0.9327\n",
            "Epoch: 009, Step: 3175, Loss: 1.1085\n",
            "Epoch: 009, Step: 3176, Loss: 1.1226\n",
            "Epoch: 009, Step: 3177, Loss: 1.0497\n",
            "Epoch: 009, Step: 3178, Loss: 1.0296\n",
            "Epoch: 009, Step: 3179, Loss: 0.8222\n",
            "Epoch: 009, Step: 3180, Loss: 0.8707\n",
            "Epoch: 009, Step: 3181, Loss: 0.9661\n",
            "Epoch: 009, Step: 3182, Loss: 0.9433\n",
            "Epoch: 009, Step: 3183, Loss: 0.9786\n",
            "Epoch: 009, Step: 3184, Loss: 0.9896\n",
            "Epoch: 009, Step: 3185, Loss: 0.9667\n",
            "Epoch: 009, Step: 3186, Loss: 0.8777\n",
            "Epoch: 009, Step: 3187, Loss: 0.9928\n",
            "Epoch: 009, Step: 3188, Loss: 0.9699\n",
            "Epoch: 009, Step: 3189, Loss: 0.9225\n",
            "Epoch: 009, Step: 3190, Loss: 0.9925\n",
            "Epoch: 009, Step: 3191, Loss: 0.9857\n",
            "Epoch: 009, Step: 3192, Loss: 0.8992\n",
            "Epoch: 009, Step: 3193, Loss: 1.0729\n",
            "Epoch: 009, Step: 3194, Loss: 0.9357\n",
            "Epoch: 009, Step: 3195, Loss: 0.9740\n",
            "Epoch: 009, Step: 3196, Loss: 1.1322\n",
            "Epoch: 009, Step: 3197, Loss: 1.1088\n",
            "Epoch: 009, Step: 3198, Loss: 0.9223\n",
            "Epoch: 009, Step: 3199, Loss: 1.0327\n",
            "Epoch: 009, Step: 3200, Loss: 1.0387\n",
            "Epoch: 009, Step: 3201, Loss: 0.8159\n",
            "Epoch: 009, Step: 3202, Loss: 0.9629\n",
            "Epoch: 009, Step: 3203, Loss: 0.9987\n",
            "Epoch: 009, Step: 3204, Loss: 0.9447\n",
            "Epoch: 009, Step: 3205, Loss: 1.1187\n",
            "Epoch: 009, Step: 3206, Loss: 0.9319\n",
            "Epoch: 009, Step: 3207, Loss: 1.0345\n",
            "Epoch: 009, Step: 3208, Loss: 1.0047\n",
            "Epoch: 009, Step: 3209, Loss: 0.9127\n",
            "Epoch: 009, Step: 3210, Loss: 0.8365\n",
            "Epoch: 009, Step: 3211, Loss: 1.0193\n",
            "Epoch: 009, Step: 3212, Loss: 0.9000\n",
            "Epoch: 009, Step: 3213, Loss: 0.8902\n",
            "Epoch: 009, Step: 3214, Loss: 1.0478\n",
            "Epoch: 009, Step: 3215, Loss: 0.9468\n",
            "Epoch: 009, Step: 3216, Loss: 1.0283\n",
            "Epoch: 009, Step: 3217, Loss: 0.8538\n",
            "Epoch: 009, Step: 3218, Loss: 1.0807\n",
            "Epoch: 009, Step: 3219, Loss: 0.9851\n",
            "Epoch: 009, Step: 3220, Loss: 0.8419\n",
            "Epoch: 009, Step: 3221, Loss: 1.0323\n",
            "Epoch: 009, Step: 3222, Loss: 1.0525\n",
            "Epoch: 009, Step: 3223, Loss: 0.8045\n",
            "Epoch: 009, Step: 3224, Loss: 0.8510\n",
            "Epoch: 009, Step: 3225, Loss: 0.9251\n",
            "Epoch: 009, Step: 3226, Loss: 1.0444\n",
            "Epoch: 009, Step: 3227, Loss: 0.8064\n",
            "Epoch: 009, Step: 3228, Loss: 0.8752\n",
            "Epoch: 009, Step: 3229, Loss: 0.9833\n",
            "Epoch: 009, Step: 3230, Loss: 1.1694\n",
            "Epoch: 009, Step: 3231, Loss: 1.0846\n",
            "Epoch: 009, Step: 3232, Loss: 1.0119\n",
            "Epoch: 009, Step: 3233, Loss: 0.8576\n",
            "Epoch: 009, Step: 3234, Loss: 0.7300\n",
            "Epoch: 009, Step: 3235, Loss: 0.9842\n",
            "Epoch: 009, Step: 3236, Loss: 1.1036\n",
            "Epoch: 009, Step: 3237, Loss: 1.0633\n",
            "Epoch: 009, Step: 3238, Loss: 1.1652\n",
            "Epoch: 009, Step: 3239, Loss: 0.9521\n",
            "Epoch: 009, Step: 3240, Loss: 0.9028\n",
            "Epoch: 009, Step: 3241, Loss: 0.9618\n",
            "Epoch: 009, Step: 3242, Loss: 1.2226\n",
            "Epoch: 009, Step: 3243, Loss: 0.9801\n",
            "Epoch: 009, Step: 3244, Loss: 0.9479\n",
            "Epoch: 009, Step: 3245, Loss: 1.0209\n",
            "Epoch: 009, Step: 3246, Loss: 0.9166\n",
            "Epoch: 009, Step: 3247, Loss: 1.0918\n",
            "Epoch: 009, Step: 3248, Loss: 0.9528\n",
            "Epoch: 009, Step: 3249, Loss: 0.9532\n",
            "Epoch: 009, Step: 3250, Loss: 1.0054\n",
            "Epoch: 009, Step: 3251, Loss: 0.9544\n",
            "Epoch: 009, Step: 3252, Loss: 0.9867\n",
            "Epoch: 009, Step: 3253, Loss: 1.0481\n",
            "Epoch: 009, Step: 3254, Loss: 1.0965\n",
            "Epoch: 009, Step: 3255, Loss: 0.9083\n",
            "Epoch: 009, Step: 3256, Loss: 0.8289\n",
            "Epoch: 009, Step: 3257, Loss: 0.9520\n",
            "Epoch: 009, Step: 3258, Loss: 0.8942\n",
            "Epoch: 009, Step: 3259, Loss: 1.0410\n",
            "Epoch: 009, Step: 3260, Loss: 1.0832\n",
            "Epoch: 009, Step: 3261, Loss: 0.8957\n",
            "Epoch: 009, Step: 3262, Loss: 1.1027\n",
            "Epoch: 009, Step: 3263, Loss: 0.9303\n",
            "Epoch: 009, Step: 3264, Loss: 0.7958\n",
            "Epoch: 009, Step: 3265, Loss: 1.0059\n",
            "Epoch: 009, Step: 3266, Loss: 0.9023\n",
            "Epoch: 009, Step: 3267, Loss: 0.9095\n",
            "Epoch: 009, Step: 3268, Loss: 0.9027\n",
            "Epoch: 009, Step: 3269, Loss: 0.8538\n",
            "Epoch: 009, Step: 3270, Loss: 0.9854\n",
            "Epoch: 009, Step: 3271, Loss: 0.8726\n",
            "Epoch: 009, Step: 3272, Loss: 1.1296\n",
            "Epoch: 009, Step: 3273, Loss: 0.9184\n",
            "Epoch: 009, Step: 3274, Loss: 0.8907\n",
            "Epoch: 009, Step: 3275, Loss: 1.0660\n",
            "Epoch: 009, Step: 3276, Loss: 0.9789\n",
            "Epoch: 009, Step: 3277, Loss: 0.9626\n",
            "Epoch: 009, Step: 3278, Loss: 0.9217\n",
            "Epoch: 009, Step: 3279, Loss: 0.8503\n",
            "Epoch: 009, Step: 3280, Loss: 1.1000\n",
            "Epoch: 009, Step: 3281, Loss: 0.8857\n",
            "Epoch: 009, Step: 3282, Loss: 0.9313\n",
            "Epoch: 009, Step: 3283, Loss: 0.8576\n",
            "Epoch: 009, Step: 3284, Loss: 0.9090\n",
            "Epoch: 009, Step: 3285, Loss: 0.8718\n",
            "Epoch: 009, Step: 3286, Loss: 0.9208\n",
            "Epoch: 009, Step: 3287, Loss: 1.0285\n",
            "Epoch: 009, Step: 3288, Loss: 1.0062\n",
            "Epoch: 009, Step: 3289, Loss: 0.7687\n",
            "Epoch: 009, Step: 3290, Loss: 1.0942\n",
            "Epoch: 009, Step: 3291, Loss: 0.8374\n",
            "Epoch: 009, Step: 3292, Loss: 0.7293\n",
            "Epoch: 009, Step: 3293, Loss: 0.8907\n",
            "Epoch: 009, Step: 3294, Loss: 1.0161\n",
            "Epoch: 009, Step: 3295, Loss: 1.1120\n",
            "Epoch: 009, Step: 3296, Loss: 0.8851\n",
            "Epoch: 009, Step: 3297, Loss: 0.8122\n",
            "Epoch: 009, Step: 3298, Loss: 0.9005\n",
            "Epoch: 009, Step: 3299, Loss: 0.8976\n",
            "Epoch: 009, Step: 3300, Loss: 1.0184\n",
            "Epoch: 009, Step: 3301, Loss: 0.9020\n",
            "Epoch: 009, Step: 3302, Loss: 0.7806\n",
            "Epoch: 009, Step: 3303, Loss: 0.8485\n",
            "Epoch: 009, Step: 3304, Loss: 1.0627\n",
            "Epoch: 009, Step: 3305, Loss: 0.9295\n",
            "Epoch: 009, Step: 3306, Loss: 0.9851\n",
            "Epoch: 009, Step: 3307, Loss: 1.0708\n",
            "Epoch: 009, Step: 3308, Loss: 0.9452\n",
            "Epoch: 009, Step: 3309, Loss: 1.0260\n",
            "Epoch: 009, Step: 3310, Loss: 0.9338\n",
            "Epoch: 009, Step: 3311, Loss: 0.8493\n",
            "Epoch: 009, Step: 3312, Loss: 0.9355\n",
            "Epoch: 009, Step: 3313, Loss: 1.0919\n",
            "Epoch: 009, Step: 3314, Loss: 0.8893\n",
            "Epoch: 009, Step: 3315, Loss: 1.0435\n",
            "Epoch: 009, Step: 3316, Loss: 0.9931\n",
            "Epoch: 009, Step: 3317, Loss: 1.0159\n",
            "Epoch: 009, Step: 3318, Loss: 0.7893\n",
            "Epoch: 009, Step: 3319, Loss: 1.0486\n",
            "Epoch: 009, Step: 3320, Loss: 0.9076\n",
            "Epoch: 009, Step: 3321, Loss: 0.9182\n",
            "Epoch: 009, Step: 3322, Loss: 1.0415\n",
            "Epoch: 009, Step: 3323, Loss: 0.9074\n",
            "Epoch: 009, Step: 3324, Loss: 0.9862\n",
            "Epoch: 009, Step: 3325, Loss: 1.0175\n",
            "Epoch: 009, Step: 3326, Loss: 0.8040\n",
            "Epoch: 009, Step: 3327, Loss: 1.0909\n",
            "Epoch: 009, Step: 3328, Loss: 0.9344\n",
            "Epoch: 009, Step: 3329, Loss: 0.8716\n",
            "Epoch: 009, Step: 3330, Loss: 0.9255\n",
            "Epoch: 009, Step: 3331, Loss: 0.9648\n",
            "Epoch: 009, Step: 3332, Loss: 0.9281\n",
            "Epoch: 009, Step: 3333, Loss: 1.0506\n",
            "Epoch: 009, Step: 3334, Loss: 0.9874\n",
            "Epoch: 009, Step: 3335, Loss: 0.9894\n",
            "Epoch: 009, Step: 3336, Loss: 0.9886\n",
            "Epoch: 009, Step: 3337, Loss: 0.9427\n",
            "Epoch: 009, Step: 3338, Loss: 1.0367\n",
            "Epoch: 009, Step: 3339, Loss: 0.9699\n",
            "Epoch: 009, Step: 3340, Loss: 1.1359\n",
            "Epoch: 009, Step: 3341, Loss: 0.9717\n",
            "Epoch: 009, Step: 3342, Loss: 0.9875\n",
            "Epoch: 009, Step: 3343, Loss: 0.9540\n",
            "Epoch: 009, Step: 3344, Loss: 1.0003\n",
            "Epoch: 009, Step: 3345, Loss: 0.9781\n",
            "Epoch: 009, Step: 3346, Loss: 0.9969\n",
            "Epoch: 009, Step: 3347, Loss: 0.9531\n",
            "Epoch: 009, Step: 3348, Loss: 1.0600\n",
            "Epoch: 009, Step: 3349, Loss: 0.9768\n",
            "Epoch: 009, Step: 3350, Loss: 1.1011\n",
            "Epoch: 009, Step: 3351, Loss: 0.8221\n",
            "Epoch: 009, Step: 3352, Loss: 1.0171\n",
            "Epoch: 009, Step: 3353, Loss: 0.9391\n",
            "Epoch: 009, Step: 3354, Loss: 0.8783\n",
            "Epoch: 009, Step: 3355, Loss: 1.0242\n",
            "Epoch: 009, Step: 3356, Loss: 0.9119\n",
            "Epoch: 009, Step: 3357, Loss: 0.7867\n",
            "Epoch: 009, Step: 3358, Loss: 0.9721\n",
            "Epoch: 009, Step: 3359, Loss: 0.9705\n",
            "Epoch: 009, Step: 3360, Loss: 0.9304\n",
            "Epoch: 009, Step: 3361, Loss: 0.9290\n",
            "Epoch: 009, Step: 3362, Loss: 1.0200\n",
            "Epoch: 009, Step: 3363, Loss: 1.0780\n",
            "Epoch: 009, Step: 3364, Loss: 0.9760\n",
            "Epoch: 009, Step: 3365, Loss: 0.9785\n",
            "Epoch: 009, Step: 3366, Loss: 0.9994\n",
            "Epoch: 009, Step: 3367, Loss: 0.9795\n",
            "Epoch: 009, Step: 3368, Loss: 0.7973\n",
            "Epoch: 009, Step: 3369, Loss: 0.9260\n",
            "Epoch: 009, Step: 3370, Loss: 0.9214\n",
            "Epoch: 009, Step: 3371, Loss: 1.0470\n",
            "Epoch: 009, Step: 3372, Loss: 1.0593\n",
            "Epoch: 009, Step: 3373, Loss: 0.9974\n",
            "Epoch: 009, Step: 3374, Loss: 0.9441\n",
            "Epoch: 009, Step: 3375, Loss: 0.9007\n",
            "Epoch: 009, Step: 3376, Loss: 1.0873\n",
            "Epoch: 009, Step: 3377, Loss: 0.9012\n",
            "Epoch: 009, Step: 3378, Loss: 0.8193\n",
            "Epoch: 009, Step: 3379, Loss: 1.0004\n",
            "Epoch: 009, Step: 3380, Loss: 1.0477\n",
            "Epoch: 009, Step: 3381, Loss: 0.9562\n",
            "Epoch: 009, Step: 3382, Loss: 0.9142\n",
            "Epoch: 009, Step: 3383, Loss: 0.8822\n",
            "Epoch: 009, Step: 3384, Loss: 1.0221\n",
            "Epoch: 009, Step: 3385, Loss: 0.8739\n",
            "Epoch: 009, Step: 3386, Loss: 0.8937\n",
            "Epoch: 009, Step: 3387, Loss: 0.9555\n",
            "Epoch: 009, Step: 3388, Loss: 0.9435\n",
            "Epoch: 009, Step: 3389, Loss: 1.1127\n",
            "Epoch: 009, Step: 3390, Loss: 1.1258\n",
            "Epoch: 009, Step: 3391, Loss: 0.9010\n",
            "Epoch: 009, Step: 3392, Loss: 0.9249\n",
            "Epoch: 009, Step: 3393, Loss: 1.0909\n",
            "Epoch: 009, Step: 3394, Loss: 0.9045\n",
            "Epoch: 009, Step: 3395, Loss: 0.9193\n",
            "Epoch: 009, Step: 3396, Loss: 0.7812\n",
            "Epoch: 009, Step: 3397, Loss: 1.0073\n",
            "Epoch: 009, Step: 3398, Loss: 0.9882\n",
            "Epoch: 009, Step: 3399, Loss: 0.9703\n",
            "Epoch: 009, Step: 3400, Loss: 0.9344\n",
            "Epoch: 009, Step: 3401, Loss: 0.8610\n",
            "Epoch: 009, Step: 3402, Loss: 1.0311\n",
            "Epoch: 009, Step: 3403, Loss: 1.0734\n",
            "Epoch: 009, Step: 3404, Loss: 1.0374\n",
            "Epoch: 009, Step: 3405, Loss: 0.9476\n",
            "Epoch: 009, Step: 3406, Loss: 0.8689\n",
            "Epoch: 009, Step: 3407, Loss: 1.0908\n",
            "Epoch: 009, Step: 3408, Loss: 0.8580\n",
            "Epoch: 009, Step: 3409, Loss: 0.9847\n",
            "Epoch: 009, Step: 3410, Loss: 0.8107\n",
            "Epoch: 009, Step: 3411, Loss: 1.0034\n",
            "Epoch: 009, Step: 3412, Loss: 1.0526\n",
            "Epoch: 009, Step: 3413, Loss: 0.7560\n",
            "Epoch: 009, Step: 3414, Loss: 0.8718\n",
            "Epoch: 009, Step: 3415, Loss: 0.8609\n",
            "Epoch: 009, Step: 3416, Loss: 0.9090\n",
            "Epoch: 009, Step: 3417, Loss: 0.8608\n",
            "Epoch: 009, Step: 3418, Loss: 0.8510\n",
            "Epoch: 009, Step: 3419, Loss: 0.7759\n",
            "Epoch: 009, Step: 3420, Loss: 1.0627\n",
            "Epoch: 009, Step: 3421, Loss: 1.0608\n",
            "Epoch: 009, Step: 3422, Loss: 0.9154\n",
            "Epoch: 009, Step: 3423, Loss: 0.9055\n",
            "Epoch: 009, Step: 3424, Loss: 0.9883\n",
            "Epoch: 009, Step: 3425, Loss: 1.0214\n",
            "Epoch: 009, Step: 3426, Loss: 1.1048\n",
            "Epoch: 009, Step: 3427, Loss: 0.9941\n",
            "Epoch: 009, Step: 3428, Loss: 0.9031\n",
            "Epoch: 009, Step: 3429, Loss: 0.9332\n",
            "Epoch: 009, Step: 3430, Loss: 0.8873\n",
            "Epoch: 009, Step: 3431, Loss: 1.0575\n",
            "Epoch: 009, Step: 3432, Loss: 0.8097\n",
            "Epoch: 009, Step: 3433, Loss: 1.0214\n",
            "Epoch: 009, Step: 3434, Loss: 1.0002\n",
            "Epoch: 009, Step: 3435, Loss: 1.0207\n",
            "Epoch: 009, Step: 3436, Loss: 1.0676\n",
            "Epoch: 009, Step: 3437, Loss: 0.8327\n",
            "Epoch: 009, Step: 3438, Loss: 0.9433\n",
            "Epoch: 009, Step: 3439, Loss: 0.9978\n",
            "Epoch: 009, Step: 3440, Loss: 1.0650\n",
            "Epoch: 009, Step: 3441, Loss: 1.0040\n",
            "Epoch: 009, Step: 3442, Loss: 0.8305\n",
            "Epoch: 009, Step: 3443, Loss: 1.1980\n",
            "Epoch: 009, Step: 3444, Loss: 0.9076\n",
            "Epoch: 009, Step: 3445, Loss: 1.1001\n",
            "Epoch: 009, Step: 3446, Loss: 1.0202\n",
            "Epoch: 009, Step: 3447, Loss: 1.0163\n",
            "Epoch: 009, Step: 3448, Loss: 1.0080\n",
            "Epoch: 009, Step: 3449, Loss: 0.9121\n",
            "Epoch: 009, Step: 3450, Loss: 0.8387\n",
            "Epoch: 009, Step: 3451, Loss: 0.8669\n",
            "Epoch: 009, Step: 3452, Loss: 0.9578\n",
            "Epoch: 009, Step: 3453, Loss: 0.9796\n",
            "Epoch: 009, Step: 3454, Loss: 0.9910\n",
            "Epoch: 009, Step: 3455, Loss: 0.9075\n",
            "Epoch: 009, Step: 3456, Loss: 0.8528\n",
            "Epoch: 009, Step: 3457, Loss: 0.8955\n",
            "Epoch: 009, Step: 3458, Loss: 0.8149\n",
            "Epoch: 009, Step: 3459, Loss: 0.9569\n",
            "Epoch: 009, Step: 3460, Loss: 0.8412\n",
            "Epoch: 009, Step: 3461, Loss: 1.0085\n",
            "Epoch: 009, Step: 3462, Loss: 0.9830\n",
            "Epoch: 009, Step: 3463, Loss: 0.9673\n",
            "Epoch: 009, Step: 3464, Loss: 0.9717\n",
            "Epoch: 009, Step: 3465, Loss: 1.0804\n",
            "Epoch: 009, Step: 3466, Loss: 0.9651\n",
            "Epoch: 009, Step: 3467, Loss: 1.0182\n",
            "Epoch: 009, Step: 3468, Loss: 0.9273\n",
            "Epoch: 009, Step: 3469, Loss: 0.8247\n",
            "Epoch: 009, Step: 3470, Loss: 1.0541\n",
            "Epoch: 009, Step: 3471, Loss: 0.8889\n",
            "Epoch: 009, Step: 3472, Loss: 0.9252\n",
            "Epoch: 009, Step: 3473, Loss: 0.8894\n",
            "Epoch: 009, Step: 3474, Loss: 0.9748\n",
            "Epoch: 009, Step: 3475, Loss: 0.8871\n",
            "Epoch: 009, Step: 3476, Loss: 0.8588\n",
            "Epoch: 009, Step: 3477, Loss: 0.9940\n",
            "Epoch: 009, Step: 3478, Loss: 0.9747\n",
            "Epoch: 009, Step: 3479, Loss: 0.8831\n",
            "Epoch: 009, Step: 3480, Loss: 0.9334\n",
            "Epoch: 009, Step: 3481, Loss: 0.9724\n",
            "Epoch: 009, Step: 3482, Loss: 0.9892\n",
            "Epoch: 009, Step: 3483, Loss: 0.9480\n",
            "Epoch: 009, Step: 3484, Loss: 0.8896\n",
            "Epoch: 009, Step: 3485, Loss: 0.9069\n",
            "Epoch: 009, Step: 3486, Loss: 0.9867\n",
            "Epoch: 009, Step: 3487, Loss: 0.8251\n",
            "Epoch: 009, Step: 3488, Loss: 0.7932\n",
            "Epoch: 009, Step: 3489, Loss: 0.9965\n",
            "Epoch: 009, Step: 3490, Loss: 1.0508\n",
            "Epoch: 009, Step: 3491, Loss: 1.0045\n",
            "Epoch: 009, Step: 3492, Loss: 0.8155\n",
            "Epoch: 009, Step: 3493, Loss: 0.9410\n",
            "Epoch: 009, Step: 3494, Loss: 0.8937\n",
            "Epoch: 009, Step: 3495, Loss: 0.9578\n",
            "Epoch: 009, Step: 3496, Loss: 0.8324\n",
            "Epoch: 009, Step: 3497, Loss: 1.0100\n",
            "Epoch: 009, Step: 3498, Loss: 0.8575\n",
            "Epoch: 009, Step: 3499, Loss: 0.8890\n",
            "Epoch: 009, Step: 3500, Loss: 0.9183\n",
            "Epoch: 009, Step: 3501, Loss: 0.9021\n",
            "Epoch: 009, Step: 3502, Loss: 0.8807\n",
            "Epoch: 009, Step: 3503, Loss: 0.9885\n",
            "Epoch: 009, Step: 3504, Loss: 0.8936\n",
            "Epoch: 009, Step: 3505, Loss: 0.8866\n",
            "Epoch: 009, Step: 3506, Loss: 0.8931\n",
            "Epoch: 009, Step: 3507, Loss: 1.0317\n",
            "Epoch: 009, Step: 3508, Loss: 1.0465\n",
            "Epoch: 009, Step: 3509, Loss: 1.0905\n",
            "Epoch: 009, Step: 3510, Loss: 1.0622\n",
            "Epoch: 009, Step: 3511, Loss: 0.8433\n",
            "Epoch: 009, Step: 3512, Loss: 1.0335\n",
            "Epoch: 009, Step: 3513, Loss: 0.9449\n",
            "Epoch: 009, Step: 3514, Loss: 0.9304\n",
            "Epoch: 009, Step: 3515, Loss: 0.8579\n",
            "Epoch: 009, Step: 3516, Loss: 0.9612\n",
            "Epoch: 009, Step: 3517, Loss: 1.0122\n",
            "Epoch: 009, Step: 3518, Loss: 1.0347\n",
            "Epoch: 009, Step: 3519, Loss: 0.8498\n",
            "Epoch: 009, Step: 3520, Loss: 1.0439\n",
            "Epoch: 009, Step: 3521, Loss: 0.9520\n",
            "Epoch: 009, Step: 3522, Loss: 1.0138\n",
            "Epoch: 009, Step: 3523, Loss: 0.9288\n",
            "Epoch: 009, Step: 3524, Loss: 0.8844\n",
            "Epoch: 009, Step: 3525, Loss: 1.1397\n",
            "Epoch: 009, Step: 3526, Loss: 0.8813\n",
            "Epoch: 009, Step: 3527, Loss: 1.0426\n",
            "Epoch: 009, Step: 3528, Loss: 1.0140\n",
            "Epoch: 009, Step: 3529, Loss: 0.8516\n",
            "Epoch: 009, Step: 3530, Loss: 0.9761\n",
            "Epoch: 009, Step: 3531, Loss: 0.9913\n",
            "Epoch: 009, Step: 3532, Loss: 0.9389\n",
            "Epoch: 009, Step: 3533, Loss: 0.9565\n",
            "Epoch: 009, Step: 3534, Loss: 0.8895\n",
            "Epoch: 009, Step: 3535, Loss: 0.9949\n",
            "Epoch: 009, Step: 3536, Loss: 1.0963\n",
            "Epoch: 009, Step: 3537, Loss: 0.8758\n",
            "Epoch: 009, Step: 3538, Loss: 1.1063\n",
            "Epoch: 009, Step: 3539, Loss: 0.8231\n",
            "Epoch: 009, Step: 3540, Loss: 1.0389\n",
            "Epoch: 009, Step: 3541, Loss: 0.9344\n",
            "Epoch: 009, Step: 3542, Loss: 0.8850\n",
            "Epoch: 009, Step: 3543, Loss: 0.9197\n",
            "Epoch: 009, Step: 3544, Loss: 1.0119\n",
            "Epoch: 009, Step: 3545, Loss: 1.1671\n",
            "Epoch: 009, Step: 3546, Loss: 0.8288\n",
            "Epoch: 009, Step: 3547, Loss: 0.9730\n",
            "Epoch: 009, Step: 3548, Loss: 1.1859\n",
            "Epoch: 009, Step: 3549, Loss: 0.8802\n",
            "Epoch: 009, Step: 3550, Loss: 0.8923\n",
            "Epoch: 009, Step: 3551, Loss: 0.9636\n",
            "Epoch: 009, Step: 3552, Loss: 0.9936\n",
            "Epoch: 009, Step: 3553, Loss: 1.0461\n",
            "Epoch: 009, Step: 3554, Loss: 1.0550\n",
            "Epoch: 009, Step: 3555, Loss: 0.9145\n",
            "Epoch: 009, Step: 3556, Loss: 1.0754\n",
            "Epoch: 009, Step: 3557, Loss: 1.0451\n",
            "Epoch: 009, Step: 3558, Loss: 0.8884\n",
            "Epoch: 009, Step: 3559, Loss: 0.9240\n",
            "Epoch: 009, Step: 3560, Loss: 0.8816\n",
            "Epoch: 009, Step: 3561, Loss: 1.0237\n",
            "Epoch: 009, Step: 3562, Loss: 0.9824\n",
            "Epoch: 009, Step: 3563, Loss: 0.9481\n",
            "Epoch: 009, Step: 3564, Loss: 0.9125\n",
            "Epoch: 009, Step: 3565, Loss: 0.8746\n",
            "Epoch: 009, Step: 3566, Loss: 0.8648\n",
            "Epoch: 009, Step: 3567, Loss: 1.1580\n",
            "Epoch: 009, Step: 3568, Loss: 1.2994\n",
            "Epoch: 009, Step: 3569, Loss: 0.9013\n",
            "Epoch: 009, Step: 3570, Loss: 0.9286\n",
            "Epoch: 009, Step: 3571, Loss: 0.9114\n",
            "Epoch: 009, Step: 3572, Loss: 1.1226\n",
            "Epoch: 009, Step: 3573, Loss: 0.9208\n",
            "Epoch: 009, Step: 3574, Loss: 1.0225\n",
            "Epoch: 009, Step: 3575, Loss: 1.0041\n",
            "Epoch: 009, Step: 3576, Loss: 0.9281\n",
            "Epoch: 009, Step: 3577, Loss: 0.9844\n",
            "Epoch: 009, Step: 3578, Loss: 0.9390\n",
            "Epoch: 009, Step: 3579, Loss: 0.8328\n",
            "Epoch: 009, Step: 3580, Loss: 0.9509\n",
            "Epoch: 009, Step: 3581, Loss: 0.9791\n",
            "Epoch: 009, Step: 3582, Loss: 1.0915\n",
            "Epoch: 009, Step: 3583, Loss: 0.9615\n",
            "Epoch: 009, Step: 3584, Loss: 1.0401\n",
            "Epoch: 009, Step: 3585, Loss: 0.8685\n",
            "Epoch: 009, Step: 3586, Loss: 0.7999\n",
            "Epoch: 009, Step: 3587, Loss: 0.9940\n",
            "Epoch: 009, Step: 3588, Loss: 1.1098\n",
            "Epoch: 009, Step: 3589, Loss: 0.8106\n",
            "Epoch: 009, Step: 3590, Loss: 0.8245\n",
            "Epoch: 009, Step: 3591, Loss: 0.8472\n",
            "Epoch: 009, Step: 3592, Loss: 1.0209\n",
            "Epoch: 009, Step: 3593, Loss: 1.0329\n",
            "Epoch: 009, Step: 3594, Loss: 1.0834\n",
            "Epoch: 009, Step: 3595, Loss: 1.0806\n",
            "Epoch: 009, Step: 3596, Loss: 0.8932\n",
            "Epoch: 009, Step: 3597, Loss: 0.9754\n",
            "Epoch: 009, Step: 3598, Loss: 0.8805\n",
            "Epoch: 009, Step: 3599, Loss: 0.9471\n",
            "Epoch: 009, Step: 3600, Loss: 0.9233\n",
            "Epoch: 009, Step: 3601, Loss: 0.8069\n",
            "Epoch: 009, Step: 3602, Loss: 0.9169\n",
            "Epoch: 009, Step: 3603, Loss: 0.7454\n",
            "Epoch: 009, Step: 3604, Loss: 1.0278\n",
            "Epoch: 009, Step: 3605, Loss: 0.8910\n",
            "Epoch: 009, Step: 3606, Loss: 1.0219\n",
            "Epoch: 009, Step: 3607, Loss: 0.9539\n",
            "Epoch: 009, Step: 3608, Loss: 0.8974\n",
            "Epoch: 009, Step: 3609, Loss: 0.9814\n",
            "Epoch: 009, Step: 3610, Loss: 0.7417\n",
            "Epoch: 009, Step: 3611, Loss: 0.9845\n",
            "Epoch: 009, Step: 3612, Loss: 0.9956\n",
            "Epoch: 009, Step: 3613, Loss: 1.0177\n",
            "Epoch: 009, Step: 3614, Loss: 1.0400\n",
            "Epoch: 009, Step: 3615, Loss: 0.8959\n",
            "Epoch: 009, Step: 3616, Loss: 0.8522\n",
            "Epoch: 009, Step: 3617, Loss: 0.8573\n",
            "Epoch: 009, Step: 3618, Loss: 0.9261\n",
            "Epoch: 009, Step: 3619, Loss: 0.9894\n",
            "Epoch: 009, Step: 3620, Loss: 1.2162\n",
            "Epoch: 009, Step: 3621, Loss: 1.1185\n",
            "Epoch: 009, Step: 3622, Loss: 0.8967\n",
            "Epoch: 009, Step: 3623, Loss: 0.9953\n",
            "Epoch: 009, Step: 3624, Loss: 1.0999\n",
            "Epoch: 009, Step: 3625, Loss: 1.0640\n",
            "Epoch: 009, Step: 3626, Loss: 0.9319\n",
            "Epoch: 009, Step: 3627, Loss: 0.8698\n",
            "Epoch: 009, Step: 3628, Loss: 0.9798\n",
            "Epoch: 009, Step: 3629, Loss: 0.8784\n",
            "Epoch: 009, Step: 3630, Loss: 0.9209\n",
            "Epoch: 009, Step: 3631, Loss: 0.8912\n",
            "Epoch: 009, Step: 3632, Loss: 0.8959\n",
            "Epoch: 009, Step: 3633, Loss: 0.9153\n",
            "Epoch: 009, Step: 3634, Loss: 0.9400\n",
            "Epoch: 009, Step: 3635, Loss: 0.8307\n",
            "Epoch: 009, Step: 3636, Loss: 0.9749\n",
            "Epoch: 009, Step: 3637, Loss: 1.0766\n",
            "Epoch: 009, Step: 3638, Loss: 1.0517\n",
            "Epoch: 009, Step: 3639, Loss: 0.9099\n",
            "Epoch: 009, Step: 3640, Loss: 1.0822\n",
            "Epoch: 009, Step: 3641, Loss: 0.8572\n",
            "Epoch: 009, Step: 3642, Loss: 0.8182\n",
            "Epoch: 009, Step: 3643, Loss: 0.8909\n",
            "Epoch: 009, Step: 3644, Loss: 0.9599\n",
            "Epoch: 009, Step: 3645, Loss: 0.9283\n",
            "Epoch: 009, Step: 3646, Loss: 0.8461\n",
            "Epoch: 009, Step: 3647, Loss: 0.9591\n",
            "Epoch: 009, Step: 3648, Loss: 0.8685\n",
            "Epoch: 009, Step: 3649, Loss: 0.9528\n",
            "Epoch: 009, Step: 3650, Loss: 0.8036\n",
            "Epoch: 009, Step: 3651, Loss: 0.7924\n",
            "Epoch: 009, Step: 3652, Loss: 0.8594\n",
            "Epoch: 009, Step: 3653, Loss: 1.0666\n",
            "Epoch: 009, Step: 3654, Loss: 0.9031\n",
            "Epoch: 009, Step: 3655, Loss: 0.8302\n",
            "Epoch: 009, Step: 3656, Loss: 0.9182\n",
            "Epoch: 009, Step: 3657, Loss: 0.9414\n",
            "Epoch: 009, Step: 3658, Loss: 0.8014\n",
            "Epoch: 009, Step: 3659, Loss: 0.8255\n",
            "Epoch: 009, Step: 3660, Loss: 0.9708\n",
            "Epoch: 009, Step: 3661, Loss: 1.1339\n",
            "Epoch: 009, Step: 3662, Loss: 0.9316\n",
            "Epoch: 009, Step: 3663, Loss: 0.5346\n",
            "Epoch: 009, Step: 000, Val Loss: 0.9970\n",
            "Epoch: 009, Step: 001, Val Loss: 1.0279\n",
            "Epoch: 009, Step: 002, Val Loss: 1.0836\n",
            "Epoch: 009, Step: 003, Val Loss: 1.1251\n",
            "Epoch: 009, Step: 004, Val Loss: 1.0702\n",
            "Epoch: 009, Step: 005, Val Loss: 0.9519\n",
            "Epoch: 009, Step: 006, Val Loss: 1.0989\n",
            "Epoch: 009, Step: 007, Val Loss: 1.1486\n",
            "Epoch: 009, Step: 008, Val Loss: 1.0000\n",
            "Epoch: 009, Step: 009, Val Loss: 1.0333\n",
            "Epoch: 009, Step: 010, Val Loss: 1.1661\n",
            "Epoch: 009, Step: 011, Val Loss: 1.1446\n",
            "Epoch: 009, Step: 012, Val Loss: 1.1739\n",
            "Epoch: 009, Step: 013, Val Loss: 1.0392\n",
            "Epoch: 009, Step: 014, Val Loss: 1.2164\n",
            "Epoch: 009, Step: 015, Val Loss: 1.2245\n",
            "Epoch: 009, Step: 016, Val Loss: 1.1131\n",
            "Epoch: 009, Step: 017, Val Loss: 1.1345\n",
            "Epoch: 009, Step: 018, Val Loss: 1.2397\n",
            "Epoch: 009, Step: 019, Val Loss: 1.2687\n",
            "Epoch: 009, Step: 020, Val Loss: 1.0359\n",
            "Epoch: 009, Step: 021, Val Loss: 1.0920\n",
            "Epoch: 009, Step: 022, Val Loss: 1.1126\n",
            "Epoch: 009, Step: 023, Val Loss: 1.1386\n",
            "Epoch: 009, Step: 024, Val Loss: 0.9787\n",
            "Epoch: 009, Step: 025, Val Loss: 1.0690\n",
            "Epoch: 009, Step: 026, Val Loss: 1.0352\n",
            "Epoch: 009, Step: 027, Val Loss: 1.1327\n",
            "Epoch: 009, Step: 028, Val Loss: 1.0790\n",
            "Epoch: 009, Step: 029, Val Loss: 1.0535\n",
            "Epoch: 009, Step: 030, Val Loss: 1.0724\n",
            "Epoch: 009, Step: 031, Val Loss: 1.0955\n",
            "Epoch: 009, Step: 032, Val Loss: 1.0447\n",
            "Epoch: 009, Step: 033, Val Loss: 1.1929\n",
            "Epoch: 009, Step: 034, Val Loss: 0.9072\n",
            "Epoch: 009, Step: 035, Val Loss: 0.9494\n",
            "Epoch: 009, Step: 036, Val Loss: 0.9666\n",
            "Epoch: 009, Step: 037, Val Loss: 1.0967\n",
            "Epoch: 009, Step: 038, Val Loss: 1.1417\n",
            "Epoch: 009, Step: 039, Val Loss: 1.1262\n",
            "Epoch: 009, Step: 040, Val Loss: 1.0226\n",
            "Epoch: 009, Step: 041, Val Loss: 1.0471\n",
            "Epoch: 009, Step: 042, Val Loss: 0.9989\n",
            "Epoch: 009, Step: 043, Val Loss: 0.9911\n",
            "Epoch: 009, Step: 044, Val Loss: 1.2811\n",
            "Epoch: 009, Step: 045, Val Loss: 1.1118\n",
            "Epoch: 009, Step: 046, Val Loss: 1.2285\n",
            "Epoch: 009, Step: 047, Val Loss: 1.1460\n",
            "Epoch: 009, Step: 048, Val Loss: 1.0038\n",
            "Epoch: 009, Step: 049, Val Loss: 1.0486\n",
            "Epoch: 009, Step: 050, Val Loss: 0.9670\n",
            "Epoch: 009, Step: 051, Val Loss: 1.0140\n",
            "Epoch: 009, Step: 052, Val Loss: 1.0100\n",
            "Epoch: 009, Step: 053, Val Loss: 1.2031\n",
            "Epoch: 009, Step: 054, Val Loss: 1.0209\n",
            "Epoch: 009, Step: 055, Val Loss: 1.2915\n",
            "Epoch: 009, Step: 056, Val Loss: 1.0780\n",
            "Epoch: 009, Step: 057, Val Loss: 1.0884\n",
            "Epoch: 009, Step: 058, Val Loss: 0.9493\n",
            "Epoch: 009, Step: 059, Val Loss: 1.0426\n",
            "Epoch: 009, Step: 060, Val Loss: 1.1931\n",
            "Epoch: 009, Step: 061, Val Loss: 1.0638\n",
            "Epoch: 009, Step: 062, Val Loss: 1.1317\n",
            "Epoch: 009, Step: 063, Val Loss: 1.0754\n",
            "Epoch: 009, Step: 064, Val Loss: 0.8954\n",
            "Epoch: 009, Step: 065, Val Loss: 1.1167\n",
            "Epoch: 009, Step: 066, Val Loss: 1.0407\n",
            "Epoch: 009, Step: 067, Val Loss: 1.1085\n",
            "Epoch: 009, Step: 068, Val Loss: 1.0963\n",
            "Epoch: 009, Step: 069, Val Loss: 1.1215\n",
            "Epoch: 009, Step: 070, Val Loss: 1.0579\n",
            "Epoch: 009, Step: 071, Val Loss: 1.0394\n",
            "Epoch: 009, Step: 072, Val Loss: 1.0164\n",
            "Epoch: 009, Step: 073, Val Loss: 1.1376\n",
            "Epoch: 009, Step: 074, Val Loss: 1.1027\n",
            "Epoch: 009, Step: 075, Val Loss: 1.0818\n",
            "Epoch: 009, Step: 076, Val Loss: 1.0533\n",
            "Epoch: 009, Step: 077, Val Loss: 0.9518\n",
            "Epoch: 009, Step: 078, Val Loss: 1.1062\n",
            "Epoch: 009, Step: 079, Val Loss: 0.9815\n",
            "Epoch: 009, Step: 080, Val Loss: 0.9482\n",
            "Epoch: 009, Step: 081, Val Loss: 1.0789\n",
            "Epoch: 009, Step: 082, Val Loss: 0.8965\n",
            "Epoch: 009, Step: 083, Val Loss: 1.0906\n",
            "Epoch: 009, Step: 084, Val Loss: 0.9755\n",
            "Epoch: 009, Step: 085, Val Loss: 1.1566\n",
            "Epoch: 009, Step: 086, Val Loss: 1.0529\n",
            "Epoch: 009, Step: 087, Val Loss: 1.0403\n",
            "Epoch: 009, Step: 088, Val Loss: 1.0719\n",
            "Epoch: 009, Step: 089, Val Loss: 1.2392\n",
            "Epoch: 009, Step: 090, Val Loss: 1.0696\n",
            "Epoch: 009, Step: 091, Val Loss: 1.0875\n",
            "Epoch: 009, Step: 092, Val Loss: 1.1048\n",
            "Epoch: 009, Step: 093, Val Loss: 1.1070\n",
            "Epoch: 009, Step: 094, Val Loss: 0.9078\n",
            "Epoch: 009, Step: 095, Val Loss: 1.0764\n",
            "Epoch: 009, Step: 096, Val Loss: 1.0612\n",
            "Epoch: 009, Step: 097, Val Loss: 1.0712\n",
            "Epoch: 009, Step: 098, Val Loss: 1.0471\n",
            "Epoch: 009, Step: 099, Val Loss: 1.0377\n",
            "Epoch: 009, Step: 100, Val Loss: 1.0061\n",
            "Epoch: 009, Step: 101, Val Loss: 1.1117\n",
            "Epoch: 009, Step: 102, Val Loss: 1.1498\n",
            "Epoch: 009, Step: 103, Val Loss: 0.9288\n",
            "Epoch: 009, Step: 104, Val Loss: 1.0760\n",
            "Epoch: 009, Step: 105, Val Loss: 1.1150\n",
            "Epoch: 009, Step: 106, Val Loss: 0.9823\n",
            "Epoch: 009, Step: 107, Val Loss: 1.0130\n",
            "Epoch: 009, Step: 108, Val Loss: 1.2079\n",
            "Epoch: 009, Step: 109, Val Loss: 1.0043\n",
            "Epoch: 009, Step: 110, Val Loss: 1.1836\n",
            "Epoch: 009, Step: 111, Val Loss: 1.0829\n",
            "Epoch: 009, Step: 112, Val Loss: 0.9726\n",
            "Epoch: 009, Step: 113, Val Loss: 1.0817\n",
            "Epoch: 009, Step: 114, Val Loss: 1.2872\n",
            "Epoch: 009, Step: 115, Val Loss: 1.0259\n",
            "Epoch: 009, Step: 116, Val Loss: 1.2223\n",
            "Epoch: 009, Step: 117, Val Loss: 1.0539\n",
            "Epoch: 009, Step: 118, Val Loss: 1.1908\n",
            "Epoch: 009, Step: 119, Val Loss: 1.0665\n",
            "Epoch: 009, Step: 120, Val Loss: 1.1508\n",
            "Epoch: 009, Step: 121, Val Loss: 1.0243\n",
            "Epoch: 009, Step: 122, Val Loss: 0.9705\n",
            "Epoch: 009, Step: 123, Val Loss: 0.8761\n",
            "Epoch: 009, Step: 124, Val Loss: 1.0754\n",
            "Epoch: 009, Step: 125, Val Loss: 1.1294\n",
            "Epoch: 009, Step: 126, Val Loss: 0.9775\n",
            "Epoch: 009, Step: 127, Val Loss: 0.9900\n",
            "Epoch: 009, Step: 128, Val Loss: 0.9729\n",
            "Epoch: 009, Step: 129, Val Loss: 1.0646\n",
            "Epoch: 009, Step: 130, Val Loss: 0.9436\n",
            "Epoch: 009, Step: 131, Val Loss: 0.9326\n",
            "Epoch: 009, Step: 132, Val Loss: 1.0510\n",
            "Epoch: 009, Step: 133, Val Loss: 0.9817\n",
            "Epoch: 009, Step: 134, Val Loss: 1.0043\n",
            "Epoch: 009, Step: 135, Val Loss: 1.0884\n",
            "Epoch: 009, Step: 136, Val Loss: 1.2513\n",
            "Epoch: 009, Step: 137, Val Loss: 0.9220\n",
            "Epoch: 009, Step: 138, Val Loss: 0.9882\n",
            "Epoch: 009, Step: 139, Val Loss: 1.0085\n",
            "Epoch: 009, Step: 140, Val Loss: 1.0818\n",
            "Epoch: 009, Step: 141, Val Loss: 0.9624\n",
            "Epoch: 009, Step: 142, Val Loss: 1.1513\n",
            "Epoch: 009, Step: 143, Val Loss: 1.0595\n",
            "Epoch: 009, Step: 144, Val Loss: 0.9611\n",
            "Epoch: 009, Step: 145, Val Loss: 1.0237\n",
            "Epoch: 009, Step: 146, Val Loss: 1.1484\n",
            "Epoch: 009, Step: 147, Val Loss: 1.0408\n",
            "Epoch: 009, Step: 148, Val Loss: 1.0135\n",
            "Epoch: 009, Step: 149, Val Loss: 1.0278\n",
            "Epoch: 009, Step: 150, Val Loss: 1.0454\n",
            "Epoch: 009, Step: 151, Val Loss: 0.9040\n",
            "Epoch: 009, Step: 152, Val Loss: 1.0463\n",
            "Epoch: 009, Step: 153, Val Loss: 1.1199\n",
            "Epoch: 009, Step: 154, Val Loss: 0.9492\n",
            "Epoch: 009, Step: 155, Val Loss: 1.1953\n",
            "Epoch: 009, Step: 156, Val Loss: 1.0824\n",
            "Epoch: 009, Step: 157, Val Loss: 1.1088\n",
            "Epoch: 009, Step: 158, Val Loss: 1.0086\n",
            "Epoch: 009, Step: 159, Val Loss: 1.0205\n",
            "Epoch: 009, Step: 160, Val Loss: 1.0228\n",
            "Epoch: 009, Step: 161, Val Loss: 1.1831\n",
            "Epoch: 009, Step: 162, Val Loss: 0.9873\n",
            "Epoch: 009, Step: 163, Val Loss: 0.9012\n",
            "Epoch: 009, Step: 164, Val Loss: 1.0032\n",
            "Epoch: 009, Step: 165, Val Loss: 1.1290\n",
            "Epoch: 009, Step: 166, Val Loss: 1.1211\n",
            "Epoch: 009, Step: 167, Val Loss: 1.1281\n",
            "Epoch: 009, Step: 168, Val Loss: 1.0919\n",
            "Epoch: 009, Step: 169, Val Loss: 0.9970\n",
            "Epoch: 009, Step: 170, Val Loss: 1.1058\n",
            "Epoch: 009, Step: 171, Val Loss: 1.1265\n",
            "Epoch: 009, Step: 172, Val Loss: 1.0922\n",
            "Epoch: 009, Step: 173, Val Loss: 1.1914\n",
            "Epoch: 009, Step: 174, Val Loss: 1.1173\n",
            "Epoch: 009, Step: 175, Val Loss: 1.1168\n",
            "Epoch: 009, Step: 176, Val Loss: 1.0974\n",
            "Epoch: 009, Step: 177, Val Loss: 0.9186\n",
            "Epoch: 009, Step: 178, Val Loss: 1.0022\n",
            "Epoch: 009, Step: 179, Val Loss: 1.0139\n",
            "Epoch: 009, Step: 180, Val Loss: 0.9448\n",
            "Epoch: 009, Step: 181, Val Loss: 1.0971\n",
            "Epoch: 009, Step: 182, Val Loss: 0.9440\n",
            "Epoch: 009, Step: 183, Val Loss: 1.0474\n",
            "Epoch: 009, Step: 184, Val Loss: 1.2400\n",
            "Epoch: 009, Step: 185, Val Loss: 0.9725\n",
            "Epoch: 009, Step: 186, Val Loss: 1.0394\n",
            "Epoch: 009, Step: 187, Val Loss: 1.0524\n",
            "Epoch: 009, Step: 188, Val Loss: 1.0786\n",
            "Epoch: 009, Step: 189, Val Loss: 0.9472\n",
            "Epoch: 009, Step: 190, Val Loss: 1.0989\n",
            "Epoch: 009, Step: 191, Val Loss: 0.9899\n",
            "Epoch: 009, Step: 192, Val Loss: 0.8718\n",
            "Epoch: 009, Step: 193, Val Loss: 0.9606\n",
            "Epoch: 009, Step: 194, Val Loss: 0.8650\n",
            "Epoch: 009, Step: 195, Val Loss: 1.0715\n",
            "Epoch: 009, Step: 196, Val Loss: 1.1183\n",
            "Epoch: 009, Step: 197, Val Loss: 0.9458\n",
            "Epoch: 009, Step: 198, Val Loss: 1.0604\n",
            "Epoch: 009, Step: 199, Val Loss: 1.1020\n",
            "Epoch: 009, Step: 200, Val Loss: 0.9918\n",
            "Epoch: 009, Step: 201, Val Loss: 0.9927\n",
            "Epoch: 009, Step: 202, Val Loss: 0.8581\n",
            "Epoch: 009, Step: 203, Val Loss: 0.8799\n",
            "Epoch: 009, Step: 204, Val Loss: 0.8918\n",
            "Epoch: 009, Step: 205, Val Loss: 0.9824\n",
            "Epoch: 009, Step: 206, Val Loss: 1.0865\n",
            "Epoch: 009, Step: 207, Val Loss: 1.1948\n",
            "Epoch: 009, Step: 208, Val Loss: 0.9616\n",
            "Epoch: 009, Step: 209, Val Loss: 1.1891\n",
            "Epoch: 009, Step: 210, Val Loss: 0.9527\n",
            "Epoch: 009, Step: 211, Val Loss: 0.8290\n",
            "Epoch: 009, Step: 212, Val Loss: 0.9655\n",
            "Epoch: 009, Step: 213, Val Loss: 0.8666\n",
            "Epoch: 009, Step: 214, Val Loss: 1.1050\n",
            "Epoch: 009, Step: 215, Val Loss: 1.0770\n",
            "Epoch: 009, Step: 216, Val Loss: 1.1117\n",
            "Epoch: 009, Step: 217, Val Loss: 0.9948\n",
            "Epoch: 009, Step: 218, Val Loss: 0.9980\n",
            "Epoch: 009, Step: 219, Val Loss: 0.8846\n",
            "Epoch: 009, Step: 220, Val Loss: 0.9493\n",
            "Epoch: 009, Step: 221, Val Loss: 0.9302\n",
            "Epoch: 009, Step: 222, Val Loss: 0.9955\n",
            "Epoch: 009, Step: 223, Val Loss: 1.0998\n",
            "Epoch: 009, Step: 224, Val Loss: 0.9191\n",
            "Epoch: 009, Step: 225, Val Loss: 0.7767\n",
            "Epoch: 009, Step: 226, Val Loss: 0.9993\n",
            "Epoch: 009, Step: 227, Val Loss: 0.9124\n",
            "Epoch: 009, Step: 228, Val Loss: 0.9302\n",
            "Epoch: 009, Step: 229, Val Loss: 1.0218\n",
            "Epoch: 009, Step: 230, Val Loss: 0.9038\n",
            "Epoch: 009, Step: 231, Val Loss: 1.1271\n",
            "Epoch: 009, Step: 232, Val Loss: 1.0931\n",
            "Epoch: 009, Step: 233, Val Loss: 0.9622\n",
            "Epoch: 009, Step: 234, Val Loss: 0.9104\n",
            "Epoch: 009, Step: 235, Val Loss: 0.9891\n",
            "Epoch: 009, Step: 236, Val Loss: 0.9044\n",
            "Epoch: 009, Step: 237, Val Loss: 0.9161\n",
            "Epoch: 009, Step: 238, Val Loss: 1.1185\n",
            "Epoch: 009, Step: 239, Val Loss: 0.9664\n",
            "Epoch: 009, Step: 240, Val Loss: 1.0280\n",
            "Epoch: 009, Step: 241, Val Loss: 0.7848\n",
            "Epoch: 009, Step: 242, Val Loss: 0.9418\n",
            "Epoch: 009, Step: 243, Val Loss: 0.8493\n",
            "Epoch: 009, Step: 244, Val Loss: 0.8700\n",
            "Epoch: 009, Step: 245, Val Loss: 1.0019\n",
            "Epoch: 009, Step: 246, Val Loss: 0.9911\n",
            "Epoch: 009, Step: 247, Val Loss: 0.8570\n",
            "Epoch: 009, Step: 248, Val Loss: 0.8577\n",
            "Epoch: 009, Step: 249, Val Loss: 0.9744\n",
            "Epoch: 009, Step: 250, Val Loss: 0.9159\n",
            "Epoch: 009, Step: 251, Val Loss: 0.9445\n",
            "Epoch: 009, Step: 252, Val Loss: 0.8140\n",
            "Epoch: 009, Step: 253, Val Loss: 0.8840\n",
            "Epoch: 009, Step: 254, Val Loss: 1.0395\n",
            "Epoch: 009, Step: 255, Val Loss: 1.0799\n",
            "Epoch: 009, Step: 256, Val Loss: 0.9798\n",
            "Epoch: 009, Step: 257, Val Loss: 0.9562\n",
            "Epoch: 009, Step: 258, Val Loss: 0.7621\n",
            "Epoch: 009, Step: 259, Val Loss: 0.9736\n",
            "Epoch: 009, Step: 260, Val Loss: 1.1643\n",
            "Epoch: 009, Step: 261, Val Loss: 1.0004\n",
            "Epoch: 009, Step: 262, Val Loss: 1.0066\n",
            "Epoch: 009, Step: 263, Val Loss: 1.0302\n",
            "Epoch: 009, Step: 264, Val Loss: 0.9413\n",
            "Epoch: 009, Step: 265, Val Loss: 0.8180\n",
            "Epoch: 009, Step: 266, Val Loss: 0.9479\n",
            "Epoch: 009, Step: 267, Val Loss: 0.9879\n",
            "Epoch: 009, Step: 268, Val Loss: 0.8580\n",
            "Epoch: 009, Step: 269, Val Loss: 0.8189\n",
            "Epoch: 009, Step: 270, Val Loss: 0.9074\n",
            "Epoch: 009, Step: 271, Val Loss: 1.0282\n",
            "Epoch: 009, Step: 272, Val Loss: 1.0660\n",
            "Epoch: 009, Step: 273, Val Loss: 1.0557\n",
            "Epoch: 009, Step: 274, Val Loss: 0.9089\n",
            "Epoch: 009, Step: 275, Val Loss: 0.8924\n",
            "Epoch: 009, Step: 276, Val Loss: 0.8761\n",
            "Epoch: 009, Step: 277, Val Loss: 1.1193\n",
            "Epoch: 009, Step: 278, Val Loss: 1.1286\n",
            "Epoch: 009, Step: 279, Val Loss: 1.0660\n",
            "Epoch: 009, Step: 280, Val Loss: 1.4607\n",
            "Epoch: 009, Step: 281, Val Loss: 2.6320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "4eFDj6sZQnpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_classification(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_classes = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            x_dict = batch.x_dict\n",
        "            edge_index_dict = batch.edge_index_dict\n",
        "            test_mask = batch['products'].test_mask\n",
        "            data_y = batch['products'].y\n",
        "\n",
        "            out = model(x_dict, edge_index_dict)\n",
        "\n",
        "            preds = out['products'][test_mask].argmax(dim=1)\n",
        "            predictions.append(preds)\n",
        "            true_classes.append(data_y[test_mask])\n",
        "\n",
        "        predictions = torch.cat(predictions, dim=0)\n",
        "        true_classes = torch.cat(true_classes, dim=0)\n",
        "\n",
        "        print(f'Accuracy: {accuracy_score(true_classes.cpu(), predictions.cpu()):.2f}')\n",
        "        print(f'Precision: {precision_score(true_classes.cpu(), predictions.cpu(), average=\"weighted\"):.2f}')\n",
        "        print(f'Recall: {recall_score(true_classes.cpu(), predictions.cpu(), average=\"weighted\"):.2f}')\n",
        "        print(f'F1-Score: {f1_score(true_classes.cpu(), predictions.cpu(), average=\"weighted\"):.2f}')"
      ],
      "metadata": {
        "id": "R6gmvlZ5DZ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_classification(model=model, test_loader=test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux9jFivYQeo7",
        "outputId": "66791cfa-0d88-44f0-e0de-4c285dae7064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n",
            "Precision: 0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.63\n",
            "F1-Score: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2"
      ],
      "metadata": {
        "id": "iDBRhZtPkR8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import SGD\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.nn import LightGCN\n",
        "from torch_geometric.datasets import IMDB\n",
        "from torch_geometric.transforms import RandomLinkSplit"
      ],
      "metadata": {
        "id": "5Od4u77OPuEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import mse_loss\n",
        "from torch_geometric.nn import to_hetero\n",
        "from torch_geometric.nn import Linear, SAGEConv\n",
        "\n",
        "\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = torch.cat([z_dict['products'][row], z_dict['customers'][col]], dim=-1)\n",
        "\n",
        "        z = self.lin1(z).relu()\n",
        "        z = self.lin2(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, data):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "\n",
        "\n",
        "def train_link_prediction(model, train_data, val_data, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
        "                     train_data['products', 'customers'].edge_label_index)\n",
        "        target = train_data['products', 'customers'].edge_label\n",
        "        loss = mse_loss(pred, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        pred = model(val_data.x_dict, val_data.edge_index_dict,\n",
        "                     val_data['products', 'customers'].edge_label_index)\n",
        "        pred = pred.clamp(min=0, max=5)\n",
        "        target = val_data['products', 'customers'].edge_label.float()\n",
        "        val_loss = mse_loss(pred, target).sqrt()\n",
        "\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "8Gl5Eom_klqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_test_split = RandomLinkSplit(num_val=0.2,\n",
        "                                       num_test=0.2,\n",
        "                                       add_negative_train_samples=True,\n",
        "                                       edge_types=[('products', 'similar_to', 'products'), ('products', 'reviewed_by', 'customers')],\n",
        "                                       rev_edge_types=[('products', 'similar_to', 'products'), ('customers', 'reviewed_by', 'products')])"
      ],
      "metadata": {
        "id": "2Mky5U0okqk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = train_val_test_split(data)"
      ],
      "metadata": {
        "id": "XqpN-oaGlHK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQJ-kId9lJh5",
        "outputId": "916bd21d-b5ac-40cf-9047-bc57d6541b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  products={\n",
              "    x=[120120, 10],\n",
              "    y=[120120],\n",
              "    train_mask=[120120],\n",
              "    test_mask=[120120],\n",
              "    val_mask=[120120],\n",
              "  },\n",
              "  customers={ x=[257068, 10] },\n",
              "  (products, similar_to, products)={\n",
              "    edge_index=[2, 70740],\n",
              "    edge_label=[141480],\n",
              "    edge_label_index=[2, 141480],\n",
              "  },\n",
              "  (products, reviewed_by, customers)={\n",
              "    edge_index=[2, 303465],\n",
              "    edge_attr=[303465],\n",
              "    edge_label=[606930],\n",
              "    edge_label_index=[2, 606930],\n",
              "  },\n",
              "  (customers, reviewed_by, products)={}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lp = Model(hidden_channels=196, data=data)"
      ],
      "metadata": {
        "id": "Hnn-9I3ilPiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_adam = Adam(model_lp.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "2Q8lhUQulWtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_link_prediction(model=model_lp,\n",
        "                      train_data=train_data,\n",
        "                      val_data=val_data,\n",
        "                      optimizer=optimizer_adam,\n",
        "                      epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcDPO7WblcAR",
        "outputId": "668b2c82-4921-4425-8a97-48404d77d6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.2212, Val Loss: 0.4710\n",
            "Epoch: 001, Loss: 0.1807, Val Loss: 0.4279\n",
            "Epoch: 002, Loss: 0.1281, Val Loss: 0.4409\n",
            "Epoch: 003, Loss: 0.1342, Val Loss: 0.4400\n",
            "Epoch: 004, Loss: 0.1366, Val Loss: 0.4043\n",
            "Epoch: 005, Loss: 0.1125, Val Loss: 0.3746\n",
            "Epoch: 006, Loss: 0.0994, Val Loss: 0.3711\n",
            "Epoch: 007, Loss: 0.1087, Val Loss: 0.3739\n",
            "Epoch: 008, Loss: 0.1167, Val Loss: 0.3682\n",
            "Epoch: 009, Loss: 0.1094, Val Loss: 0.3671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "QiOO9A_gVzPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_link_prediction(model, test_data):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        # Predict edge weights for the test set\n",
        "        pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
        "                     test_data['products', 'customers'].edge_label_index)\n",
        "\n",
        "        # Clamp predictions to the valid range (e.g., ratings between 0 and 5)\n",
        "        pred = pred.clamp(min=0, max=5)\n",
        "\n",
        "        # Convert predictions to discrete classes (round to nearest integer)\n",
        "        pred_classes = pred.round().long()\n",
        "\n",
        "        # True labels (ground truth)\n",
        "        target = test_data['products', 'customers'].edge_label.long()\n",
        "\n",
        "        # Compute metrics\n",
        "        accuracy = accuracy_score(target.cpu(), pred_classes.cpu())\n",
        "        precision = precision_score(target.cpu(), pred_classes.cpu(), average='weighted', zero_division=0)\n",
        "        recall = recall_score(target.cpu(), pred_classes.cpu(), average='weighted', zero_division=0)\n",
        "        f1 = f1_score(target.cpu(), pred_classes.cpu(), average='weighted', zero_division=0)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'Precision: {precision:.2f}')\n",
        "        print(f'Recall: {recall:.2f}')\n",
        "        print(f'F1: {f1:.2f}')"
      ],
      "metadata": {
        "id": "HrEm3X_LT9d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_link_prediction(model_lp, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdlX03lT852",
        "outputId": "ec63afb4-4a0d-4474-b438-72e1b7218f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n",
            "Precision: 0.86\n",
            "Recall: 0.85\n",
            "F1: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 3"
      ],
      "metadata": {
        "id": "pcEdzN2hs6fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def train(dataset, train_loader, model, optimizer, num_products, num_customers, epochs=1):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_examples = 0, 0\n",
        "\n",
        "        for node_ids in train_loader:\n",
        "            pos_edge_label_index = dataset.edge_index[:, node_ids]\n",
        "            neg_edge_label_index = torch.stack([pos_edge_label_index[0],\n",
        "                                                torch.randint(num_products, num_products + num_customers,\n",
        "                                                              (node_ids.numel(),))],\n",
        "                                               dim=0)\n",
        "            edge_label_index = torch.cat([pos_edge_label_index, neg_edge_label_index], dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pos_rank, neg_rank = model(dataset.edge_index, edge_label_index).chunk(2)\n",
        "\n",
        "            loss = model.recommendation_loss(pos_rank, neg_rank, node_id=edge_label_index.unique())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += float(loss) * pos_rank.numel()\n",
        "            total_examples += pos_rank.numel()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}')"
      ],
      "metadata": {
        "id": "mR_5wU2rs7J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_test_split = RandomLinkSplit(num_val=0.0,\n",
        "                                       num_test=0.2,\n",
        "                                       add_negative_train_samples=True,\n",
        "                                       edge_types=[('products', 'similar_to', 'products'), ('products', 'reviewed_by', 'customers')],\n",
        "                                       rev_edge_types=[('products', 'similar_to', 'products'), ('customers', 'reviewed_by', 'products')])"
      ],
      "metadata": {
        "id": "dq1DL1w-tAq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = train_val_test_split(data)"
      ],
      "metadata": {
        "id": "sledQTmwtF7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data2 = train_data.to_homogeneous()"
      ],
      "metadata": {
        "id": "tINyxLruXEXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lightgcn = LightGCN(num_nodes=train_data2.num_nodes,\n",
        "                          embedding_dim=128,\n",
        "                          num_layers=1)"
      ],
      "metadata": {
        "id": "6GyGOYXoXQXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(range(train_data2.edge_index.size(1)),\n",
        "                         shuffle=True,\n",
        "                         batch_size=16)"
      ],
      "metadata": {
        "id": "pcRhzUC4tMiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(model_lightgcn.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "VfCDTqCNtOrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset=train_data2,\n",
        "      train_loader=data_loader,\n",
        "      model=model_lightgcn,\n",
        "      optimizer=opt,\n",
        "      num_products=data['products'].num_nodes,\n",
        "      num_customers=data['customers'].num_nodes,\n",
        "      epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thEzT-LTtVxk",
        "outputId": "585ea542-fe18-437a-f6b4-362064e88142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.6913\n",
            "Epoch: 000, Loss: 0.6917\n",
            "Epoch: 000, Loss: 0.6919\n",
            "Epoch: 000, Loss: 0.6918\n",
            "Epoch: 000, Loss: 0.6918\n",
            "Epoch: 000, Loss: 0.6916\n",
            "Epoch: 000, Loss: 0.6917\n",
            "Epoch: 000, Loss: 0.6912\n",
            "Epoch: 000, Loss: 0.6911\n",
            "Epoch: 000, Loss: 0.6911\n",
            "Epoch: 000, Loss: 0.6911\n",
            "Epoch: 000, Loss: 0.6911\n",
            "Epoch: 000, Loss: 0.6911\n",
            "Epoch: 000, Loss: 0.6909\n",
            "Epoch: 000, Loss: 0.6909\n",
            "Epoch: 000, Loss: 0.6903\n",
            "Epoch: 000, Loss: 0.6897\n",
            "Epoch: 000, Loss: 0.6898\n",
            "Epoch: 000, Loss: 0.6897\n",
            "Epoch: 000, Loss: 0.6897\n",
            "Epoch: 000, Loss: 0.6894\n",
            "Epoch: 000, Loss: 0.6889\n",
            "Epoch: 000, Loss: 0.6890\n",
            "Epoch: 000, Loss: 0.6891\n",
            "Epoch: 000, Loss: 0.6889\n",
            "Epoch: 000, Loss: 0.6888\n",
            "Epoch: 000, Loss: 0.6889\n",
            "Epoch: 000, Loss: 0.6890\n",
            "Epoch: 000, Loss: 0.6888\n",
            "Epoch: 000, Loss: 0.6883\n",
            "Epoch: 000, Loss: 0.6880\n",
            "Epoch: 000, Loss: 0.6877\n",
            "Epoch: 000, Loss: 0.6874\n",
            "Epoch: 000, Loss: 0.6871\n",
            "Epoch: 000, Loss: 0.6869\n",
            "Epoch: 000, Loss: 0.6870\n",
            "Epoch: 000, Loss: 0.6868\n",
            "Epoch: 000, Loss: 0.6867\n",
            "Epoch: 000, Loss: 0.6867\n",
            "Epoch: 000, Loss: 0.6860\n",
            "Epoch: 000, Loss: 0.6854\n",
            "Epoch: 000, Loss: 0.6847\n",
            "Epoch: 000, Loss: 0.6840\n",
            "Epoch: 000, Loss: 0.6835\n",
            "Epoch: 000, Loss: 0.6832\n",
            "Epoch: 000, Loss: 0.6827\n",
            "Epoch: 000, Loss: 0.6824\n",
            "Epoch: 000, Loss: 0.6825\n",
            "Epoch: 000, Loss: 0.6812\n",
            "Epoch: 000, Loss: 0.6802\n",
            "Epoch: 000, Loss: 0.6799\n",
            "Epoch: 000, Loss: 0.6796\n",
            "Epoch: 000, Loss: 0.6789\n",
            "Epoch: 000, Loss: 0.6778\n",
            "Epoch: 000, Loss: 0.6774\n",
            "Epoch: 000, Loss: 0.6762\n",
            "Epoch: 000, Loss: 0.6756\n",
            "Epoch: 000, Loss: 0.6750\n",
            "Epoch: 000, Loss: 0.6747\n",
            "Epoch: 000, Loss: 0.6747\n",
            "Epoch: 000, Loss: 0.6742\n",
            "Epoch: 000, Loss: 0.6740\n",
            "Epoch: 000, Loss: 0.6735\n",
            "Epoch: 000, Loss: 0.6729\n",
            "Epoch: 000, Loss: 0.6731\n",
            "Epoch: 000, Loss: 0.6728\n",
            "Epoch: 000, Loss: 0.6721\n",
            "Epoch: 000, Loss: 0.6710\n",
            "Epoch: 000, Loss: 0.6712\n",
            "Epoch: 000, Loss: 0.6713\n",
            "Epoch: 000, Loss: 0.6710\n",
            "Epoch: 000, Loss: 0.6707\n",
            "Epoch: 000, Loss: 0.6687\n",
            "Epoch: 000, Loss: 0.6681\n",
            "Epoch: 000, Loss: 0.6668\n",
            "Epoch: 000, Loss: 0.6664\n",
            "Epoch: 000, Loss: 0.6661\n",
            "Epoch: 000, Loss: 0.6647\n",
            "Epoch: 000, Loss: 0.6638\n",
            "Epoch: 000, Loss: 0.6631\n",
            "Epoch: 000, Loss: 0.6627\n",
            "Epoch: 000, Loss: 0.6621\n",
            "Epoch: 000, Loss: 0.6624\n",
            "Epoch: 000, Loss: 0.6621\n",
            "Epoch: 000, Loss: 0.6619\n",
            "Epoch: 000, Loss: 0.6612\n",
            "Epoch: 000, Loss: 0.6614\n",
            "Epoch: 000, Loss: 0.6613\n",
            "Epoch: 000, Loss: 0.6600\n",
            "Epoch: 000, Loss: 0.6599\n",
            "Epoch: 000, Loss: 0.6588\n",
            "Epoch: 000, Loss: 0.6590\n",
            "Epoch: 000, Loss: 0.6585\n",
            "Epoch: 000, Loss: 0.6584\n",
            "Epoch: 000, Loss: 0.6574\n",
            "Epoch: 000, Loss: 0.6575\n",
            "Epoch: 000, Loss: 0.6578\n",
            "Epoch: 000, Loss: 0.6572\n",
            "Epoch: 000, Loss: 0.6567\n",
            "Epoch: 000, Loss: 0.6569\n",
            "Epoch: 000, Loss: 0.6556\n",
            "Epoch: 000, Loss: 0.6551\n",
            "Epoch: 000, Loss: 0.6546\n",
            "Epoch: 000, Loss: 0.6544\n",
            "Epoch: 000, Loss: 0.6547\n",
            "Epoch: 000, Loss: 0.6544\n",
            "Epoch: 000, Loss: 0.6546\n",
            "Epoch: 000, Loss: 0.6544\n",
            "Epoch: 000, Loss: 0.6541\n",
            "Epoch: 000, Loss: 0.6529\n",
            "Epoch: 000, Loss: 0.6527\n",
            "Epoch: 000, Loss: 0.6529\n",
            "Epoch: 000, Loss: 0.6528\n",
            "Epoch: 000, Loss: 0.6523\n",
            "Epoch: 000, Loss: 0.6518\n",
            "Epoch: 000, Loss: 0.6516\n",
            "Epoch: 000, Loss: 0.6513\n",
            "Epoch: 000, Loss: 0.6502\n",
            "Epoch: 000, Loss: 0.6488\n",
            "Epoch: 000, Loss: 0.6483\n",
            "Epoch: 000, Loss: 0.6476\n",
            "Epoch: 000, Loss: 0.6471\n",
            "Epoch: 000, Loss: 0.6464\n",
            "Epoch: 000, Loss: 0.6455\n",
            "Epoch: 000, Loss: 0.6451\n",
            "Epoch: 000, Loss: 0.6450\n",
            "Epoch: 000, Loss: 0.6444\n",
            "Epoch: 000, Loss: 0.6441\n",
            "Epoch: 000, Loss: 0.6436\n",
            "Epoch: 000, Loss: 0.6433\n",
            "Epoch: 000, Loss: 0.6431\n",
            "Epoch: 000, Loss: 0.6425\n",
            "Epoch: 000, Loss: 0.6427\n",
            "Epoch: 000, Loss: 0.6420\n",
            "Epoch: 000, Loss: 0.6419\n",
            "Epoch: 000, Loss: 0.6421\n",
            "Epoch: 000, Loss: 0.6408\n",
            "Epoch: 000, Loss: 0.6406\n",
            "Epoch: 000, Loss: 0.6402\n",
            "Epoch: 000, Loss: 0.6401\n",
            "Epoch: 000, Loss: 0.6397\n",
            "Epoch: 000, Loss: 0.6394\n",
            "Epoch: 000, Loss: 0.6392\n",
            "Epoch: 000, Loss: 0.6390\n",
            "Epoch: 000, Loss: 0.6383\n",
            "Epoch: 000, Loss: 0.6375\n",
            "Epoch: 000, Loss: 0.6369\n",
            "Epoch: 000, Loss: 0.6368\n",
            "Epoch: 000, Loss: 0.6357\n",
            "Epoch: 000, Loss: 0.6354\n",
            "Epoch: 000, Loss: 0.6349\n",
            "Epoch: 000, Loss: 0.6347\n",
            "Epoch: 000, Loss: 0.6335\n",
            "Epoch: 000, Loss: 0.6328\n",
            "Epoch: 000, Loss: 0.6323\n",
            "Epoch: 000, Loss: 0.6323\n",
            "Epoch: 000, Loss: 0.6325\n",
            "Epoch: 000, Loss: 0.6324\n",
            "Epoch: 000, Loss: 0.6323\n",
            "Epoch: 000, Loss: 0.6319\n",
            "Epoch: 000, Loss: 0.6316\n",
            "Epoch: 000, Loss: 0.6319\n",
            "Epoch: 000, Loss: 0.6320\n",
            "Epoch: 000, Loss: 0.6319\n",
            "Epoch: 000, Loss: 0.6319\n",
            "Epoch: 000, Loss: 0.6313\n",
            "Epoch: 000, Loss: 0.6310\n",
            "Epoch: 000, Loss: 0.6308\n",
            "Epoch: 000, Loss: 0.6310\n",
            "Epoch: 000, Loss: 0.6311\n",
            "Epoch: 000, Loss: 0.6307\n",
            "Epoch: 000, Loss: 0.6306\n",
            "Epoch: 000, Loss: 0.6305\n",
            "Epoch: 000, Loss: 0.6304\n",
            "Epoch: 000, Loss: 0.6297\n",
            "Epoch: 000, Loss: 0.6294\n",
            "Epoch: 000, Loss: 0.6291\n",
            "Epoch: 000, Loss: 0.6288\n",
            "Epoch: 000, Loss: 0.6288\n",
            "Epoch: 000, Loss: 0.6281\n",
            "Epoch: 000, Loss: 0.6277\n",
            "Epoch: 000, Loss: 0.6276\n",
            "Epoch: 000, Loss: 0.6275\n",
            "Epoch: 000, Loss: 0.6270\n",
            "Epoch: 000, Loss: 0.6264\n",
            "Epoch: 000, Loss: 0.6261\n",
            "Epoch: 000, Loss: 0.6256\n",
            "Epoch: 000, Loss: 0.6253\n",
            "Epoch: 000, Loss: 0.6248\n",
            "Epoch: 000, Loss: 0.6248\n",
            "Epoch: 000, Loss: 0.6247\n",
            "Epoch: 000, Loss: 0.6246\n",
            "Epoch: 000, Loss: 0.6240\n",
            "Epoch: 000, Loss: 0.6238\n",
            "Epoch: 000, Loss: 0.6235\n",
            "Epoch: 000, Loss: 0.6235\n",
            "Epoch: 000, Loss: 0.6231\n",
            "Epoch: 000, Loss: 0.6230\n",
            "Epoch: 000, Loss: 0.6224\n",
            "Epoch: 000, Loss: 0.6217\n",
            "Epoch: 000, Loss: 0.6211\n",
            "Epoch: 000, Loss: 0.6209\n",
            "Epoch: 000, Loss: 0.6204\n",
            "Epoch: 000, Loss: 0.6197\n",
            "Epoch: 000, Loss: 0.6192\n",
            "Epoch: 000, Loss: 0.6193\n",
            "Epoch: 000, Loss: 0.6189\n",
            "Epoch: 000, Loss: 0.6186\n",
            "Epoch: 000, Loss: 0.6183\n",
            "Epoch: 000, Loss: 0.6176\n",
            "Epoch: 000, Loss: 0.6171\n",
            "Epoch: 000, Loss: 0.6171\n",
            "Epoch: 000, Loss: 0.6170\n",
            "Epoch: 000, Loss: 0.6166\n",
            "Epoch: 000, Loss: 0.6166\n",
            "Epoch: 000, Loss: 0.6165\n",
            "Epoch: 000, Loss: 0.6163\n",
            "Epoch: 000, Loss: 0.6161\n",
            "Epoch: 000, Loss: 0.6159\n",
            "Epoch: 000, Loss: 0.6154\n",
            "Epoch: 000, Loss: 0.6150\n",
            "Epoch: 000, Loss: 0.6143\n",
            "Epoch: 000, Loss: 0.6142\n",
            "Epoch: 000, Loss: 0.6139\n",
            "Epoch: 000, Loss: 0.6131\n",
            "Epoch: 000, Loss: 0.6124\n",
            "Epoch: 000, Loss: 0.6120\n",
            "Epoch: 000, Loss: 0.6117\n",
            "Epoch: 000, Loss: 0.6114\n",
            "Epoch: 000, Loss: 0.6109\n",
            "Epoch: 000, Loss: 0.6100\n",
            "Epoch: 000, Loss: 0.6095\n",
            "Epoch: 000, Loss: 0.6093\n",
            "Epoch: 000, Loss: 0.6088\n",
            "Epoch: 000, Loss: 0.6084\n",
            "Epoch: 000, Loss: 0.6081\n",
            "Epoch: 000, Loss: 0.6080\n",
            "Epoch: 000, Loss: 0.6074\n",
            "Epoch: 000, Loss: 0.6074\n",
            "Epoch: 000, Loss: 0.6072\n",
            "Epoch: 000, Loss: 0.6069\n",
            "Epoch: 000, Loss: 0.6064\n",
            "Epoch: 000, Loss: 0.6057\n",
            "Epoch: 000, Loss: 0.6058\n",
            "Epoch: 000, Loss: 0.6055\n",
            "Epoch: 000, Loss: 0.6052\n",
            "Epoch: 000, Loss: 0.6048\n",
            "Epoch: 000, Loss: 0.6042\n",
            "Epoch: 000, Loss: 0.6039\n",
            "Epoch: 000, Loss: 0.6036\n",
            "Epoch: 000, Loss: 0.6032\n",
            "Epoch: 000, Loss: 0.6032\n",
            "Epoch: 000, Loss: 0.6027\n",
            "Epoch: 000, Loss: 0.6024\n",
            "Epoch: 000, Loss: 0.6021\n",
            "Epoch: 000, Loss: 0.6017\n",
            "Epoch: 000, Loss: 0.6018\n",
            "Epoch: 000, Loss: 0.6015\n",
            "Epoch: 000, Loss: 0.6008\n",
            "Epoch: 000, Loss: 0.6008\n",
            "Epoch: 000, Loss: 0.6000\n",
            "Epoch: 000, Loss: 0.5997\n",
            "Epoch: 000, Loss: 0.5995\n",
            "Epoch: 000, Loss: 0.5990\n",
            "Epoch: 000, Loss: 0.5990\n",
            "Epoch: 000, Loss: 0.5985\n",
            "Epoch: 000, Loss: 0.5982\n",
            "Epoch: 000, Loss: 0.5975\n",
            "Epoch: 000, Loss: 0.5971\n",
            "Epoch: 000, Loss: 0.5967\n",
            "Epoch: 000, Loss: 0.5966\n",
            "Epoch: 000, Loss: 0.5966\n",
            "Epoch: 000, Loss: 0.5963\n",
            "Epoch: 000, Loss: 0.5961\n",
            "Epoch: 000, Loss: 0.5956\n",
            "Epoch: 000, Loss: 0.5954\n",
            "Epoch: 000, Loss: 0.5953\n",
            "Epoch: 000, Loss: 0.5952\n",
            "Epoch: 000, Loss: 0.5950\n",
            "Epoch: 000, Loss: 0.5949\n",
            "Epoch: 000, Loss: 0.5950\n",
            "Epoch: 000, Loss: 0.5948\n",
            "Epoch: 000, Loss: 0.5943\n",
            "Epoch: 000, Loss: 0.5939\n",
            "Epoch: 000, Loss: 0.5941\n",
            "Epoch: 000, Loss: 0.5936\n",
            "Epoch: 000, Loss: 0.5935\n",
            "Epoch: 000, Loss: 0.5933\n",
            "Epoch: 000, Loss: 0.5928\n",
            "Epoch: 000, Loss: 0.5925\n",
            "Epoch: 000, Loss: 0.5922\n",
            "Epoch: 000, Loss: 0.5918\n",
            "Epoch: 000, Loss: 0.5915\n",
            "Epoch: 000, Loss: 0.5913\n",
            "Epoch: 000, Loss: 0.5907\n",
            "Epoch: 000, Loss: 0.5906\n",
            "Epoch: 000, Loss: 0.5905\n",
            "Epoch: 000, Loss: 0.5899\n",
            "Epoch: 000, Loss: 0.5896\n",
            "Epoch: 000, Loss: 0.5894\n",
            "Epoch: 000, Loss: 0.5889\n",
            "Epoch: 000, Loss: 0.5886\n",
            "Epoch: 000, Loss: 0.5885\n",
            "Epoch: 000, Loss: 0.5883\n",
            "Epoch: 000, Loss: 0.5880\n",
            "Epoch: 000, Loss: 0.5879\n",
            "Epoch: 000, Loss: 0.5877\n",
            "Epoch: 000, Loss: 0.5877\n",
            "Epoch: 000, Loss: 0.5875\n",
            "Epoch: 000, Loss: 0.5874\n",
            "Epoch: 000, Loss: 0.5873\n",
            "Epoch: 000, Loss: 0.5872\n",
            "Epoch: 000, Loss: 0.5869\n",
            "Epoch: 000, Loss: 0.5868\n",
            "Epoch: 000, Loss: 0.5865\n",
            "Epoch: 000, Loss: 0.5865\n",
            "Epoch: 000, Loss: 0.5863\n",
            "Epoch: 000, Loss: 0.5863\n",
            "Epoch: 000, Loss: 0.5861\n",
            "Epoch: 000, Loss: 0.5860\n",
            "Epoch: 000, Loss: 0.5858\n",
            "Epoch: 000, Loss: 0.5852\n",
            "Epoch: 000, Loss: 0.5847\n",
            "Epoch: 000, Loss: 0.5841\n",
            "Epoch: 000, Loss: 0.5842\n",
            "Epoch: 000, Loss: 0.5840\n",
            "Epoch: 000, Loss: 0.5838\n",
            "Epoch: 000, Loss: 0.5834\n",
            "Epoch: 000, Loss: 0.5831\n",
            "Epoch: 000, Loss: 0.5828\n",
            "Epoch: 000, Loss: 0.5827\n",
            "Epoch: 000, Loss: 0.5825\n",
            "Epoch: 000, Loss: 0.5820\n",
            "Epoch: 000, Loss: 0.5816\n",
            "Epoch: 000, Loss: 0.5814\n",
            "Epoch: 000, Loss: 0.5813\n",
            "Epoch: 000, Loss: 0.5811\n",
            "Epoch: 000, Loss: 0.5805\n",
            "Epoch: 000, Loss: 0.5799\n",
            "Epoch: 000, Loss: 0.5798\n",
            "Epoch: 000, Loss: 0.5798\n",
            "Epoch: 000, Loss: 0.5795\n",
            "Epoch: 000, Loss: 0.5792\n",
            "Epoch: 000, Loss: 0.5792\n",
            "Epoch: 000, Loss: 0.5790\n",
            "Epoch: 000, Loss: 0.5788\n",
            "Epoch: 000, Loss: 0.5786\n",
            "Epoch: 000, Loss: 0.5780\n",
            "Epoch: 000, Loss: 0.5779\n",
            "Epoch: 000, Loss: 0.5777\n",
            "Epoch: 000, Loss: 0.5776\n",
            "Epoch: 000, Loss: 0.5775\n",
            "Epoch: 000, Loss: 0.5769\n",
            "Epoch: 000, Loss: 0.5765\n",
            "Epoch: 000, Loss: 0.5763\n",
            "Epoch: 000, Loss: 0.5760\n",
            "Epoch: 000, Loss: 0.5758\n",
            "Epoch: 000, Loss: 0.5760\n",
            "Epoch: 000, Loss: 0.5755\n",
            "Epoch: 000, Loss: 0.5751\n",
            "Epoch: 000, Loss: 0.5749\n",
            "Epoch: 000, Loss: 0.5744\n",
            "Epoch: 000, Loss: 0.5742\n",
            "Epoch: 000, Loss: 0.5740\n",
            "Epoch: 000, Loss: 0.5742\n",
            "Epoch: 000, Loss: 0.5740\n",
            "Epoch: 000, Loss: 0.5737\n",
            "Epoch: 000, Loss: 0.5730\n",
            "Epoch: 000, Loss: 0.5726\n",
            "Epoch: 000, Loss: 0.5723\n",
            "Epoch: 000, Loss: 0.5721\n",
            "Epoch: 000, Loss: 0.5721\n",
            "Epoch: 000, Loss: 0.5719\n",
            "Epoch: 000, Loss: 0.5719\n",
            "Epoch: 000, Loss: 0.5718\n",
            "Epoch: 000, Loss: 0.5719\n",
            "Epoch: 000, Loss: 0.5719\n",
            "Epoch: 000, Loss: 0.5716\n",
            "Epoch: 000, Loss: 0.5714\n",
            "Epoch: 000, Loss: 0.5714\n",
            "Epoch: 000, Loss: 0.5713\n",
            "Epoch: 000, Loss: 0.5711\n",
            "Epoch: 000, Loss: 0.5708\n",
            "Epoch: 000, Loss: 0.5708\n",
            "Epoch: 000, Loss: 0.5705\n",
            "Epoch: 000, Loss: 0.5705\n",
            "Epoch: 000, Loss: 0.5703\n",
            "Epoch: 000, Loss: 0.5703\n",
            "Epoch: 000, Loss: 0.5700\n",
            "Epoch: 000, Loss: 0.5701\n",
            "Epoch: 000, Loss: 0.5696\n",
            "Epoch: 000, Loss: 0.5691\n",
            "Epoch: 000, Loss: 0.5689\n",
            "Epoch: 000, Loss: 0.5686\n",
            "Epoch: 000, Loss: 0.5682\n",
            "Epoch: 000, Loss: 0.5681\n",
            "Epoch: 000, Loss: 0.5674\n",
            "Epoch: 000, Loss: 0.5672\n",
            "Epoch: 000, Loss: 0.5665\n",
            "Epoch: 000, Loss: 0.5660\n",
            "Epoch: 000, Loss: 0.5655\n",
            "Epoch: 000, Loss: 0.5652\n",
            "Epoch: 000, Loss: 0.5651\n",
            "Epoch: 000, Loss: 0.5647\n",
            "Epoch: 000, Loss: 0.5644\n",
            "Epoch: 000, Loss: 0.5642\n",
            "Epoch: 000, Loss: 0.5639\n",
            "Epoch: 000, Loss: 0.5636\n",
            "Epoch: 000, Loss: 0.5631\n",
            "Epoch: 000, Loss: 0.5628\n",
            "Epoch: 000, Loss: 0.5627\n",
            "Epoch: 000, Loss: 0.5625\n",
            "Epoch: 000, Loss: 0.5624\n",
            "Epoch: 000, Loss: 0.5621\n",
            "Epoch: 000, Loss: 0.5622\n",
            "Epoch: 000, Loss: 0.5618\n",
            "Epoch: 000, Loss: 0.5614\n",
            "Epoch: 000, Loss: 0.5609\n",
            "Epoch: 000, Loss: 0.5608\n",
            "Epoch: 000, Loss: 0.5606\n",
            "Epoch: 000, Loss: 0.5604\n",
            "Epoch: 000, Loss: 0.5601\n",
            "Epoch: 000, Loss: 0.5599\n",
            "Epoch: 000, Loss: 0.5596\n",
            "Epoch: 000, Loss: 0.5593\n",
            "Epoch: 000, Loss: 0.5593\n",
            "Epoch: 000, Loss: 0.5594\n",
            "Epoch: 000, Loss: 0.5593\n",
            "Epoch: 000, Loss: 0.5592\n",
            "Epoch: 000, Loss: 0.5590\n",
            "Epoch: 000, Loss: 0.5590\n",
            "Epoch: 000, Loss: 0.5588\n",
            "Epoch: 000, Loss: 0.5584\n",
            "Epoch: 000, Loss: 0.5581\n",
            "Epoch: 000, Loss: 0.5581\n",
            "Epoch: 000, Loss: 0.5577\n",
            "Epoch: 000, Loss: 0.5572\n",
            "Epoch: 000, Loss: 0.5569\n",
            "Epoch: 000, Loss: 0.5568\n",
            "Epoch: 000, Loss: 0.5565\n",
            "Epoch: 000, Loss: 0.5563\n",
            "Epoch: 000, Loss: 0.5564\n",
            "Epoch: 000, Loss: 0.5562\n",
            "Epoch: 000, Loss: 0.5561\n",
            "Epoch: 000, Loss: 0.5557\n",
            "Epoch: 000, Loss: 0.5557\n",
            "Epoch: 000, Loss: 0.5554\n",
            "Epoch: 000, Loss: 0.5554\n",
            "Epoch: 000, Loss: 0.5555\n",
            "Epoch: 000, Loss: 0.5554\n",
            "Epoch: 000, Loss: 0.5554\n",
            "Epoch: 000, Loss: 0.5550\n",
            "Epoch: 000, Loss: 0.5547\n",
            "Epoch: 000, Loss: 0.5546\n",
            "Epoch: 000, Loss: 0.5543\n",
            "Epoch: 000, Loss: 0.5541\n",
            "Epoch: 000, Loss: 0.5541\n",
            "Epoch: 000, Loss: 0.5539\n",
            "Epoch: 000, Loss: 0.5537\n",
            "Epoch: 000, Loss: 0.5536\n",
            "Epoch: 000, Loss: 0.5533\n",
            "Epoch: 000, Loss: 0.5531\n",
            "Epoch: 000, Loss: 0.5528\n",
            "Epoch: 000, Loss: 0.5525\n",
            "Epoch: 000, Loss: 0.5523\n",
            "Epoch: 000, Loss: 0.5521\n",
            "Epoch: 000, Loss: 0.5519\n",
            "Epoch: 000, Loss: 0.5514\n",
            "Epoch: 000, Loss: 0.5513\n",
            "Epoch: 000, Loss: 0.5515\n",
            "Epoch: 000, Loss: 0.5513\n",
            "Epoch: 000, Loss: 0.5511\n",
            "Epoch: 000, Loss: 0.5509\n",
            "Epoch: 000, Loss: 0.5508\n",
            "Epoch: 000, Loss: 0.5507\n",
            "Epoch: 000, Loss: 0.5506\n",
            "Epoch: 000, Loss: 0.5504\n",
            "Epoch: 000, Loss: 0.5501\n",
            "Epoch: 000, Loss: 0.5498\n",
            "Epoch: 000, Loss: 0.5496\n",
            "Epoch: 000, Loss: 0.5494\n",
            "Epoch: 000, Loss: 0.5491\n",
            "Epoch: 000, Loss: 0.5487\n",
            "Epoch: 000, Loss: 0.5486\n",
            "Epoch: 000, Loss: 0.5483\n",
            "Epoch: 000, Loss: 0.5481\n",
            "Epoch: 000, Loss: 0.5479\n",
            "Epoch: 000, Loss: 0.5479\n",
            "Epoch: 000, Loss: 0.5476\n",
            "Epoch: 000, Loss: 0.5476\n",
            "Epoch: 000, Loss: 0.5476\n",
            "Epoch: 000, Loss: 0.5474\n",
            "Epoch: 000, Loss: 0.5472\n",
            "Epoch: 000, Loss: 0.5469\n",
            "Epoch: 000, Loss: 0.5466\n",
            "Epoch: 000, Loss: 0.5464\n",
            "Epoch: 000, Loss: 0.5464\n",
            "Epoch: 000, Loss: 0.5463\n",
            "Epoch: 000, Loss: 0.5465\n",
            "Epoch: 000, Loss: 0.5463\n",
            "Epoch: 000, Loss: 0.5462\n",
            "Epoch: 000, Loss: 0.5462\n",
            "Epoch: 000, Loss: 0.5459\n",
            "Epoch: 000, Loss: 0.5458\n",
            "Epoch: 000, Loss: 0.5455\n",
            "Epoch: 000, Loss: 0.5452\n",
            "Epoch: 000, Loss: 0.5450\n",
            "Epoch: 000, Loss: 0.5448\n",
            "Epoch: 000, Loss: 0.5445\n",
            "Epoch: 000, Loss: 0.5443\n",
            "Epoch: 000, Loss: 0.5440\n",
            "Epoch: 000, Loss: 0.5439\n",
            "Epoch: 000, Loss: 0.5438\n",
            "Epoch: 000, Loss: 0.5437\n",
            "Epoch: 000, Loss: 0.5435\n",
            "Epoch: 000, Loss: 0.5435\n",
            "Epoch: 000, Loss: 0.5431\n",
            "Epoch: 000, Loss: 0.5428\n",
            "Epoch: 000, Loss: 0.5427\n",
            "Epoch: 000, Loss: 0.5427\n",
            "Epoch: 000, Loss: 0.5426\n",
            "Epoch: 000, Loss: 0.5425\n",
            "Epoch: 000, Loss: 0.5422\n",
            "Epoch: 000, Loss: 0.5423\n",
            "Epoch: 000, Loss: 0.5422\n",
            "Epoch: 000, Loss: 0.5419\n",
            "Epoch: 000, Loss: 0.5415\n",
            "Epoch: 000, Loss: 0.5411\n",
            "Epoch: 000, Loss: 0.5409\n",
            "Epoch: 000, Loss: 0.5406\n",
            "Epoch: 000, Loss: 0.5405\n",
            "Epoch: 000, Loss: 0.5405\n",
            "Epoch: 000, Loss: 0.5402\n",
            "Epoch: 000, Loss: 0.5400\n",
            "Epoch: 000, Loss: 0.5399\n",
            "Epoch: 000, Loss: 0.5396\n",
            "Epoch: 000, Loss: 0.5393\n",
            "Epoch: 000, Loss: 0.5393\n",
            "Epoch: 000, Loss: 0.5387\n",
            "Epoch: 000, Loss: 0.5387\n",
            "Epoch: 000, Loss: 0.5385\n",
            "Epoch: 000, Loss: 0.5383\n",
            "Epoch: 000, Loss: 0.5382\n",
            "Epoch: 000, Loss: 0.5378\n",
            "Epoch: 000, Loss: 0.5374\n",
            "Epoch: 000, Loss: 0.5372\n",
            "Epoch: 000, Loss: 0.5371\n",
            "Epoch: 000, Loss: 0.5370\n",
            "Epoch: 000, Loss: 0.5369\n",
            "Epoch: 000, Loss: 0.5365\n",
            "Epoch: 000, Loss: 0.5365\n",
            "Epoch: 000, Loss: 0.5364\n",
            "Epoch: 000, Loss: 0.5361\n",
            "Epoch: 000, Loss: 0.5358\n",
            "Epoch: 000, Loss: 0.5358\n",
            "Epoch: 000, Loss: 0.5354\n",
            "Epoch: 000, Loss: 0.5352\n",
            "Epoch: 000, Loss: 0.5349\n",
            "Epoch: 000, Loss: 0.5347\n",
            "Epoch: 000, Loss: 0.5346\n",
            "Epoch: 000, Loss: 0.5345\n",
            "Epoch: 000, Loss: 0.5342\n",
            "Epoch: 000, Loss: 0.5340\n",
            "Epoch: 000, Loss: 0.5338\n",
            "Epoch: 000, Loss: 0.5338\n",
            "Epoch: 000, Loss: 0.5337\n",
            "Epoch: 000, Loss: 0.5337\n",
            "Epoch: 000, Loss: 0.5333\n",
            "Epoch: 000, Loss: 0.5332\n",
            "Epoch: 000, Loss: 0.5331\n",
            "Epoch: 000, Loss: 0.5331\n",
            "Epoch: 000, Loss: 0.5328\n",
            "Epoch: 000, Loss: 0.5325\n",
            "Epoch: 000, Loss: 0.5325\n",
            "Epoch: 000, Loss: 0.5323\n",
            "Epoch: 000, Loss: 0.5323\n",
            "Epoch: 000, Loss: 0.5321\n",
            "Epoch: 000, Loss: 0.5317\n",
            "Epoch: 000, Loss: 0.5315\n",
            "Epoch: 000, Loss: 0.5314\n",
            "Epoch: 000, Loss: 0.5314\n",
            "Epoch: 000, Loss: 0.5309\n",
            "Epoch: 000, Loss: 0.5308\n",
            "Epoch: 000, Loss: 0.5305\n",
            "Epoch: 000, Loss: 0.5303\n",
            "Epoch: 000, Loss: 0.5303\n",
            "Epoch: 000, Loss: 0.5301\n",
            "Epoch: 000, Loss: 0.5297\n",
            "Epoch: 000, Loss: 0.5299\n",
            "Epoch: 000, Loss: 0.5296\n",
            "Epoch: 000, Loss: 0.5296\n",
            "Epoch: 000, Loss: 0.5293\n",
            "Epoch: 000, Loss: 0.5293\n",
            "Epoch: 000, Loss: 0.5292\n",
            "Epoch: 000, Loss: 0.5299\n",
            "Epoch: 000, Loss: 0.5298\n",
            "Epoch: 000, Loss: 0.5296\n",
            "Epoch: 000, Loss: 0.5297\n",
            "Epoch: 000, Loss: 0.5295\n",
            "Epoch: 000, Loss: 0.5295\n",
            "Epoch: 000, Loss: 0.5293\n",
            "Epoch: 000, Loss: 0.5292\n",
            "Epoch: 000, Loss: 0.5290\n",
            "Epoch: 000, Loss: 0.5288\n",
            "Epoch: 000, Loss: 0.5288\n",
            "Epoch: 000, Loss: 0.5286\n",
            "Epoch: 000, Loss: 0.5284\n",
            "Epoch: 000, Loss: 0.5284\n",
            "Epoch: 000, Loss: 0.5282\n",
            "Epoch: 000, Loss: 0.5279\n",
            "Epoch: 000, Loss: 0.5279\n",
            "Epoch: 000, Loss: 0.5275\n",
            "Epoch: 000, Loss: 0.5274\n",
            "Epoch: 000, Loss: 0.5274\n",
            "Epoch: 000, Loss: 0.5273\n",
            "Epoch: 000, Loss: 0.5274\n",
            "Epoch: 000, Loss: 0.5271\n",
            "Epoch: 000, Loss: 0.5273\n",
            "Epoch: 000, Loss: 0.5269\n",
            "Epoch: 000, Loss: 0.5267\n",
            "Epoch: 000, Loss: 0.5265\n",
            "Epoch: 000, Loss: 0.5262\n",
            "Epoch: 000, Loss: 0.5262\n",
            "Epoch: 000, Loss: 0.5262\n",
            "Epoch: 000, Loss: 0.5261\n",
            "Epoch: 000, Loss: 0.5259\n",
            "Epoch: 000, Loss: 0.5258\n",
            "Epoch: 000, Loss: 0.5256\n",
            "Epoch: 000, Loss: 0.5253\n",
            "Epoch: 000, Loss: 0.5252\n",
            "Epoch: 000, Loss: 0.5250\n",
            "Epoch: 000, Loss: 0.5248\n",
            "Epoch: 000, Loss: 0.5247\n",
            "Epoch: 000, Loss: 0.5245\n",
            "Epoch: 000, Loss: 0.5243\n",
            "Epoch: 000, Loss: 0.5241\n",
            "Epoch: 000, Loss: 0.5239\n",
            "Epoch: 000, Loss: 0.5236\n",
            "Epoch: 000, Loss: 0.5234\n",
            "Epoch: 000, Loss: 0.5231\n",
            "Epoch: 000, Loss: 0.5229\n",
            "Epoch: 000, Loss: 0.5227\n",
            "Epoch: 000, Loss: 0.5226\n",
            "Epoch: 000, Loss: 0.5223\n",
            "Epoch: 000, Loss: 0.5221\n",
            "Epoch: 000, Loss: 0.5218\n",
            "Epoch: 000, Loss: 0.5220\n",
            "Epoch: 000, Loss: 0.5217\n",
            "Epoch: 000, Loss: 0.5215\n",
            "Epoch: 000, Loss: 0.5215\n",
            "Epoch: 000, Loss: 0.5210\n",
            "Epoch: 000, Loss: 0.5208\n",
            "Epoch: 000, Loss: 0.5207\n",
            "Epoch: 000, Loss: 0.5204\n",
            "Epoch: 000, Loss: 0.5203\n",
            "Epoch: 000, Loss: 0.5202\n",
            "Epoch: 000, Loss: 0.5200\n",
            "Epoch: 000, Loss: 0.5199\n",
            "Epoch: 000, Loss: 0.5197\n",
            "Epoch: 000, Loss: 0.5196\n",
            "Epoch: 000, Loss: 0.5194\n",
            "Epoch: 000, Loss: 0.5193\n",
            "Epoch: 000, Loss: 0.5192\n",
            "Epoch: 000, Loss: 0.5191\n",
            "Epoch: 000, Loss: 0.5191\n",
            "Epoch: 000, Loss: 0.5189\n",
            "Epoch: 000, Loss: 0.5188\n",
            "Epoch: 000, Loss: 0.5187\n",
            "Epoch: 000, Loss: 0.5185\n",
            "Epoch: 000, Loss: 0.5184\n",
            "Epoch: 000, Loss: 0.5184\n",
            "Epoch: 000, Loss: 0.5185\n",
            "Epoch: 000, Loss: 0.5184\n",
            "Epoch: 000, Loss: 0.5185\n",
            "Epoch: 000, Loss: 0.5182\n",
            "Epoch: 000, Loss: 0.5182\n",
            "Epoch: 000, Loss: 0.5180\n",
            "Epoch: 000, Loss: 0.5179\n",
            "Epoch: 000, Loss: 0.5177\n",
            "Epoch: 000, Loss: 0.5175\n",
            "Epoch: 000, Loss: 0.5176\n",
            "Epoch: 000, Loss: 0.5174\n",
            "Epoch: 000, Loss: 0.5172\n",
            "Epoch: 000, Loss: 0.5170\n",
            "Epoch: 000, Loss: 0.5169\n",
            "Epoch: 000, Loss: 0.5166\n",
            "Epoch: 000, Loss: 0.5165\n",
            "Epoch: 000, Loss: 0.5164\n",
            "Epoch: 000, Loss: 0.5164\n",
            "Epoch: 000, Loss: 0.5162\n",
            "Epoch: 000, Loss: 0.5159\n",
            "Epoch: 000, Loss: 0.5158\n",
            "Epoch: 000, Loss: 0.5156\n",
            "Epoch: 000, Loss: 0.5156\n",
            "Epoch: 000, Loss: 0.5157\n",
            "Epoch: 000, Loss: 0.5157\n",
            "Epoch: 000, Loss: 0.5156\n",
            "Epoch: 000, Loss: 0.5155\n",
            "Epoch: 000, Loss: 0.5154\n",
            "Epoch: 000, Loss: 0.5152\n",
            "Epoch: 000, Loss: 0.5151\n",
            "Epoch: 000, Loss: 0.5148\n",
            "Epoch: 000, Loss: 0.5146\n",
            "Epoch: 000, Loss: 0.5148\n",
            "Epoch: 000, Loss: 0.5146\n",
            "Epoch: 000, Loss: 0.5146\n",
            "Epoch: 000, Loss: 0.5146\n",
            "Epoch: 000, Loss: 0.5143\n",
            "Epoch: 000, Loss: 0.5142\n",
            "Epoch: 000, Loss: 0.5141\n",
            "Epoch: 000, Loss: 0.5141\n",
            "Epoch: 000, Loss: 0.5139\n",
            "Epoch: 000, Loss: 0.5136\n",
            "Epoch: 000, Loss: 0.5135\n",
            "Epoch: 000, Loss: 0.5133\n",
            "Epoch: 000, Loss: 0.5132\n",
            "Epoch: 000, Loss: 0.5131\n",
            "Epoch: 000, Loss: 0.5128\n",
            "Epoch: 000, Loss: 0.5128\n",
            "Epoch: 000, Loss: 0.5127\n",
            "Epoch: 000, Loss: 0.5126\n",
            "Epoch: 000, Loss: 0.5123\n",
            "Epoch: 000, Loss: 0.5121\n",
            "Epoch: 000, Loss: 0.5118\n",
            "Epoch: 000, Loss: 0.5117\n",
            "Epoch: 000, Loss: 0.5115\n",
            "Epoch: 000, Loss: 0.5111\n",
            "Epoch: 000, Loss: 0.5110\n",
            "Epoch: 000, Loss: 0.5110\n",
            "Epoch: 000, Loss: 0.5109\n",
            "Epoch: 000, Loss: 0.5108\n",
            "Epoch: 000, Loss: 0.5107\n",
            "Epoch: 000, Loss: 0.5107\n",
            "Epoch: 000, Loss: 0.5104\n",
            "Epoch: 000, Loss: 0.5102\n",
            "Epoch: 000, Loss: 0.5101\n",
            "Epoch: 000, Loss: 0.5099\n",
            "Epoch: 000, Loss: 0.5098\n",
            "Epoch: 000, Loss: 0.5097\n",
            "Epoch: 000, Loss: 0.5098\n",
            "Epoch: 000, Loss: 0.5095\n",
            "Epoch: 000, Loss: 0.5094\n",
            "Epoch: 000, Loss: 0.5094\n",
            "Epoch: 000, Loss: 0.5092\n",
            "Epoch: 000, Loss: 0.5088\n",
            "Epoch: 000, Loss: 0.5087\n",
            "Epoch: 000, Loss: 0.5086\n",
            "Epoch: 000, Loss: 0.5085\n",
            "Epoch: 000, Loss: 0.5083\n",
            "Epoch: 000, Loss: 0.5081\n",
            "Epoch: 000, Loss: 0.5081\n",
            "Epoch: 000, Loss: 0.5080\n",
            "Epoch: 000, Loss: 0.5076\n",
            "Epoch: 000, Loss: 0.5075\n",
            "Epoch: 000, Loss: 0.5073\n",
            "Epoch: 000, Loss: 0.5072\n",
            "Epoch: 000, Loss: 0.5072\n",
            "Epoch: 000, Loss: 0.5071\n",
            "Epoch: 000, Loss: 0.5069\n",
            "Epoch: 000, Loss: 0.5066\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5064\n",
            "Epoch: 000, Loss: 0.5062\n",
            "Epoch: 000, Loss: 0.5060\n",
            "Epoch: 000, Loss: 0.5058\n",
            "Epoch: 000, Loss: 0.5058\n",
            "Epoch: 000, Loss: 0.5055\n",
            "Epoch: 000, Loss: 0.5054\n",
            "Epoch: 000, Loss: 0.5052\n",
            "Epoch: 000, Loss: 0.5052\n",
            "Epoch: 000, Loss: 0.5050\n",
            "Epoch: 000, Loss: 0.5048\n",
            "Epoch: 000, Loss: 0.5045\n",
            "Epoch: 000, Loss: 0.5043\n",
            "Epoch: 000, Loss: 0.5042\n",
            "Epoch: 000, Loss: 0.5040\n",
            "Epoch: 000, Loss: 0.5041\n",
            "Epoch: 000, Loss: 0.5038\n",
            "Epoch: 000, Loss: 0.5035\n",
            "Epoch: 000, Loss: 0.5034\n",
            "Epoch: 000, Loss: 0.5035\n",
            "Epoch: 000, Loss: 0.5033\n",
            "Epoch: 000, Loss: 0.5029\n",
            "Epoch: 000, Loss: 0.5028\n",
            "Epoch: 000, Loss: 0.5028\n",
            "Epoch: 000, Loss: 0.5026\n",
            "Epoch: 000, Loss: 0.5027\n",
            "Epoch: 000, Loss: 0.5024\n",
            "Epoch: 000, Loss: 0.5021\n",
            "Epoch: 000, Loss: 0.5021\n",
            "Epoch: 000, Loss: 0.5019\n",
            "Epoch: 000, Loss: 0.5019\n",
            "Epoch: 000, Loss: 0.5019\n",
            "Epoch: 000, Loss: 0.5018\n",
            "Epoch: 000, Loss: 0.5017\n",
            "Epoch: 000, Loss: 0.5017\n",
            "Epoch: 000, Loss: 0.5016\n",
            "Epoch: 000, Loss: 0.5015\n",
            "Epoch: 000, Loss: 0.5014\n",
            "Epoch: 000, Loss: 0.5012\n",
            "Epoch: 000, Loss: 0.5011\n",
            "Epoch: 000, Loss: 0.5011\n",
            "Epoch: 000, Loss: 0.5012\n",
            "Epoch: 000, Loss: 0.5014\n",
            "Epoch: 000, Loss: 0.5013\n",
            "Epoch: 000, Loss: 0.5013\n",
            "Epoch: 000, Loss: 0.5012\n",
            "Epoch: 000, Loss: 0.5011\n",
            "Epoch: 000, Loss: 0.5014\n",
            "Epoch: 000, Loss: 0.5012\n",
            "Epoch: 000, Loss: 0.5012\n",
            "Epoch: 000, Loss: 0.5009\n",
            "Epoch: 000, Loss: 0.5007\n",
            "Epoch: 000, Loss: 0.5006\n",
            "Epoch: 000, Loss: 0.5003\n",
            "Epoch: 000, Loss: 0.5000\n",
            "Epoch: 000, Loss: 0.4999\n",
            "Epoch: 000, Loss: 0.4997\n",
            "Epoch: 000, Loss: 0.4993\n",
            "Epoch: 000, Loss: 0.4993\n",
            "Epoch: 000, Loss: 0.4993\n",
            "Epoch: 000, Loss: 0.4994\n",
            "Epoch: 000, Loss: 0.4994\n",
            "Epoch: 000, Loss: 0.4994\n",
            "Epoch: 000, Loss: 0.4993\n",
            "Epoch: 000, Loss: 0.4992\n",
            "Epoch: 000, Loss: 0.4993\n",
            "Epoch: 000, Loss: 0.4992\n",
            "Epoch: 000, Loss: 0.4990\n",
            "Epoch: 000, Loss: 0.4990\n",
            "Epoch: 000, Loss: 0.4990\n",
            "Epoch: 000, Loss: 0.4988\n",
            "Epoch: 000, Loss: 0.4987\n",
            "Epoch: 000, Loss: 0.4986\n",
            "Epoch: 000, Loss: 0.4984\n",
            "Epoch: 000, Loss: 0.4983\n",
            "Epoch: 000, Loss: 0.4984\n",
            "Epoch: 000, Loss: 0.4983\n",
            "Epoch: 000, Loss: 0.4981\n",
            "Epoch: 000, Loss: 0.4981\n",
            "Epoch: 000, Loss: 0.4981\n",
            "Epoch: 000, Loss: 0.4981\n",
            "Epoch: 000, Loss: 0.4980\n",
            "Epoch: 000, Loss: 0.4980\n",
            "Epoch: 000, Loss: 0.4979\n",
            "Epoch: 000, Loss: 0.4978\n",
            "Epoch: 000, Loss: 0.4977\n",
            "Epoch: 000, Loss: 0.4977\n",
            "Epoch: 000, Loss: 0.4977\n",
            "Epoch: 000, Loss: 0.4976\n",
            "Epoch: 000, Loss: 0.4974\n",
            "Epoch: 000, Loss: 0.4971\n",
            "Epoch: 000, Loss: 0.4969\n",
            "Epoch: 000, Loss: 0.4968\n",
            "Epoch: 000, Loss: 0.4968\n",
            "Epoch: 000, Loss: 0.4967\n",
            "Epoch: 000, Loss: 0.4966\n",
            "Epoch: 000, Loss: 0.4966\n",
            "Epoch: 000, Loss: 0.4964\n",
            "Epoch: 000, Loss: 0.4963\n",
            "Epoch: 000, Loss: 0.4962\n",
            "Epoch: 000, Loss: 0.4961\n",
            "Epoch: 000, Loss: 0.4960\n",
            "Epoch: 000, Loss: 0.4959\n",
            "Epoch: 000, Loss: 0.4956\n",
            "Epoch: 000, Loss: 0.4955\n",
            "Epoch: 000, Loss: 0.4954\n",
            "Epoch: 000, Loss: 0.4954\n",
            "Epoch: 000, Loss: 0.4953\n",
            "Epoch: 000, Loss: 0.4952\n",
            "Epoch: 000, Loss: 0.4950\n",
            "Epoch: 000, Loss: 0.4949\n",
            "Epoch: 000, Loss: 0.4949\n",
            "Epoch: 000, Loss: 0.4946\n",
            "Epoch: 000, Loss: 0.4945\n",
            "Epoch: 000, Loss: 0.4943\n",
            "Epoch: 000, Loss: 0.4941\n",
            "Epoch: 000, Loss: 0.4940\n",
            "Epoch: 000, Loss: 0.4939\n",
            "Epoch: 000, Loss: 0.4939\n",
            "Epoch: 000, Loss: 0.4938\n",
            "Epoch: 000, Loss: 0.4937\n",
            "Epoch: 000, Loss: 0.4935\n",
            "Epoch: 000, Loss: 0.4934\n",
            "Epoch: 000, Loss: 0.4932\n",
            "Epoch: 000, Loss: 0.4931\n",
            "Epoch: 000, Loss: 0.4932\n",
            "Epoch: 000, Loss: 0.4931\n",
            "Epoch: 000, Loss: 0.4931\n",
            "Epoch: 000, Loss: 0.4930\n",
            "Epoch: 000, Loss: 0.4928\n",
            "Epoch: 000, Loss: 0.4926\n",
            "Epoch: 000, Loss: 0.4927\n",
            "Epoch: 000, Loss: 0.4924\n",
            "Epoch: 000, Loss: 0.4922\n",
            "Epoch: 000, Loss: 0.4921\n",
            "Epoch: 000, Loss: 0.4918\n",
            "Epoch: 000, Loss: 0.4918\n",
            "Epoch: 000, Loss: 0.4917\n",
            "Epoch: 000, Loss: 0.4916\n",
            "Epoch: 000, Loss: 0.4917\n",
            "Epoch: 000, Loss: 0.4915\n",
            "Epoch: 000, Loss: 0.4913\n",
            "Epoch: 000, Loss: 0.4912\n",
            "Epoch: 000, Loss: 0.4908\n",
            "Epoch: 000, Loss: 0.4906\n",
            "Epoch: 000, Loss: 0.4904\n",
            "Epoch: 000, Loss: 0.4904\n",
            "Epoch: 000, Loss: 0.4900\n",
            "Epoch: 000, Loss: 0.4901\n",
            "Epoch: 000, Loss: 0.4899\n",
            "Epoch: 000, Loss: 0.4898\n",
            "Epoch: 000, Loss: 0.4897\n",
            "Epoch: 000, Loss: 0.4896\n",
            "Epoch: 000, Loss: 0.4894\n",
            "Epoch: 000, Loss: 0.4895\n",
            "Epoch: 000, Loss: 0.4895\n",
            "Epoch: 000, Loss: 0.4895\n",
            "Epoch: 000, Loss: 0.4894\n",
            "Epoch: 000, Loss: 0.4893\n",
            "Epoch: 000, Loss: 0.4892\n",
            "Epoch: 000, Loss: 0.4890\n",
            "Epoch: 000, Loss: 0.4889\n",
            "Epoch: 000, Loss: 0.4888\n",
            "Epoch: 000, Loss: 0.4886\n",
            "Epoch: 000, Loss: 0.4886\n",
            "Epoch: 000, Loss: 0.4886\n",
            "Epoch: 000, Loss: 0.4884\n",
            "Epoch: 000, Loss: 0.4884\n",
            "Epoch: 000, Loss: 0.4882\n",
            "Epoch: 000, Loss: 0.4882\n",
            "Epoch: 000, Loss: 0.4892\n",
            "Epoch: 000, Loss: 0.4892\n",
            "Epoch: 000, Loss: 0.4890\n",
            "Epoch: 000, Loss: 0.4889\n",
            "Epoch: 000, Loss: 0.4888\n",
            "Epoch: 000, Loss: 0.4886\n",
            "Epoch: 000, Loss: 0.4885\n",
            "Epoch: 000, Loss: 0.4885\n",
            "Epoch: 000, Loss: 0.4884\n",
            "Epoch: 000, Loss: 0.4883\n",
            "Epoch: 000, Loss: 0.4882\n",
            "Epoch: 000, Loss: 0.4883\n",
            "Epoch: 000, Loss: 0.4882\n",
            "Epoch: 000, Loss: 0.4881\n",
            "Epoch: 000, Loss: 0.4879\n",
            "Epoch: 000, Loss: 0.4879\n",
            "Epoch: 000, Loss: 0.4879\n",
            "Epoch: 000, Loss: 0.4876\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4872\n",
            "Epoch: 000, Loss: 0.4871\n",
            "Epoch: 000, Loss: 0.4871\n",
            "Epoch: 000, Loss: 0.4871\n",
            "Epoch: 000, Loss: 0.4870\n",
            "Epoch: 000, Loss: 0.4888\n",
            "Epoch: 000, Loss: 0.4887\n",
            "Epoch: 000, Loss: 0.4889\n",
            "Epoch: 000, Loss: 0.4888\n",
            "Epoch: 000, Loss: 0.4886\n",
            "Epoch: 000, Loss: 0.4885\n",
            "Epoch: 000, Loss: 0.4884\n",
            "Epoch: 000, Loss: 0.4882\n",
            "Epoch: 000, Loss: 0.4880\n",
            "Epoch: 000, Loss: 0.4878\n",
            "Epoch: 000, Loss: 0.4877\n",
            "Epoch: 000, Loss: 0.4876\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4875\n",
            "Epoch: 000, Loss: 0.4874\n",
            "Epoch: 000, Loss: 0.4872\n",
            "Epoch: 000, Loss: 0.4871\n",
            "Epoch: 000, Loss: 0.4873\n",
            "Epoch: 000, Loss: 0.4872\n",
            "Epoch: 000, Loss: 0.4872\n",
            "Epoch: 000, Loss: 0.4871\n",
            "Epoch: 000, Loss: 0.4870\n",
            "Epoch: 000, Loss: 0.4869\n",
            "Epoch: 000, Loss: 0.4869\n",
            "Epoch: 000, Loss: 0.4868\n",
            "Epoch: 000, Loss: 0.4868\n",
            "Epoch: 000, Loss: 0.4867\n",
            "Epoch: 000, Loss: 0.4866\n",
            "Epoch: 000, Loss: 0.4864\n",
            "Epoch: 000, Loss: 0.4862\n",
            "Epoch: 000, Loss: 0.4862\n",
            "Epoch: 000, Loss: 0.4863\n",
            "Epoch: 000, Loss: 0.4862\n",
            "Epoch: 000, Loss: 0.4861\n",
            "Epoch: 000, Loss: 0.4861\n",
            "Epoch: 000, Loss: 0.4861\n",
            "Epoch: 000, Loss: 0.4859\n",
            "Epoch: 000, Loss: 0.4857\n",
            "Epoch: 000, Loss: 0.4857\n",
            "Epoch: 000, Loss: 0.4856\n",
            "Epoch: 000, Loss: 0.4854\n",
            "Epoch: 000, Loss: 0.4853\n",
            "Epoch: 000, Loss: 0.4851\n",
            "Epoch: 000, Loss: 0.4851\n",
            "Epoch: 000, Loss: 0.4850\n",
            "Epoch: 000, Loss: 0.4848\n",
            "Epoch: 000, Loss: 0.4846\n",
            "Epoch: 000, Loss: 0.4846\n",
            "Epoch: 000, Loss: 0.4846\n",
            "Epoch: 000, Loss: 0.4846\n",
            "Epoch: 000, Loss: 0.4844\n",
            "Epoch: 000, Loss: 0.4842\n",
            "Epoch: 000, Loss: 0.4842\n",
            "Epoch: 000, Loss: 0.4843\n",
            "Epoch: 000, Loss: 0.4845\n",
            "Epoch: 000, Loss: 0.4843\n",
            "Epoch: 000, Loss: 0.4842\n",
            "Epoch: 000, Loss: 0.4842\n",
            "Epoch: 000, Loss: 0.4840\n",
            "Epoch: 000, Loss: 0.4840\n",
            "Epoch: 000, Loss: 0.4839\n",
            "Epoch: 000, Loss: 0.4838\n",
            "Epoch: 000, Loss: 0.4838\n",
            "Epoch: 000, Loss: 0.4836\n",
            "Epoch: 000, Loss: 0.4835\n",
            "Epoch: 000, Loss: 0.4835\n",
            "Epoch: 000, Loss: 0.4834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_embeddings(model):\n",
        "    if hasattr(model, 'embedding'):\n",
        "        embeddings = model.embedding.weight\n",
        "        print(f\"Embeddings found: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "    else:\n",
        "        print(\"No direct embeddings found in model.\")\n",
        "        return None\n",
        "\n",
        "embeddings_all_nodes = get_model_embeddings(model_lightgcn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ1FSxL5mT5X",
        "outputId": "f872ccbc-6de4-4efb-f538-36d944b345d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings found: torch.Size([17576, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = data.to_homogeneous()"
      ],
      "metadata": {
        "id": "cmswYqrke4zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def calculate_metrics_for_embedded_nodes(embeddings, edge_index, k_values=[1, 5, 10]):\n",
        "    num_nodes = embeddings.shape[0]\n",
        "    metrics = {k: {'recall': 0, 'precision': 0, 'f1': 0} for k in k_values}\n",
        "\n",
        "    similarity_matrix = torch.matmul(embeddings, embeddings.t())\n",
        "\n",
        "    positive_edges = edge_index.t().cpu().numpy()\n",
        "\n",
        "    for k in k_values:\n",
        "        for node in range(num_nodes):\n",
        "            scores = similarity_matrix[node]\n",
        "\n",
        "            sorted_nodes = torch.argsort(scores, descending=True).cpu().numpy()\n",
        "\n",
        "            top_k_recommendations = sorted_nodes[sorted_nodes != node][:k]\n",
        "\n",
        "            true_positives = sum([1 for n in top_k_recommendations if n in positive_edges[node]])\n",
        "            recall = true_positives / k\n",
        "            precision = true_positives / len(top_k_recommendations)\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "            metrics[k]['recall'] += recall\n",
        "            metrics[k]['precision'] += precision\n",
        "            metrics[k]['f1'] += f1\n",
        "\n",
        "    for k in k_values:\n",
        "        metrics[k]['recall'] /= num_nodes\n",
        "        metrics[k]['precision'] /= num_nodes\n",
        "        metrics[k]['f1'] /= num_nodes\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = calculate_metrics_for_embedded_nodes(embeddings_all_nodes, dataset2.edge_index, k_values=[1, 5, 10])\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    print(f\"Recall@{k}: {metrics[k]['recall']:.4f}, Precision@{k}: {metrics[k]['precision']:.4f}, F1@{k}: {metrics[k]['f1']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQfFySDth8V",
        "outputId": "1ed54c1c-e8ba-478a-d663-62f500d5284e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@1: 0.1273, Precision@1: 0.1273, F1@1: 0.1273\n",
            "Recall@5: 0.0440, Precision@5: 0.0440, F1@5: 0.0440\n",
            "Recall@10: 0.0266, Precision@10: 0.0266, F1@10: 0.0266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-8zDqTitil6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}